{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Importing Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import config\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from agents.tfagents.dqn import DQNAgent\n",
    "from agents.tfagents.ppo import PPOAgent\n",
    "from environments.environment import TradingEnvironment\n",
    "from environments.wrappers.tf.tfenv import TFTradingEnvironment\n",
    "from environments.rewards.marketlimitorder import MarketLimitOrderRF\n",
    "from metrics.trading.pnl import CumulativeLogReturn\n",
    "from metrics.trading.risk import InvestmentRisk\n",
    "from metrics.trading.sharpe import SharpeRatio\n",
    "from metrics.trading.sortino import SortinoRatio\n",
    "from metrics.trading.drawdown import MaximumDrawdown"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building Eval Environments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def build_eval_environments(\n",
    "        dataset_filepath,\n",
    "        timeframe_size,\n",
    "        target_horizon_len,\n",
    "        num_eval_samples,\n",
    "        fees,\n",
    "        reward_fn_instance\n",
    "):\n",
    "    # Reading dataset\n",
    "    crypto_dataset_df = pd.read_csv(config.dataset_save_filepath.format(dataset_filepath))\n",
    "    samples_df = crypto_dataset_df[config.regression_features]\n",
    "\n",
    "    # Scaling data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1.0))\n",
    "    samples = samples_df.to_numpy(dtype=np.float32)\n",
    "\n",
    "    num_train_scale_samples = samples.shape[0] - num_eval_samples - target_horizon_len - timeframe_size + 1\n",
    "    samples[: num_train_scale_samples] = scaler.fit_transform(samples[: num_train_scale_samples])\n",
    "    samples[num_train_scale_samples: ] = scaler.transform(samples[num_train_scale_samples: ])\n",
    "\n",
    "    # Constructing timeframes for train, test\n",
    "    inputs = np.float32([samples[i: i + timeframe_size] for i in range(samples.shape[0] - timeframe_size - target_horizon_len + 1)])\n",
    "\n",
    "    # Splitting inputs to train-test data\n",
    "    num_train_inputs = inputs.shape[0] - num_eval_samples\n",
    "    x_eval = inputs[num_train_inputs:]\n",
    "\n",
    "    # Computing reward functions for train, test data\n",
    "    closes = crypto_dataset_df['close'].to_numpy(dtype=np.float32)\n",
    "    highs = crypto_dataset_df['high'].to_numpy(dtype=np.float32)\n",
    "    lows = crypto_dataset_df['low'].to_numpy(dtype=np.float32)\n",
    "\n",
    "    eval_reward_fn = reward_fn_instance(\n",
    "        timeframe_size=timeframe_size,\n",
    "        target_horizon_len=target_horizon_len,\n",
    "        highs=highs[samples.shape[0] - num_eval_samples - timeframe_size - target_horizon_len + 1:],\n",
    "        lows=lows[samples.shape[0] - num_eval_samples - timeframe_size - target_horizon_len + 1:],\n",
    "        closes=closes[samples.shape[0] - num_eval_samples - timeframe_size - target_horizon_len + 1:],\n",
    "        fees_percentage=fees\n",
    "    )\n",
    "\n",
    "    assert x_eval.shape[0] == eval_reward_fn.get_reward_fn_shape()[0], \\\n",
    "        f'AssertionError: DimensionMismatch: x_eval: {x_eval.shape}, eval_reward_fn: {eval_reward_fn.get_reward_fn_shape()}'\n",
    "\n",
    "    eval_env = TFTradingEnvironment(\n",
    "        env=TradingEnvironment(env_config={\n",
    "            'states': x_eval,\n",
    "            'reward_fn': eval_reward_fn,\n",
    "            'episode_steps': x_eval.shape[0] - 1,\n",
    "            'metrics': [CumulativeLogReturn(), InvestmentRisk(), SharpeRatio(), SortinoRatio(), MaximumDrawdown()]\n",
    "        })\n",
    "    )\n",
    "    tf_eval_env = TFPyEnvironment(environment=eval_env)\n",
    "    return eval_env, tf_eval_env"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building & Loading Agents"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def build_agent(\n",
    "        agent_instance,\n",
    "        env,\n",
    "        checkpoint_filepath,\n",
    "        fc_layers,\n",
    "        conv_layers\n",
    "):\n",
    "    agent = agent_instance(\n",
    "        input_tensor_spec=env.observation_spec(),\n",
    "        action_spec=env.action_spec(),\n",
    "        time_step_spec=env.time_step_spec(),\n",
    "        env_batch_size=env.batch_size,\n",
    "        checkpoint_filepath=checkpoint_filepath,\n",
    "        fc_layers=fc_layers,\n",
    "        conv_layers=conv_layers\n",
    "    )\n",
    "    agent.initialize()\n",
    "    return agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building Eval Method"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def eval_tradernet(tradernet_policy, tf_env_wrapper):\n",
    "    time_step = tf_env_wrapper.reset()\n",
    "    tradernet_policy_state = tradernet_policy.get_initial_state(tf_env_wrapper.batch_size)\n",
    "    cumulative_rewards = 0.0\n",
    "    cumulative_pnls = 0.0\n",
    "    pnls = []\n",
    "\n",
    "    while not time_step.is_last():\n",
    "        action = tradernet_policy.action(time_step=time_step, policy_state=tradernet_policy_state).action\n",
    "        time_step = tf_env_wrapper.step(action)\n",
    "        reward = time_step.reward.numpy()[0]\n",
    "        cumulative_rewards += reward\n",
    "\n",
    "        if action != 2:\n",
    "            cumulative_pnls += reward\n",
    "        pnls.append(cumulative_pnls)\n",
    "    return cumulative_rewards, pnls"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building Configs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "datasets_dict = {'BTC': 'BTC', 'ETH': 'ETH', 'ADA': 'ADA', 'XRP': 'XRP', 'LTC': 'LTC'}\n",
    "agent_dict = {\n",
    "    'PPO': {\n",
    "        'agent_instance': PPOAgent,\n",
    "        'fc_layers': [256, 256],\n",
    "        'conv_layers': [(32, 3, 1)]\n",
    "    },\n",
    "    'DDQN': {\n",
    "        'agent_instance': DQNAgent,\n",
    "        'fc_layers': [256, 256],\n",
    "        'conv_layers': [(32, 3, 1)]\n",
    "    },\n",
    "}\n",
    "env_dict = {\n",
    "    'timeframe_size': 12,\n",
    "    'target_horizon_len': 20,\n",
    "    'num_eval_samples': 2250,\n",
    "    'fees': 0.007\n",
    "}\n",
    "\n",
    "reward_fn_name = 'Market-Limit Orders'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Running Experiments for TraderNet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   average_returns  Cumulative Log Returns  Investment Risk    Sharpe  \\\n",
      "0        13.173482               13.173482         0.477546  1.357097   \n",
      "\n",
      "    Sortino  Maximum Drawdown  \n",
      "0  4.475173          0.357828   \n",
      "\n",
      "              0\n",
      "2244  13.170206\n",
      "2245  13.159067\n",
      "2246  13.153347\n",
      "2247  13.160080\n",
      "2248  13.173482\n",
      "   average_returns  Cumulative Log Returns  Investment Risk    Sharpe  \\\n",
      "0        35.808246               35.808246         0.331258  1.776735   \n",
      "\n",
      "    Sortino  Maximum Drawdown  \n",
      "0  54.24813          0.112173   \n",
      "\n",
      "              0\n",
      "2244  35.782564\n",
      "2245  35.776924\n",
      "2246  35.771749\n",
      "2247  35.786066\n",
      "2248  35.808246\n",
      "   average_returns  Cumulative Log Returns  Investment Risk    Sharpe  \\\n",
      "0        29.304636               29.933476         0.338129  1.736712   \n",
      "\n",
      "     Sortino  Maximum Drawdown  \n",
      "0  26.970217           1.19898   \n",
      "\n",
      "              0\n",
      "2244  29.878721\n",
      "2245  29.897086\n",
      "2246  29.913147\n",
      "2247  29.938231\n",
      "2248  29.933476\n",
      "   average_returns  Cumulative Log Returns  Investment Risk    Sharpe  \\\n",
      "0        37.607645               37.967527         0.341551  1.668653   \n",
      "\n",
      "     Sortino  Maximum Drawdown  \n",
      "0  73.033221          0.328964   \n",
      "\n",
      "              0\n",
      "2244  37.978799\n",
      "2245  37.974140\n",
      "2246  37.972376\n",
      "2247  37.974382\n",
      "2248  37.967527\n",
      "   average_returns  Cumulative Log Returns  Investment Risk    Sharpe  \\\n",
      "0        27.939025               27.939025         0.353935  1.701618   \n",
      "\n",
      "     Sortino  Maximum Drawdown  \n",
      "0  24.905018          0.156468   \n",
      "\n",
      "              0\n",
      "2244  27.856676\n",
      "2245  27.878817\n",
      "2246  27.910505\n",
      "2247  27.942432\n",
      "2248  27.939025\n",
      "   average_returns  Cumulative Log Returns  Investment Risk   Sharpe  \\\n",
      "0        13.630851                13.85867         0.472033  1.37504   \n",
      "\n",
      "    Sortino  Maximum Drawdown  \n",
      "0  4.804385          1.108619   \n",
      "\n",
      "              0\n",
      "2244  13.871340\n",
      "2245  13.870484\n",
      "2246  13.870613\n",
      "2247  13.867976\n",
      "2248  13.858670\n",
      "   average_returns  Cumulative Log Returns  Investment Risk    Sharpe  \\\n",
      "0        35.510421               35.510421         0.300133  1.825952   \n",
      "\n",
      "     Sortino  Maximum Drawdown  \n",
      "0  55.600972          0.032735   \n",
      "\n",
      "              0\n",
      "2244  35.522693\n",
      "2245  35.522909\n",
      "2246  35.517734\n",
      "2247  35.518009\n",
      "2248  35.510421\n",
      "   average_returns  Cumulative Log Returns  Investment Risk    Sharpe  \\\n",
      "0        30.451432               30.451432          0.34771  1.687213   \n",
      "\n",
      "    Sortino  Maximum Drawdown  \n",
      "0  27.49597          0.204019   \n",
      "\n",
      "              0\n",
      "2244  30.353633\n",
      "2245  30.371998\n",
      "2246  30.388058\n",
      "2247  30.413143\n",
      "2248  30.451432\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).agent._optimizer.iter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).agent._optimizer.iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).agent._optimizer.beta_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).agent._optimizer.beta_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).agent._optimizer.beta_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).agent._optimizer.beta_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).agent._optimizer.decay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).agent._optimizer.decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).agent._optimizer.learning_rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).agent._optimizer.learning_rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   average_returns  Cumulative Log Returns  Investment Risk    Sharpe  \\\n",
      "0        39.913524               39.913524         0.327701  1.721084   \n",
      "\n",
      "     Sortino  Maximum Drawdown  \n",
      "0  93.209064          0.583794   \n",
      "\n",
      "              0\n",
      "2244  39.909773\n",
      "2245  39.903396\n",
      "2246  39.901632\n",
      "2247  39.903638\n",
      "2248  39.913524\n",
      "   average_returns  Cumulative Log Returns  Investment Risk    Sharpe  \\\n",
      "0        30.108601               30.108601         0.351267  1.695454   \n",
      "\n",
      "     Sortino  Maximum Drawdown  \n",
      "0  31.782358          0.144869   \n",
      "\n",
      "              0\n",
      "2244  30.071010\n",
      "2245  30.078573\n",
      "2246  30.092790\n",
      "2247  30.112009\n",
      "2248  30.108601\n"
     ]
    }
   ],
   "source": [
    "for agent_name, agent_config in agent_dict.items():\n",
    "    for dataset_name, dataset_filepath in datasets_dict.items():\n",
    "        eval_env, tf_eval_env = build_eval_environments(\n",
    "            dataset_filepath=dataset_filepath,\n",
    "            reward_fn_instance=MarketLimitOrderRF,\n",
    "            **env_dict\n",
    "        )\n",
    "        tradernet = build_agent(\n",
    "            env=tf_eval_env,\n",
    "            checkpoint_filepath=f'database/storage/checkpoints/experiments/tradernet/{agent_name}/{dataset_name}/{reward_fn_name}/',\n",
    "            **agent_config\n",
    "        )\n",
    "        average_returns, pnls = eval_tradernet(\n",
    "            tradernet_policy=tradernet.policy,\n",
    "            tf_env_wrapper=tf_eval_env\n",
    "        )\n",
    "        metrics = {\n",
    "            'average_returns': [average_returns],\n",
    "            **{key: [metric] for key, metric in eval_env.get_episode_metrics().items()}\n",
    "        }\n",
    "        results_df = pd.DataFrame(metrics)\n",
    "        results_df.to_csv(f'experiments/tradernet/{agent_name}/{dataset_name}_{reward_fn_name}_metrics.csv', index=False)\n",
    "\n",
    "        print(results_df, '\\n')\n",
    "\n",
    "        episode_pnls_df = pd.DataFrame(pnls)\n",
    "        episode_pnls_df.to_csv(f'experiments/tradernet/{agent_name}/{dataset_name}_{reward_fn_name}_eval_cumul_pnls.csv', index=False)\n",
    "\n",
    "        print(episode_pnls_df.tail(5))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}