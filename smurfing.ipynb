{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Importing Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import config\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from agents.tfagents.dqn import DQNAgent\n",
    "from agents.tfagents.ppo import PPOAgent\n",
    "from environments.environment import TradingEnvironment\n",
    "from environments.wrappers.tf.tfenv import TFTradingEnvironment\n",
    "from environments.rewards.marketlimitorder import MarketLimitOrderRF\n",
    "from metrics.trading.pnl import CumulativeLogReturn\n",
    "from metrics.trading.risk import InvestmentRisk\n",
    "from metrics.trading.sharpe import SharpeRatio\n",
    "from metrics.trading.sortino import SortinoRatio\n",
    "from metrics.trading.drawdown import MaximumDrawdown"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building Eval Environments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def build_eval_environments(\n",
    "        dataset_filepath,\n",
    "        timeframe_size,\n",
    "        target_horizon_len,\n",
    "        num_eval_samples,\n",
    "        fees,\n",
    "        reward_fn_instance\n",
    "):\n",
    "    # Reading dataset\n",
    "    crypto_dataset_df = pd.read_csv(config.dataset_save_filepath.format(dataset_filepath))\n",
    "    samples_df = crypto_dataset_df[config.regression_features]\n",
    "\n",
    "    # Scaling data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1.0))\n",
    "    samples = samples_df.to_numpy(dtype=np.float32)\n",
    "\n",
    "    num_train_scale_samples = samples.shape[0] - num_eval_samples - target_horizon_len - timeframe_size + 1\n",
    "    samples[: num_train_scale_samples] = scaler.fit_transform(samples[: num_train_scale_samples])\n",
    "    samples[num_train_scale_samples: ] = scaler.transform(samples[num_train_scale_samples: ])\n",
    "\n",
    "    # Constructing timeframes for train, test\n",
    "    inputs = np.float32([samples[i: i + timeframe_size] for i in range(samples.shape[0] - timeframe_size - target_horizon_len + 1)])\n",
    "\n",
    "    # Splitting inputs to train-test data\n",
    "    num_train_inputs = inputs.shape[0] - num_eval_samples\n",
    "    x_eval = inputs[num_train_inputs:]\n",
    "\n",
    "    # Computing reward functions for train, test data\n",
    "    closes = crypto_dataset_df['close'].to_numpy(dtype=np.float32)\n",
    "    highs = crypto_dataset_df['high'].to_numpy(dtype=np.float32)\n",
    "    lows = crypto_dataset_df['low'].to_numpy(dtype=np.float32)\n",
    "\n",
    "    eval_reward_fn = reward_fn_instance(\n",
    "        timeframe_size=timeframe_size,\n",
    "        target_horizon_len=target_horizon_len,\n",
    "        highs=highs[samples.shape[0] - num_eval_samples - timeframe_size - target_horizon_len + 1:],\n",
    "        lows=lows[samples.shape[0] - num_eval_samples - timeframe_size - target_horizon_len + 1:],\n",
    "        closes=closes[samples.shape[0] - num_eval_samples - timeframe_size - target_horizon_len + 1:],\n",
    "        fees_percentage=fees\n",
    "    )\n",
    "\n",
    "    assert x_eval.shape[0] == eval_reward_fn.get_reward_fn_shape()[0], \\\n",
    "        f'AssertionError: DimensionMismatch: x_eval: {x_eval.shape}, eval_reward_fn: {eval_reward_fn.get_reward_fn_shape()}'\n",
    "\n",
    "    eval_env = TFTradingEnvironment(\n",
    "        env=TradingEnvironment(env_config={\n",
    "            'states': x_eval,\n",
    "            'reward_fn': eval_reward_fn,\n",
    "            'episode_steps': x_eval.shape[0] - 1,\n",
    "            'metrics': [CumulativeLogReturn(), InvestmentRisk(), SharpeRatio(), SortinoRatio(), MaximumDrawdown()]\n",
    "        })\n",
    "    )\n",
    "    tf_eval_env = TFPyEnvironment(environment=eval_env)\n",
    "    return eval_env, tf_eval_env"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building & Loading Agents"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def build_agent(\n",
    "        agent_instance,\n",
    "        env,\n",
    "        checkpoint_filepath,\n",
    "        fc_layers,\n",
    "        conv_layers\n",
    "):\n",
    "    agent = agent_instance(\n",
    "        input_tensor_spec=env.observation_spec(),\n",
    "        action_spec=env.action_spec(),\n",
    "        time_step_spec=env.time_step_spec(),\n",
    "        env_batch_size=env.batch_size,\n",
    "        checkpoint_filepath=checkpoint_filepath,\n",
    "        fc_layers=fc_layers,\n",
    "        conv_layers=conv_layers\n",
    "    )\n",
    "    agent.initialize()\n",
    "    return agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building Eval Method"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def eval_tradernet_smurf(tradernet_policy, smurf_policy, tf_env_wrapper):\n",
    "    time_step = tf_env_wrapper.reset()\n",
    "    tradernet_policy_state = tradernet_policy.get_initial_state(tf_env_wrapper.batch_size)\n",
    "    smurf_policy_state = smurf_policy.get_initial_state(tf_env_wrapper.batch_size)\n",
    "    cumulative_rewards = 0.0\n",
    "    cumulative_pnls = 0.0\n",
    "    pnls = []\n",
    "\n",
    "    while not time_step.is_last():\n",
    "        smurf_action = smurf_policy.action(time_step=time_step, policy_state=smurf_policy_state).action\n",
    "        action = 2 if smurf_action == 2 else tradernet_policy.action(time_step=time_step, policy_state=tradernet_policy_state).action\n",
    "        time_step = tf_env_wrapper.step(action)\n",
    "        reward = time_step.reward.numpy()[0]\n",
    "        cumulative_rewards += reward\n",
    "\n",
    "        if action != 2:\n",
    "            cumulative_pnls += reward\n",
    "        pnls.append(cumulative_pnls)\n",
    "    return cumulative_rewards, pnls"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building Configs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "datasets_dict = {'BTC': 'BTC', 'ETH': 'ETH', 'ADA': 'ADA', 'XRP': 'XRP', 'LTC': 'LTC'}\n",
    "agent_dict = {\n",
    "    'PPO': {\n",
    "        'agent_instance': PPOAgent,\n",
    "        'fc_layers': [256, 256],\n",
    "        'conv_layers': [(32, 3, 1)]\n",
    "    },\n",
    "    'DDQN': {\n",
    "        'agent_instance': DQNAgent,\n",
    "        'fc_layers': [256, 256],\n",
    "        'conv_layers': [(32, 3, 1)]\n",
    "    },\n",
    "}\n",
    "env_dict = {\n",
    "    'timeframe_size': 12,\n",
    "    'target_horizon_len': 20,\n",
    "    'num_eval_samples': 2250,\n",
    "    'fees': 0.007\n",
    "}\n",
    "\n",
    "reward_fn_name = 'Market-Limit Orders'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Running Experiments for TraderNet + Smurf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kohli\\Desktop\\TraderNetv2\\metrics\\trading\\sharpe.py:20: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.exp(average_returns/std_returns)\n",
      "C:\\Users\\Kohli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\Kohli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "C:\\Users\\Kohli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   average_returns  Cumulative Log Returns  Investment Risk  Sharpe  Sortino  \\\n",
      "0       -34.009088                     0.0                0     NaN      NaN   \n",
      "\n",
      "   Maximum Drawdown  \n",
      "0                 0   \n",
      "\n",
      "        0\n",
      "2244  0.0\n",
      "2245  0.0\n",
      "2246  0.0\n",
      "2247  0.0\n",
      "2248  0.0\n",
      "   average_returns  Cumulative Log Returns  Investment Risk    Sharpe  \\\n",
      "0        35.808246               35.808246         0.331258  1.776735   \n",
      "\n",
      "    Sortino  Maximum Drawdown  \n",
      "0  54.24813          0.112173   \n",
      "\n",
      "              0\n",
      "2244  35.782564\n",
      "2245  35.776924\n",
      "2246  35.771749\n",
      "2247  35.786066\n",
      "2248  35.808246\n",
      "   average_returns  Cumulative Log Returns  Investment Risk    Sharpe  \\\n",
      "0        -2.324393                19.50032         0.325718  1.537387   \n",
      "\n",
      "    Sortino  Maximum Drawdown  \n",
      "0  8.341776        203.941111   \n",
      "\n",
      "             0\n",
      "2244  19.50032\n",
      "2245  19.50032\n",
      "2246  19.50032\n",
      "2247  19.50032\n",
      "2248  19.50032\n",
      "   average_returns  Cumulative Log Returns  Investment Risk    Sharpe  \\\n",
      "0        19.905309               32.385187         0.327098  1.579083   \n",
      "\n",
      "     Sortino  Maximum Drawdown  \n",
      "0  36.215739          0.324601   \n",
      "\n",
      "              0\n",
      "2244  32.396459\n",
      "2245  32.391800\n",
      "2246  32.390036\n",
      "2247  32.392042\n",
      "2248  32.385187\n",
      "   average_returns  Cumulative Log Returns  Investment Risk    Sharpe  \\\n",
      "0        27.939025               27.939025         0.353935  1.701618   \n",
      "\n",
      "     Sortino  Maximum Drawdown  \n",
      "0  24.905018          0.156468   \n",
      "\n",
      "              0\n",
      "2244  27.856676\n",
      "2245  27.878817\n",
      "2246  27.910505\n",
      "2247  27.942432\n",
      "2248  27.939025\n",
      "   average_returns  Cumulative Log Returns  Investment Risk    Sharpe  \\\n",
      "0       -22.734728                4.126795          0.39418  1.197693   \n",
      "\n",
      "   Sortino  Maximum Drawdown  \n",
      "0  1.54957          0.927298   \n",
      "\n",
      "             0\n",
      "2244  4.126795\n",
      "2245  4.126795\n",
      "2246  4.126795\n",
      "2247  4.126795\n",
      "2248  4.126795\n",
      "   average_returns  Cumulative Log Returns  Investment Risk    Sharpe  \\\n",
      "0        28.370841               33.601245         0.294316  1.783569   \n",
      "\n",
      "     Sortino  Maximum Drawdown  \n",
      "0  45.097287           0.03555   \n",
      "\n",
      "              0\n",
      "2244  33.613517\n",
      "2245  33.613734\n",
      "2246  33.608559\n",
      "2247  33.608833\n",
      "2248  33.601245\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).agent._optimizer.iter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).agent._optimizer.iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).agent._optimizer.beta_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).agent._optimizer.beta_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).agent._optimizer.beta_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).agent._optimizer.beta_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).agent._optimizer.decay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).agent._optimizer.decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).agent._optimizer.learning_rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).agent._optimizer.learning_rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   average_returns  Cumulative Log Returns  Investment Risk    Sharpe  \\\n",
      "0        17.658513               26.034626         0.349792  1.609989   \n",
      "\n",
      "    Sortino  Maximum Drawdown  \n",
      "0  17.91851          0.230554   \n",
      "\n",
      "              0\n",
      "2244  26.034626\n",
      "2245  26.034626\n",
      "2246  26.034626\n",
      "2247  26.034626\n",
      "2248  26.034626\n",
      "   average_returns  Cumulative Log Returns  Investment Risk    Sharpe  \\\n",
      "0        37.158294               38.576688         0.330631  1.702091   \n",
      "\n",
      "     Sortino  Maximum Drawdown  \n",
      "0  80.101707            0.5823   \n",
      "\n",
      "              0\n",
      "2244  38.572936\n",
      "2245  38.566560\n",
      "2246  38.564796\n",
      "2247  38.566802\n",
      "2248  38.576688\n",
      "   average_returns  Cumulative Log Returns  Investment Risk    Sharpe  \\\n",
      "0        12.706338               24.789818         0.344731  1.588669   \n",
      "\n",
      "    Sortino  Maximum Drawdown  \n",
      "0  17.03119          0.160215   \n",
      "\n",
      "              0\n",
      "2244  24.759789\n",
      "2245  24.759789\n",
      "2246  24.774006\n",
      "2247  24.793225\n",
      "2248  24.789818\n"
     ]
    }
   ],
   "source": [
    "for agent_name, agent_config in agent_dict.items():\n",
    "    for dataset_name, dataset_filepath in datasets_dict.items():\n",
    "        eval_env, tf_eval_env = build_eval_environments(\n",
    "            dataset_filepath=dataset_filepath,\n",
    "            reward_fn_instance=MarketLimitOrderRF,\n",
    "            **env_dict\n",
    "        )\n",
    "        tradernet = build_agent(\n",
    "            env=tf_eval_env,\n",
    "            checkpoint_filepath=f'database/storage/checkpoints/experiments/tradernet/{agent_name}/{dataset_name}/{reward_fn_name}/',\n",
    "            **agent_config\n",
    "        )\n",
    "\n",
    "        smurf_agent = build_agent(\n",
    "            env=tf_eval_env,\n",
    "            checkpoint_filepath=f'database/storage/checkpoints/experiments/smurf/{agent_name}/{dataset_name}/{reward_fn_name}/',\n",
    "            **agent_config\n",
    "        )\n",
    "        average_returns, pnls = eval_tradernet_smurf(\n",
    "            tradernet_policy=tradernet.policy,\n",
    "            smurf_policy=smurf_agent.policy,\n",
    "            tf_env_wrapper=tf_eval_env\n",
    "        )\n",
    "        metrics = {\n",
    "            'average_returns': [average_returns],\n",
    "            **{key: [metric] for key, metric in eval_env.get_episode_metrics().items()}\n",
    "        }\n",
    "        results_df = pd.DataFrame(metrics)\n",
    "        results_df.to_csv(f'experiments/smurf/{agent_name}/{dataset_name}_{reward_fn_name}_metrics.csv', index=False)\n",
    "\n",
    "        print(results_df, '\\n')\n",
    "\n",
    "        episode_pnls_df = pd.DataFrame(pnls)\n",
    "        episode_pnls_df.to_csv(f'experiments/smurf/{agent_name}/{dataset_name}_{reward_fn_name}_eval_cumul_pnls.csv', index=False)\n",
    "\n",
    "        print(episode_pnls_df.tail(5))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}