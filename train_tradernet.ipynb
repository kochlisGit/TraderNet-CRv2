{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Importing Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import config\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from agents.tfagents.dqn import DQNAgent\n",
    "from agents.tfagents.ppo import PPOAgent\n",
    "from environments.environment import TradingEnvironment\n",
    "from environments.wrappers.tf.tfenv import TFTradingEnvironment\n",
    "from environments.rewards.marketorder import MarketOrderRF\n",
    "from environments.rewards.marketlimitorder import MarketLimitOrderRF\n",
    "from metrics.trading.pnl import CumulativeLogReturn\n",
    "from metrics.trading.risk import InvestmentRisk\n",
    "from metrics.trading.sharpe import SharpeRatio\n",
    "from metrics.trading.sortino import SortinoRatio\n",
    "from metrics.trading.drawdown import MaximumDrawdown"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reading Datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def read_dataset(\n",
    "        dataset_filepath,\n",
    "        timeframe_size,\n",
    "        target_horizon_len,\n",
    "        num_eval_samples,\n",
    "        fees,\n",
    "        reward_fn_instance\n",
    "):\n",
    "    # Reading dataset\n",
    "    crypto_dataset_df = pd.read_csv(config.dataset_save_filepath.format(dataset_filepath))\n",
    "    samples_df = crypto_dataset_df[config.regression_features]\n",
    "\n",
    "    # Scaling data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1.0))\n",
    "    samples = samples_df.to_numpy(dtype=np.float32)\n",
    "\n",
    "    num_train_scale_samples = samples.shape[0] - num_eval_samples - target_horizon_len - timeframe_size + 1\n",
    "    samples[: num_train_scale_samples] = scaler.fit_transform(samples[: num_train_scale_samples])\n",
    "    samples[num_train_scale_samples: ] = scaler.transform(samples[num_train_scale_samples: ])\n",
    "\n",
    "    # Constructing timeframes for train, test\n",
    "    inputs = np.float32([samples[i: i + timeframe_size] for i in range(samples.shape[0] - timeframe_size - target_horizon_len + 1)])\n",
    "\n",
    "    # Splitting inputs to train-test data\n",
    "    num_train_inputs = inputs.shape[0] - num_eval_samples\n",
    "    x_train = inputs[: num_train_inputs]\n",
    "    x_eval = inputs[num_train_inputs:]\n",
    "\n",
    "    # Computing reward functions for train, test data\n",
    "    closes = crypto_dataset_df['close'].to_numpy(dtype=np.float32)\n",
    "    highs = crypto_dataset_df['high'].to_numpy(dtype=np.float32)\n",
    "    lows = crypto_dataset_df['low'].to_numpy(dtype=np.float32)\n",
    "\n",
    "    train_reward_fn = reward_fn_instance(\n",
    "        timeframe_size=timeframe_size,\n",
    "        target_horizon_len=target_horizon_len,\n",
    "        highs=highs[: samples.shape[0] - num_eval_samples],\n",
    "        lows=lows[: samples.shape[0] - num_eval_samples],\n",
    "        closes=closes[: samples.shape[0] - num_eval_samples],\n",
    "        fees_percentage=fees\n",
    "    )\n",
    "\n",
    "    eval_reward_fn = reward_fn_instance(\n",
    "        timeframe_size=timeframe_size,\n",
    "        target_horizon_len=target_horizon_len,\n",
    "        highs=highs[samples.shape[0] - num_eval_samples - timeframe_size - target_horizon_len + 1:],\n",
    "        lows=lows[samples.shape[0] - num_eval_samples - timeframe_size - target_horizon_len + 1:],\n",
    "        closes=closes[samples.shape[0] - num_eval_samples - timeframe_size - target_horizon_len + 1:],\n",
    "        fees_percentage=fees\n",
    "    )\n",
    "\n",
    "    assert x_train.shape[0] == train_reward_fn.get_reward_fn_shape()[0], \\\n",
    "        f'AssertionError: DimensionMismatch: x_train: {x_train.shape}, train_reward_fn: {train_reward_fn.get_reward_fn_shape()}'\n",
    "    assert x_eval.shape[0] == eval_reward_fn.get_reward_fn_shape()[0], \\\n",
    "        f'AssertionError: DimensionMismatch: x_eval: {x_eval.shape}, eval_reward_fn: {eval_reward_fn.get_reward_fn_shape()}'\n",
    "\n",
    "    return x_train, train_reward_fn, x_eval, eval_reward_fn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building Agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def build_agent(\n",
    "        agent_instance,\n",
    "        observation_spec,\n",
    "        action_spec,\n",
    "        time_step_spec,\n",
    "        env_batch_size,\n",
    "        checkpoint_filepath,\n",
    "        fc_layers,\n",
    "        conv_layers\n",
    "):\n",
    "    return agent_instance(\n",
    "        input_tensor_spec=observation_spec,\n",
    "        action_spec=action_spec,\n",
    "        time_step_spec=time_step_spec,\n",
    "        env_batch_size=env_batch_size,\n",
    "        checkpoint_filepath=checkpoint_filepath,\n",
    "        fc_layers=fc_layers,\n",
    "        conv_layers=conv_layers\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building Trainer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def train(\n",
    "        dataset_filepath,\n",
    "        timeframe_size,\n",
    "        target_horizon_len,\n",
    "        num_eval_samples,\n",
    "        fees,\n",
    "        reward_fn_instance,\n",
    "        agent_instance,\n",
    "        checkpoint_filepath,\n",
    "        fc_layers,\n",
    "        conv_layers,\n",
    "        train_episode_steps,\n",
    "        train_iterations,\n",
    "        eval_episodes,\n",
    "        steps_per_eval,\n",
    "        steps_per_log,\n",
    "        steps_per_checkpoint,\n",
    "        save_best_only\n",
    "):\n",
    "    x_train, train_reward_fn, x_eval, eval_reward_fn = read_dataset(\n",
    "        dataset_filepath=dataset_filepath,\n",
    "        timeframe_size=timeframe_size,\n",
    "        target_horizon_len=target_horizon_len,\n",
    "        num_eval_samples=num_eval_samples,\n",
    "        fees=fees,\n",
    "        reward_fn_instance=reward_fn_instance\n",
    "    )\n",
    "    train_env = TFTradingEnvironment(\n",
    "        env=TradingEnvironment(env_config={\n",
    "            'states': x_train,\n",
    "            'reward_fn': train_reward_fn,\n",
    "            'episode_steps': train_episode_steps,\n",
    "            'metrics': [CumulativeLogReturn(), InvestmentRisk(), SharpeRatio(), SortinoRatio(), MaximumDrawdown()]\n",
    "        })\n",
    "    )\n",
    "    eval_env = TFTradingEnvironment(\n",
    "        env=TradingEnvironment(env_config={\n",
    "            'states': x_eval,\n",
    "            'reward_fn': eval_reward_fn,\n",
    "            'episode_steps': x_eval.shape[0] - 1,\n",
    "            'metrics': [CumulativeLogReturn(), InvestmentRisk(), SharpeRatio(), SortinoRatio(), MaximumDrawdown()]\n",
    "        })\n",
    "    )\n",
    "\n",
    "    tf_train_env = TFPyEnvironment(environment=train_env)\n",
    "    tf_eval_env = TFPyEnvironment(environment=eval_env)\n",
    "\n",
    "    agent = build_agent(\n",
    "        agent_instance=agent_instance,\n",
    "        observation_spec=tf_train_env.observation_spec(),\n",
    "        action_spec=tf_train_env.action_spec(),\n",
    "        time_step_spec=tf_train_env.time_step_spec(),\n",
    "        env_batch_size=tf_train_env.batch_size,\n",
    "        checkpoint_filepath=checkpoint_filepath,\n",
    "        fc_layers=fc_layers,\n",
    "        conv_layers=conv_layers,\n",
    "    )\n",
    "\n",
    "    agent.initialize()\n",
    "\n",
    "    eval_avg_returns = agent.train(\n",
    "        train_env=tf_train_env,\n",
    "        eval_env=tf_eval_env,\n",
    "        train_iterations=train_iterations,\n",
    "        eval_episodes=eval_episodes,\n",
    "        iterations_per_eval=steps_per_eval,\n",
    "        iterations_per_log=steps_per_log,\n",
    "        iterations_per_checkpoint=steps_per_checkpoint,\n",
    "        save_best_only=save_best_only\n",
    "    )\n",
    "    eval_metrics = eval_env.get_metrics()\n",
    "    return eval_avg_returns, eval_metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building Train Configs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "datasets_dict = {'BTC': 'BTC', 'ETH': 'ETH', 'ADA': 'ADA', 'XRP': 'XRP', 'LTC': 'LTC'}\n",
    "rewards_dict = {\n",
    "    'Market-Orders':  MarketOrderRF,\n",
    "    'Market-Limit Orders': MarketLimitOrderRF\n",
    "}\n",
    "agents_configs = {\n",
    "    'DDQN': {\n",
    "        'agent_instance': DQNAgent,\n",
    "        'train_iterations': 50000,\n",
    "        'steps_per_eval': 500,\n",
    "        'steps_per_log': 500,\n",
    "        'steps_per_checkpoint': 500\n",
    "    },\n",
    "    'PPO': {\n",
    "        'agent_instance': PPOAgent,\n",
    "        'train_iterations': 1000,\n",
    "        'steps_per_eval': 10,\n",
    "        'steps_per_log': 10,\n",
    "        'steps_per_checkpoint': 10\n",
    "    }\n",
    "}\n",
    "train_dict = {\n",
    "    'timeframe_size': 12,\n",
    "    'target_horizon_len': 20,\n",
    "    'num_eval_samples': 2250,\n",
    "    'fees': 0.007,\n",
    "    'fc_layers': [256, 256],\n",
    "    'conv_layers': [(32, 3, 1)],\n",
    "    'train_episode_steps': 100,\n",
    "    'eval_episodes': 1,\n",
    "    'save_best_only': True\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run TraderNet Experiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Initial Samples...\n",
      "Training has started...\n",
      "\n",
      "New best average return found at -3.3251941204071045! Saving checkpoint at iteration 0\n",
      "WARNING:tensorflow:From C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best average return found at -2.6267342567443848! Saving checkpoint at iteration 500\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 0.004517136141657829\n",
      "Average Return: -2.6267342567443848\n",
      "\n",
      "New best average return found at -2.1988868713378906! Saving checkpoint at iteration 1000\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 0.002659676130861044\n",
      "Average Return: -2.1988868713378906\n",
      "\n",
      "New best average return found at -1.1928369998931885! Saving checkpoint at iteration 1500\n",
      "\n",
      "Iteration: 1500\n",
      "Train Loss: 0.00444418378174305\n",
      "Average Return: -1.1928369998931885\n",
      "\n",
      "Iteration: 2000\n",
      "Train Loss: 0.003950432874262333\n",
      "Average Return: -1.593271017074585\n",
      "\n",
      "New best average return found at -0.10673896968364716! Saving checkpoint at iteration 2500\n",
      "\n",
      "Iteration: 2500\n",
      "Train Loss: 0.003947971388697624\n",
      "Average Return: -0.10673896968364716\n",
      "\n",
      "Iteration: 3000\n",
      "Train Loss: 0.0027603469789028168\n",
      "Average Return: -6.610193252563477\n",
      "\n",
      "Iteration: 3500\n",
      "Train Loss: 0.003471020143479109\n",
      "Average Return: -0.64287930727005\n",
      "\n",
      "New best average return found at 0.26988711953163147! Saving checkpoint at iteration 4000\n",
      "\n",
      "Iteration: 4000\n",
      "Train Loss: 0.003284275531768799\n",
      "Average Return: 0.26988711953163147\n",
      "\n",
      "New best average return found at 1.4300651550292969! Saving checkpoint at iteration 4500\n",
      "\n",
      "Iteration: 4500\n",
      "Train Loss: 0.0018765713321045041\n",
      "Average Return: 1.4300651550292969\n",
      "\n",
      "Iteration: 5000\n",
      "Train Loss: 0.0030547953210771084\n",
      "Average Return: -1.8633091449737549\n",
      "\n",
      "Iteration: 5500\n",
      "Train Loss: 0.00371134327724576\n",
      "Average Return: 0.49418583512306213\n",
      "\n",
      "Iteration: 6000\n",
      "Train Loss: 0.002536296145990491\n",
      "Average Return: 1.0524494647979736\n",
      "\n",
      "Iteration: 6500\n",
      "Train Loss: 0.0034819324500858784\n",
      "Average Return: 0.44075900316238403\n",
      "\n",
      "Iteration: 7000\n",
      "Train Loss: 0.002485145814716816\n",
      "Average Return: 1.4166224002838135\n",
      "\n",
      "Iteration: 7500\n",
      "Train Loss: 0.004277217201888561\n",
      "Average Return: 1.2338757514953613\n",
      "\n",
      "Iteration: 8000\n",
      "Train Loss: 0.005150902085006237\n",
      "Average Return: -0.5476500391960144\n",
      "\n",
      "New best average return found at 1.5128898620605469! Saving checkpoint at iteration 8500\n",
      "\n",
      "Iteration: 8500\n",
      "Train Loss: 0.0056409165263175964\n",
      "Average Return: 1.5128898620605469\n",
      "\n",
      "Iteration: 9000\n",
      "Train Loss: 0.0023802167270332575\n",
      "Average Return: -0.9223018288612366\n",
      "\n",
      "Iteration: 9500\n",
      "Train Loss: 0.008540058508515358\n",
      "Average Return: -0.5625291466712952\n",
      "\n",
      "New best average return found at 1.8649308681488037! Saving checkpoint at iteration 10000\n",
      "\n",
      "Iteration: 10000\n",
      "Train Loss: 0.003588468302041292\n",
      "Average Return: 1.8649308681488037\n",
      "\n",
      "Iteration: 10500\n",
      "Train Loss: 0.004626028705388308\n",
      "Average Return: -0.35136136412620544\n",
      "\n",
      "Iteration: 11000\n",
      "Train Loss: 0.00627045938745141\n",
      "Average Return: -1.1136596202850342\n",
      "\n",
      "Iteration: 11500\n",
      "Train Loss: 0.004262169823050499\n",
      "Average Return: -3.957099199295044\n",
      "\n",
      "Iteration: 12000\n",
      "Train Loss: 0.00608963705599308\n",
      "Average Return: -10.1170654296875\n",
      "\n",
      "Iteration: 12500\n",
      "Train Loss: 0.0029133770149201155\n",
      "Average Return: -2.486375093460083\n",
      "\n",
      "Iteration: 13000\n",
      "Train Loss: 0.007922138087451458\n",
      "Average Return: -2.122560977935791\n",
      "\n",
      "Iteration: 13500\n",
      "Train Loss: 0.003332233987748623\n",
      "Average Return: -0.35658806562423706\n",
      "\n",
      "Iteration: 14000\n",
      "Train Loss: 0.006915050558745861\n",
      "Average Return: -11.399165153503418\n",
      "\n",
      "Iteration: 14500\n",
      "Train Loss: 0.004967795219272375\n",
      "Average Return: -14.135310173034668\n",
      "\n",
      "Iteration: 15000\n",
      "Train Loss: 0.01010408066213131\n",
      "Average Return: 1.1594135761260986\n",
      "\n",
      "Iteration: 15500\n",
      "Train Loss: 0.005002512596547604\n",
      "Average Return: -8.289400100708008\n",
      "\n",
      "Iteration: 16000\n",
      "Train Loss: 0.004671748261898756\n",
      "Average Return: 1.1563522815704346\n",
      "\n",
      "New best average return found at 1.9630203247070312! Saving checkpoint at iteration 16500\n",
      "\n",
      "Iteration: 16500\n",
      "Train Loss: 0.004587126430124044\n",
      "Average Return: 1.9630203247070312\n",
      "\n",
      "Iteration: 17000\n",
      "Train Loss: 0.00824192725121975\n",
      "Average Return: -3.3367693424224854\n",
      "\n",
      "Iteration: 17500\n",
      "Train Loss: 0.007104896008968353\n",
      "Average Return: -1.5858206748962402\n",
      "\n",
      "Iteration: 18000\n",
      "Train Loss: 0.004175573121756315\n",
      "Average Return: 0.23090189695358276\n",
      "\n",
      "Iteration: 18500\n",
      "Train Loss: 0.007864575833082199\n",
      "Average Return: 0.12447400391101837\n",
      "\n",
      "Iteration: 19000\n",
      "Train Loss: 0.009032703936100006\n",
      "Average Return: 0.5437994599342346\n",
      "\n",
      "Iteration: 19500\n",
      "Train Loss: 0.012586879543960094\n",
      "Average Return: -4.941676139831543\n",
      "\n",
      "Iteration: 20000\n",
      "Train Loss: 0.010752459056675434\n",
      "Average Return: -0.7246016263961792\n",
      "\n",
      "New best average return found at 2.3755111694335938! Saving checkpoint at iteration 20500\n",
      "\n",
      "Iteration: 20500\n",
      "Train Loss: 0.010837195441126823\n",
      "Average Return: 2.3755111694335938\n",
      "\n",
      "Iteration: 21000\n",
      "Train Loss: 0.0115090012550354\n",
      "Average Return: 0.4572712481021881\n",
      "\n",
      "Iteration: 21500\n",
      "Train Loss: 0.00458418857306242\n",
      "Average Return: 1.5899443626403809\n",
      "\n",
      "Iteration: 22000\n",
      "Train Loss: 0.00946071743965149\n",
      "Average Return: 0.08347630500793457\n",
      "\n",
      "Iteration: 22500\n",
      "Train Loss: 0.013713662512600422\n",
      "Average Return: -0.5114959478378296\n",
      "\n",
      "Iteration: 23000\n",
      "Train Loss: 0.014691232703626156\n",
      "Average Return: 1.1496012210845947\n",
      "\n",
      "Iteration: 23500\n",
      "Train Loss: 0.013218051753938198\n",
      "Average Return: 0.46198052167892456\n",
      "\n",
      "Iteration: 24000\n",
      "Train Loss: 0.012148733250796795\n",
      "Average Return: -1.7066423892974854\n",
      "\n",
      "Iteration: 24500\n",
      "Train Loss: 0.023540057241916656\n",
      "Average Return: -0.054105695337057114\n",
      "\n",
      "Iteration: 25000\n",
      "Train Loss: 0.003517645876854658\n",
      "Average Return: -0.42419686913490295\n",
      "\n",
      "Iteration: 25500\n",
      "Train Loss: 0.005718512460589409\n",
      "Average Return: -1.772891640663147\n",
      "\n",
      "Iteration: 26000\n",
      "Train Loss: 0.010224609635770321\n",
      "Average Return: -1.3455853462219238\n",
      "\n",
      "Iteration: 26500\n",
      "Train Loss: 0.0074816858395934105\n",
      "Average Return: -0.46244028210639954\n",
      "\n",
      "Iteration: 27000\n",
      "Train Loss: 0.007197184022516012\n",
      "Average Return: -1.8270294666290283\n",
      "\n",
      "Iteration: 27500\n",
      "Train Loss: 0.02825900726020336\n",
      "Average Return: -0.9843820929527283\n",
      "\n",
      "Iteration: 28000\n",
      "Train Loss: 0.005171783268451691\n",
      "Average Return: -5.722950458526611\n",
      "\n",
      "Iteration: 28500\n",
      "Train Loss: 0.0060277655720710754\n",
      "Average Return: -3.08048152923584\n",
      "\n",
      "Iteration: 29000\n",
      "Train Loss: 0.01661021076142788\n",
      "Average Return: -1.9312208890914917\n",
      "\n",
      "Iteration: 29500\n",
      "Train Loss: 0.004864417016506195\n",
      "Average Return: -2.5246658325195312\n",
      "\n",
      "Iteration: 30000\n",
      "Train Loss: 0.004200350493192673\n",
      "Average Return: -0.489668071269989\n",
      "\n",
      "Iteration: 30500\n",
      "Train Loss: 0.007931552827358246\n",
      "Average Return: -2.468716859817505\n",
      "\n",
      "Iteration: 31000\n",
      "Train Loss: 0.006465802900493145\n",
      "Average Return: -2.462980270385742\n",
      "\n",
      "Iteration: 31500\n",
      "Train Loss: 0.007899109274148941\n",
      "Average Return: -0.8135148286819458\n",
      "\n",
      "Iteration: 32000\n",
      "Train Loss: 0.005674577318131924\n",
      "Average Return: -0.7152639031410217\n",
      "\n",
      "Iteration: 32500\n",
      "Train Loss: 0.004810309503227472\n",
      "Average Return: 0.3539341390132904\n",
      "\n",
      "Iteration: 33000\n",
      "Train Loss: 0.012381871230900288\n",
      "Average Return: -0.3432334363460541\n",
      "\n",
      "Iteration: 33500\n",
      "Train Loss: 0.013729281723499298\n",
      "Average Return: -1.2974762916564941\n",
      "\n",
      "Iteration: 34000\n",
      "Train Loss: 0.007587028201669455\n",
      "Average Return: -2.5157175064086914\n",
      "\n",
      "Iteration: 34500\n",
      "Train Loss: 0.01033468171954155\n",
      "Average Return: -2.36543607711792\n",
      "\n",
      "Iteration: 35000\n",
      "Train Loss: 0.013409886509180069\n",
      "Average Return: -0.6903045177459717\n",
      "\n",
      "Iteration: 35500\n",
      "Train Loss: 0.017461679875850677\n",
      "Average Return: -2.63220477104187\n",
      "\n",
      "Iteration: 36000\n",
      "Train Loss: 0.015354245901107788\n",
      "Average Return: -1.08450186252594\n",
      "\n",
      "Iteration: 36500\n",
      "Train Loss: 0.0082017183303833\n",
      "Average Return: -1.9765955209732056\n",
      "\n",
      "Iteration: 37000\n",
      "Train Loss: 0.011532722041010857\n",
      "Average Return: -1.2612545490264893\n",
      "\n",
      "Iteration: 37500\n",
      "Train Loss: 0.008413244970142841\n",
      "Average Return: -2.108757734298706\n",
      "\n",
      "Iteration: 38000\n",
      "Train Loss: 0.021756816655397415\n",
      "Average Return: -5.255423545837402\n",
      "\n",
      "Iteration: 38500\n",
      "Train Loss: 0.007302701007574797\n",
      "Average Return: -2.77022385597229\n",
      "\n",
      "Iteration: 39000\n",
      "Train Loss: 0.016087837517261505\n",
      "Average Return: -5.5408830642700195\n",
      "\n",
      "Iteration: 39500\n",
      "Train Loss: 0.020783226937055588\n",
      "Average Return: -3.603827476501465\n",
      "\n",
      "Iteration: 40000\n",
      "Train Loss: 0.01782047003507614\n",
      "Average Return: -5.380031108856201\n",
      "\n",
      "Iteration: 40500\n",
      "Train Loss: 0.010788587853312492\n",
      "Average Return: -3.5818371772766113\n",
      "\n",
      "Iteration: 41000\n",
      "Train Loss: 0.010497240349650383\n",
      "Average Return: -5.797184467315674\n",
      "\n",
      "Iteration: 41500\n",
      "Train Loss: 0.03236836567521095\n",
      "Average Return: -4.907263278961182\n",
      "\n",
      "Iteration: 42000\n",
      "Train Loss: 0.006485805846750736\n",
      "Average Return: -8.726724624633789\n",
      "\n",
      "Iteration: 42500\n",
      "Train Loss: 0.02130240947008133\n",
      "Average Return: -7.696349620819092\n",
      "\n",
      "Iteration: 43000\n",
      "Train Loss: 0.01232011616230011\n",
      "Average Return: -2.9162001609802246\n",
      "\n",
      "Iteration: 43500\n",
      "Train Loss: 0.014777308329939842\n",
      "Average Return: -2.140463352203369\n",
      "\n",
      "Iteration: 44000\n",
      "Train Loss: 0.011308260262012482\n",
      "Average Return: -3.332646369934082\n",
      "\n",
      "Iteration: 44500\n",
      "Train Loss: 0.009752931073307991\n",
      "Average Return: -3.935310125350952\n",
      "\n",
      "Iteration: 45000\n",
      "Train Loss: 0.020609542727470398\n",
      "Average Return: -1.8367596864700317\n",
      "\n",
      "Iteration: 45500\n",
      "Train Loss: 0.015398784540593624\n",
      "Average Return: -2.937124490737915\n",
      "\n",
      "Iteration: 46000\n",
      "Train Loss: 0.017583733424544334\n",
      "Average Return: -4.769311904907227\n",
      "\n",
      "Iteration: 46500\n",
      "Train Loss: 0.022816192358732224\n",
      "Average Return: -1.8276519775390625\n",
      "\n",
      "Iteration: 47000\n",
      "Train Loss: 0.015431247651576996\n",
      "Average Return: -1.282228946685791\n",
      "\n",
      "Iteration: 47500\n",
      "Train Loss: 0.019705310463905334\n",
      "Average Return: -3.7647857666015625\n",
      "\n",
      "Iteration: 48000\n",
      "Train Loss: 0.008198021911084652\n",
      "Average Return: -5.548557758331299\n",
      "\n",
      "Iteration: 48500\n",
      "Train Loss: 0.024069657549262047\n",
      "Average Return: -4.326942443847656\n",
      "\n",
      "Iteration: 49000\n",
      "Train Loss: 0.013101810589432716\n",
      "Average Return: -5.044973373413086\n",
      "\n",
      "Iteration: 49500\n",
      "Train Loss: 0.014290663413703442\n",
      "Average Return: -2.279237985610962\n",
      "\n",
      "Iteration: 50000\n",
      "Train Loss: 0.006985205225646496\n",
      "Average Return: -0.6277677416801453\n",
      "Collecting Initial Samples...\n",
      "Training has started...\n",
      "\n",
      "New best average return found at 7.247411251068115! Saving checkpoint at iteration 0\n",
      "\n",
      "New best average return found at 9.204045295715332! Saving checkpoint at iteration 500\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 0.005582769401371479\n",
      "Average Return: 9.204045295715332\n",
      "\n",
      "New best average return found at 12.120357513427734! Saving checkpoint at iteration 1000\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 0.005061245523393154\n",
      "Average Return: 12.120357513427734\n",
      "\n",
      "New best average return found at 12.279134750366211! Saving checkpoint at iteration 1500\n",
      "\n",
      "Iteration: 1500\n",
      "Train Loss: 0.00642402283847332\n",
      "Average Return: 12.279134750366211\n",
      "\n",
      "Iteration: 2000\n",
      "Train Loss: 0.007850215770304203\n",
      "Average Return: 9.512455940246582\n",
      "\n",
      "Iteration: 2500\n",
      "Train Loss: 0.008758891373872757\n",
      "Average Return: 11.876684188842773\n",
      "\n",
      "Iteration: 3000\n",
      "Train Loss: 0.007652045227587223\n",
      "Average Return: 1.3607683181762695\n",
      "\n",
      "Iteration: 3500\n",
      "Train Loss: 0.007501585874706507\n",
      "Average Return: 11.411060333251953\n",
      "\n",
      "Iteration: 4000\n",
      "Train Loss: 0.008439733646810055\n",
      "Average Return: 10.535136222839355\n",
      "\n",
      "New best average return found at 12.423233032226562! Saving checkpoint at iteration 4500\n",
      "\n",
      "Iteration: 4500\n",
      "Train Loss: 0.006422798149287701\n",
      "Average Return: 12.423233032226562\n",
      "\n",
      "Iteration: 5000\n",
      "Train Loss: 0.007994737476110458\n",
      "Average Return: 8.659591674804688\n",
      "\n",
      "Iteration: 5500\n",
      "Train Loss: 0.010526640340685844\n",
      "Average Return: 12.232836723327637\n",
      "\n",
      "Iteration: 6000\n",
      "Train Loss: 0.15210716426372528\n",
      "Average Return: 11.598977088928223\n",
      "\n",
      "Iteration: 6500\n",
      "Train Loss: 0.011112602427601814\n",
      "Average Return: 7.666211128234863\n",
      "\n",
      "Iteration: 7000\n",
      "Train Loss: 0.007416301406919956\n",
      "Average Return: 11.905735969543457\n",
      "\n",
      "Iteration: 7500\n",
      "Train Loss: 0.006612963508814573\n",
      "Average Return: 8.55688190460205\n",
      "\n",
      "Iteration: 8000\n",
      "Train Loss: 0.013625167310237885\n",
      "Average Return: 9.138694763183594\n",
      "\n",
      "Iteration: 8500\n",
      "Train Loss: 0.018736936151981354\n",
      "Average Return: 11.858162879943848\n",
      "\n",
      "Iteration: 9000\n",
      "Train Loss: 0.006755776237696409\n",
      "Average Return: 12.142813682556152\n",
      "\n",
      "Iteration: 9500\n",
      "Train Loss: 0.013840906322002411\n",
      "Average Return: 12.24528980255127\n",
      "\n",
      "Iteration: 10000\n",
      "Train Loss: 0.00955360010266304\n",
      "Average Return: 12.117239952087402\n",
      "\n",
      "New best average return found at 12.883391380310059! Saving checkpoint at iteration 10500\n",
      "\n",
      "Iteration: 10500\n",
      "Train Loss: 0.010460149496793747\n",
      "Average Return: 12.883391380310059\n",
      "\n",
      "Iteration: 11000\n",
      "Train Loss: 0.023001786321401596\n",
      "Average Return: 11.437911033630371\n",
      "\n",
      "New best average return found at 13.630852699279785! Saving checkpoint at iteration 11500\n",
      "\n",
      "Iteration: 11500\n",
      "Train Loss: 0.008777104318141937\n",
      "Average Return: 13.630852699279785\n",
      "\n",
      "Iteration: 12000\n",
      "Train Loss: 0.00837338250130415\n",
      "Average Return: 9.128414154052734\n",
      "\n",
      "Iteration: 12500\n",
      "Train Loss: 0.009240513667464256\n",
      "Average Return: 7.787055015563965\n",
      "\n",
      "Iteration: 13000\n",
      "Train Loss: 0.018833491951227188\n",
      "Average Return: 11.12488079071045\n",
      "\n",
      "Iteration: 13500\n",
      "Train Loss: 0.013441042974591255\n",
      "Average Return: 12.451764106750488\n",
      "\n",
      "Iteration: 14000\n",
      "Train Loss: 0.015859566628932953\n",
      "Average Return: 1.976043939590454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\metrics\\trading\\sortino.py:20: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.exp(average_returns/std_downfall_returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 14500\n",
      "Train Loss: 0.00968550518155098\n",
      "Average Return: 4.744236946105957\n",
      "\n",
      "Iteration: 15000\n",
      "Train Loss: 0.030995724722743034\n",
      "Average Return: 12.404884338378906\n",
      "\n",
      "Iteration: 15500\n",
      "Train Loss: 0.01618315279483795\n",
      "Average Return: 11.38774299621582\n",
      "\n",
      "Iteration: 16000\n",
      "Train Loss: 0.017214620485901833\n",
      "Average Return: 12.028968811035156\n",
      "\n",
      "Iteration: 16500\n",
      "Train Loss: 0.020484738051891327\n",
      "Average Return: 11.729836463928223\n",
      "\n",
      "Iteration: 17000\n",
      "Train Loss: 0.021418271586298943\n",
      "Average Return: 9.963153839111328\n",
      "\n",
      "Iteration: 17500\n",
      "Train Loss: 0.020487137138843536\n",
      "Average Return: 8.861655235290527\n",
      "\n",
      "Iteration: 18000\n",
      "Train Loss: 0.017623163759708405\n",
      "Average Return: 9.623153686523438\n",
      "\n",
      "Iteration: 18500\n",
      "Train Loss: 0.316775918006897\n",
      "Average Return: 11.739766120910645\n",
      "\n",
      "Iteration: 19000\n",
      "Train Loss: 0.03245686739683151\n",
      "Average Return: 12.01941967010498\n",
      "\n",
      "Iteration: 19500\n",
      "Train Loss: 0.03994602710008621\n",
      "Average Return: 9.00653076171875\n",
      "\n",
      "Iteration: 20000\n",
      "Train Loss: 0.04353069141507149\n",
      "Average Return: 0.5452691912651062\n",
      "\n",
      "Iteration: 20500\n",
      "Train Loss: 0.020341385155916214\n",
      "Average Return: 11.772904396057129\n",
      "\n",
      "Iteration: 21000\n",
      "Train Loss: 0.024094948545098305\n",
      "Average Return: 10.159093856811523\n",
      "\n",
      "Iteration: 21500\n",
      "Train Loss: 0.024611838161945343\n",
      "Average Return: 8.910920143127441\n",
      "\n",
      "Iteration: 22000\n",
      "Train Loss: 0.017912201583385468\n",
      "Average Return: 10.058998107910156\n",
      "\n",
      "Iteration: 22500\n",
      "Train Loss: 0.018720123916864395\n",
      "Average Return: 4.7183027267456055\n",
      "\n",
      "Iteration: 23000\n",
      "Train Loss: 0.028660427778959274\n",
      "Average Return: 7.323130130767822\n",
      "\n",
      "Iteration: 23500\n",
      "Train Loss: 0.037774454802274704\n",
      "Average Return: 8.57011890411377\n",
      "\n",
      "Iteration: 24000\n",
      "Train Loss: 0.04323359206318855\n",
      "Average Return: 6.900426387786865\n",
      "\n",
      "Iteration: 24500\n",
      "Train Loss: 0.05633158981800079\n",
      "Average Return: 7.78210973739624\n",
      "\n",
      "Iteration: 25000\n",
      "Train Loss: 0.00922180525958538\n",
      "Average Return: 9.390490531921387\n",
      "\n",
      "Iteration: 25500\n",
      "Train Loss: 0.019764812663197517\n",
      "Average Return: 9.54084300994873\n",
      "\n",
      "Iteration: 26000\n",
      "Train Loss: 0.03538276255130768\n",
      "Average Return: 6.512945175170898\n",
      "\n",
      "Iteration: 26500\n",
      "Train Loss: 0.3223232328891754\n",
      "Average Return: 7.949275016784668\n",
      "\n",
      "Iteration: 27000\n",
      "Train Loss: 0.015685340389609337\n",
      "Average Return: 4.8543500900268555\n",
      "\n",
      "Iteration: 27500\n",
      "Train Loss: 0.0766310915350914\n",
      "Average Return: 6.675524711608887\n",
      "\n",
      "Iteration: 28000\n",
      "Train Loss: 0.017287112772464752\n",
      "Average Return: 4.684099197387695\n",
      "\n",
      "Iteration: 28500\n",
      "Train Loss: 0.020136762410402298\n",
      "Average Return: 7.738862991333008\n",
      "\n",
      "Iteration: 29000\n",
      "Train Loss: 0.05108971893787384\n",
      "Average Return: 5.2511515617370605\n",
      "\n",
      "Iteration: 29500\n",
      "Train Loss: 0.022519171237945557\n",
      "Average Return: 4.203627109527588\n",
      "\n",
      "Iteration: 30000\n",
      "Train Loss: 0.015880461782217026\n",
      "Average Return: 5.61973762512207\n",
      "\n",
      "Iteration: 30500\n",
      "Train Loss: 0.04162788763642311\n",
      "Average Return: 6.399942398071289\n",
      "\n",
      "Iteration: 31000\n",
      "Train Loss: 0.03331900015473366\n",
      "Average Return: 4.762518882751465\n",
      "\n",
      "Iteration: 31500\n",
      "Train Loss: 0.026435669511556625\n",
      "Average Return: 8.153903007507324\n",
      "\n",
      "Iteration: 32000\n",
      "Train Loss: 0.027647167444229126\n",
      "Average Return: 6.319290637969971\n",
      "\n",
      "Iteration: 32500\n",
      "Train Loss: 0.016084326431155205\n",
      "Average Return: 8.526905059814453\n",
      "\n",
      "Iteration: 33000\n",
      "Train Loss: 0.03247809037566185\n",
      "Average Return: 9.376204490661621\n",
      "\n",
      "Iteration: 33500\n",
      "Train Loss: 0.031172264367341995\n",
      "Average Return: 6.555168151855469\n",
      "\n",
      "Iteration: 34000\n",
      "Train Loss: 0.016574902459979057\n",
      "Average Return: 11.77291202545166\n",
      "\n",
      "Iteration: 34500\n",
      "Train Loss: 0.026708921417593956\n",
      "Average Return: 4.7952561378479\n",
      "\n",
      "Iteration: 35000\n",
      "Train Loss: 0.04502835124731064\n",
      "Average Return: 11.355709075927734\n",
      "\n",
      "Iteration: 35500\n",
      "Train Loss: 0.03238306939601898\n",
      "Average Return: 7.363356590270996\n",
      "\n",
      "Iteration: 36000\n",
      "Train Loss: 0.038777243345975876\n",
      "Average Return: 4.532904148101807\n",
      "\n",
      "Iteration: 36500\n",
      "Train Loss: 0.011689944192767143\n",
      "Average Return: 10.04805850982666\n",
      "\n",
      "Iteration: 37000\n",
      "Train Loss: 0.04932080954313278\n",
      "Average Return: 6.290106773376465\n",
      "\n",
      "Iteration: 37500\n",
      "Train Loss: 0.021538443863391876\n",
      "Average Return: 1.7772904634475708\n",
      "\n",
      "Iteration: 38000\n",
      "Train Loss: 0.02968655154109001\n",
      "Average Return: 4.594333648681641\n",
      "\n",
      "Iteration: 38500\n",
      "Train Loss: 0.018875133246183395\n",
      "Average Return: 7.261316776275635\n",
      "\n",
      "Iteration: 39000\n",
      "Train Loss: 0.05658266320824623\n",
      "Average Return: 6.333657741546631\n",
      "\n",
      "Iteration: 39500\n",
      "Train Loss: 0.05239170044660568\n",
      "Average Return: 3.1047656536102295\n",
      "\n",
      "Iteration: 40000\n",
      "Train Loss: 0.030168019235134125\n",
      "Average Return: 5.073600769042969\n",
      "\n",
      "Iteration: 40500\n",
      "Train Loss: 0.020374804735183716\n",
      "Average Return: 6.49639892578125\n",
      "\n",
      "Iteration: 41000\n",
      "Train Loss: 0.017074909061193466\n",
      "Average Return: 5.871684551239014\n",
      "\n",
      "Iteration: 41500\n",
      "Train Loss: 0.04812655597925186\n",
      "Average Return: 4.892786979675293\n",
      "\n",
      "Iteration: 42000\n",
      "Train Loss: 0.024866320192813873\n",
      "Average Return: 7.514442443847656\n",
      "\n",
      "Iteration: 42500\n",
      "Train Loss: 0.03513597697019577\n",
      "Average Return: 4.928811550140381\n",
      "\n",
      "Iteration: 43000\n",
      "Train Loss: 0.023227829486131668\n",
      "Average Return: 6.961268424987793\n",
      "\n",
      "Iteration: 43500\n",
      "Train Loss: 0.030017901211977005\n",
      "Average Return: 8.858549118041992\n",
      "\n",
      "Iteration: 44000\n",
      "Train Loss: 0.03587200492620468\n",
      "Average Return: 9.282783508300781\n",
      "\n",
      "Iteration: 44500\n",
      "Train Loss: 0.020499927923083305\n",
      "Average Return: 5.928642749786377\n",
      "\n",
      "Iteration: 45000\n",
      "Train Loss: 0.04103367030620575\n",
      "Average Return: 5.778047561645508\n",
      "\n",
      "Iteration: 45500\n",
      "Train Loss: 0.02187103033065796\n",
      "Average Return: -0.7324146032333374\n",
      "\n",
      "Iteration: 46000\n",
      "Train Loss: 0.04693891108036041\n",
      "Average Return: 4.431589603424072\n",
      "\n",
      "Iteration: 46500\n",
      "Train Loss: 0.034816086292266846\n",
      "Average Return: 5.105605602264404\n",
      "\n",
      "Iteration: 47000\n",
      "Train Loss: 0.041328106075525284\n",
      "Average Return: 6.987704277038574\n",
      "\n",
      "Iteration: 47500\n",
      "Train Loss: 0.037294451147317886\n",
      "Average Return: 7.428328514099121\n",
      "\n",
      "Iteration: 48000\n",
      "Train Loss: 0.020940832793712616\n",
      "Average Return: 7.5052924156188965\n",
      "\n",
      "Iteration: 48500\n",
      "Train Loss: 0.03710412606596947\n",
      "Average Return: 4.521782398223877\n",
      "\n",
      "Iteration: 49000\n",
      "Train Loss: 0.04354353994131088\n",
      "Average Return: 5.45110559463501\n",
      "\n",
      "Iteration: 49500\n",
      "Train Loss: 0.031865768134593964\n",
      "Average Return: 6.7708845138549805\n",
      "\n",
      "Iteration: 50000\n",
      "Train Loss: 0.01900496892631054\n",
      "Average Return: 8.199563026428223\n",
      "Collecting Initial Samples...\n",
      "Training has started...\n",
      "\n",
      "New best average return found at 12.004199981689453! Saving checkpoint at iteration 0\n",
      "\n",
      "New best average return found at 13.581841468811035! Saving checkpoint at iteration 500\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 0.008654411882162094\n",
      "Average Return: 13.581841468811035\n",
      "\n",
      "New best average return found at 13.976587295532227! Saving checkpoint at iteration 1000\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 0.005523160099983215\n",
      "Average Return: 13.976587295532227\n",
      "\n",
      "New best average return found at 16.004934310913086! Saving checkpoint at iteration 1500\n",
      "\n",
      "Iteration: 1500\n",
      "Train Loss: 0.0037000570446252823\n",
      "Average Return: 16.004934310913086\n",
      "\n",
      "Iteration: 2000\n",
      "Train Loss: 0.0021025114692747593\n",
      "Average Return: 14.186115264892578\n",
      "\n",
      "Iteration: 2500\n",
      "Train Loss: 0.004486399702727795\n",
      "Average Return: 12.872990608215332\n",
      "\n",
      "Iteration: 3000\n",
      "Train Loss: 0.0051362114027142525\n",
      "Average Return: 12.738874435424805\n",
      "\n",
      "Iteration: 3500\n",
      "Train Loss: 0.0062807705253362656\n",
      "Average Return: 15.101211547851562\n",
      "\n",
      "Iteration: 4000\n",
      "Train Loss: 0.009621855802834034\n",
      "Average Return: 14.049280166625977\n",
      "\n",
      "Iteration: 4500\n",
      "Train Loss: 0.0066278958693146706\n",
      "Average Return: 15.318187713623047\n",
      "\n",
      "Iteration: 5000\n",
      "Train Loss: 0.004941370338201523\n",
      "Average Return: 14.813423156738281\n",
      "\n",
      "New best average return found at 16.931243896484375! Saving checkpoint at iteration 5500\n",
      "\n",
      "Iteration: 5500\n",
      "Train Loss: 0.009635618887841702\n",
      "Average Return: 16.931243896484375\n",
      "\n",
      "New best average return found at 18.62547492980957! Saving checkpoint at iteration 6000\n",
      "\n",
      "Iteration: 6000\n",
      "Train Loss: 0.006504340097308159\n",
      "Average Return: 18.62547492980957\n",
      "\n",
      "Iteration: 6500\n",
      "Train Loss: 0.013679775409400463\n",
      "Average Return: 15.516317367553711\n",
      "\n",
      "New best average return found at 20.027151107788086! Saving checkpoint at iteration 7000\n",
      "\n",
      "Iteration: 7000\n",
      "Train Loss: 0.005218712612986565\n",
      "Average Return: 20.027151107788086\n",
      "\n",
      "Iteration: 7500\n",
      "Train Loss: 0.0054507507011294365\n",
      "Average Return: 15.73133373260498\n",
      "\n",
      "Iteration: 8000\n",
      "Train Loss: 0.011132996529340744\n",
      "Average Return: 15.604771614074707\n",
      "\n",
      "Iteration: 8500\n",
      "Train Loss: 0.009916726499795914\n",
      "Average Return: 18.865211486816406\n",
      "\n",
      "Iteration: 9000\n",
      "Train Loss: 0.005951304920017719\n",
      "Average Return: 19.6132755279541\n",
      "\n",
      "Iteration: 9500\n",
      "Train Loss: 0.011358387768268585\n",
      "Average Return: 15.675336837768555\n",
      "\n",
      "Iteration: 10000\n",
      "Train Loss: 0.008583550341427326\n",
      "Average Return: 14.708433151245117\n",
      "\n",
      "Iteration: 10500\n",
      "Train Loss: 0.007853814400732517\n",
      "Average Return: 16.98993682861328\n",
      "\n",
      "Iteration: 11000\n",
      "Train Loss: 0.01231043878942728\n",
      "Average Return: 18.205968856811523\n",
      "\n",
      "Iteration: 11500\n",
      "Train Loss: 0.008071965537965298\n",
      "Average Return: 15.191888809204102\n",
      "\n",
      "Iteration: 12000\n",
      "Train Loss: 0.007197813596576452\n",
      "Average Return: 13.143274307250977\n",
      "\n",
      "Iteration: 12500\n",
      "Train Loss: 0.007174902595579624\n",
      "Average Return: 15.95513916015625\n",
      "\n",
      "Iteration: 13000\n",
      "Train Loss: 0.0066867126151919365\n",
      "Average Return: 16.80101203918457\n",
      "\n",
      "Iteration: 13500\n",
      "Train Loss: 0.008649636059999466\n",
      "Average Return: 14.447735786437988\n",
      "\n",
      "Iteration: 14000\n",
      "Train Loss: 0.012838097289204597\n",
      "Average Return: 16.349929809570312\n",
      "\n",
      "Iteration: 14500\n",
      "Train Loss: 0.0069521828554570675\n",
      "Average Return: 18.328893661499023\n",
      "\n",
      "Iteration: 15000\n",
      "Train Loss: 0.011251090094447136\n",
      "Average Return: 15.094565391540527\n",
      "\n",
      "Iteration: 15500\n",
      "Train Loss: 0.018357187509536743\n",
      "Average Return: 16.905132293701172\n",
      "\n",
      "Iteration: 16000\n",
      "Train Loss: 0.01245716493576765\n",
      "Average Return: 14.603859901428223\n",
      "\n",
      "Iteration: 16500\n",
      "Train Loss: 0.016857177019119263\n",
      "Average Return: 5.787375450134277\n",
      "\n",
      "Iteration: 17000\n",
      "Train Loss: 0.025276608765125275\n",
      "Average Return: 14.938735008239746\n",
      "\n",
      "Iteration: 17500\n",
      "Train Loss: 0.011223819106817245\n",
      "Average Return: 17.62645149230957\n",
      "\n",
      "Iteration: 18000\n",
      "Train Loss: 0.007973255589604378\n",
      "Average Return: 14.922987937927246\n",
      "\n",
      "Iteration: 18500\n",
      "Train Loss: 0.022115524858236313\n",
      "Average Return: 9.582432746887207\n",
      "\n",
      "Iteration: 19000\n",
      "Train Loss: 0.015839338302612305\n",
      "Average Return: 16.05495262145996\n",
      "\n",
      "Iteration: 19500\n",
      "Train Loss: 0.0377720445394516\n",
      "Average Return: 14.175439834594727\n",
      "\n",
      "Iteration: 20000\n",
      "Train Loss: 0.017260748893022537\n",
      "Average Return: 14.520284652709961\n",
      "\n",
      "Iteration: 20500\n",
      "Train Loss: 0.016447342932224274\n",
      "Average Return: 12.689767837524414\n",
      "\n",
      "Iteration: 21000\n",
      "Train Loss: 0.012888381257653236\n",
      "Average Return: 14.848457336425781\n",
      "\n",
      "Iteration: 21500\n",
      "Train Loss: 0.00927185919135809\n",
      "Average Return: 15.138240814208984\n",
      "\n",
      "Iteration: 22000\n",
      "Train Loss: 0.015500502660870552\n",
      "Average Return: 10.825852394104004\n",
      "\n",
      "Iteration: 22500\n",
      "Train Loss: 0.020054884254932404\n",
      "Average Return: 14.243067741394043\n",
      "\n",
      "Iteration: 23000\n",
      "Train Loss: 0.013914660550653934\n",
      "Average Return: 13.852825164794922\n",
      "\n",
      "Iteration: 23500\n",
      "Train Loss: 0.023220403119921684\n",
      "Average Return: 12.345067977905273\n",
      "\n",
      "Iteration: 24000\n",
      "Train Loss: 0.023196332156658173\n",
      "Average Return: 2.4774510860443115\n",
      "\n",
      "Iteration: 24500\n",
      "Train Loss: 0.029171057045459747\n",
      "Average Return: 14.003418922424316\n",
      "\n",
      "Iteration: 25000\n",
      "Train Loss: 0.020042892545461655\n",
      "Average Return: 12.717952728271484\n",
      "\n",
      "Iteration: 25500\n",
      "Train Loss: 0.026535581797361374\n",
      "Average Return: 3.8874473571777344\n",
      "\n",
      "Iteration: 26000\n",
      "Train Loss: 0.014537905342876911\n",
      "Average Return: 13.924173355102539\n",
      "\n",
      "Iteration: 26500\n",
      "Train Loss: 0.027216043323278427\n",
      "Average Return: 15.037667274475098\n",
      "\n",
      "Iteration: 27000\n",
      "Train Loss: 0.024045689031481743\n",
      "Average Return: 13.264504432678223\n",
      "\n",
      "Iteration: 27500\n",
      "Train Loss: 0.03029712289571762\n",
      "Average Return: 10.8944673538208\n",
      "\n",
      "Iteration: 28000\n",
      "Train Loss: 0.0172587763518095\n",
      "Average Return: 15.894518852233887\n",
      "\n",
      "Iteration: 28500\n",
      "Train Loss: 0.014054393395781517\n",
      "Average Return: 11.708648681640625\n",
      "\n",
      "Iteration: 29000\n",
      "Train Loss: 0.028133966028690338\n",
      "Average Return: 15.05141830444336\n",
      "\n",
      "Iteration: 29500\n",
      "Train Loss: 0.02106926590204239\n",
      "Average Return: 13.851861000061035\n",
      "\n",
      "Iteration: 30000\n",
      "Train Loss: 0.028720298781991005\n",
      "Average Return: 14.861380577087402\n",
      "\n",
      "Iteration: 30500\n",
      "Train Loss: 0.016994312405586243\n",
      "Average Return: 14.744378089904785\n",
      "\n",
      "Iteration: 31000\n",
      "Train Loss: 0.04885013401508331\n",
      "Average Return: 12.18026065826416\n",
      "\n",
      "Iteration: 31500\n",
      "Train Loss: 0.03549046814441681\n",
      "Average Return: 12.965165138244629\n",
      "\n",
      "Iteration: 32000\n",
      "Train Loss: 0.016347652301192284\n",
      "Average Return: 13.358160972595215\n",
      "\n",
      "Iteration: 32500\n",
      "Train Loss: 0.02520769275724888\n",
      "Average Return: 14.359953880310059\n",
      "\n",
      "Iteration: 33000\n",
      "Train Loss: 0.020893041044473648\n",
      "Average Return: 13.020051956176758\n",
      "\n",
      "Iteration: 33500\n",
      "Train Loss: 0.03338461369276047\n",
      "Average Return: 10.92371940612793\n",
      "\n",
      "Iteration: 34000\n",
      "Train Loss: 0.01252518780529499\n",
      "Average Return: 10.38348388671875\n",
      "\n",
      "Iteration: 34500\n",
      "Train Loss: 0.01725589670240879\n",
      "Average Return: 11.249000549316406\n",
      "\n",
      "Iteration: 35000\n",
      "Train Loss: 0.03594125062227249\n",
      "Average Return: 11.054915428161621\n",
      "\n",
      "Iteration: 35500\n",
      "Train Loss: 0.01840200647711754\n",
      "Average Return: 8.675664901733398\n",
      "\n",
      "Iteration: 36000\n",
      "Train Loss: 0.020628787577152252\n",
      "Average Return: 14.057106018066406\n",
      "\n",
      "Iteration: 36500\n",
      "Train Loss: 0.01786494255065918\n",
      "Average Return: 13.07896614074707\n",
      "\n",
      "Iteration: 37000\n",
      "Train Loss: 0.021296074613928795\n",
      "Average Return: 13.594141960144043\n",
      "\n",
      "Iteration: 37500\n",
      "Train Loss: 0.028720201924443245\n",
      "Average Return: 5.314340591430664\n",
      "\n",
      "Iteration: 38000\n",
      "Train Loss: 0.039933498948812485\n",
      "Average Return: 12.30990219116211\n",
      "\n",
      "Iteration: 38500\n",
      "Train Loss: 0.02896803244948387\n",
      "Average Return: 5.011396884918213\n",
      "\n",
      "Iteration: 39000\n",
      "Train Loss: 0.033847540616989136\n",
      "Average Return: 11.172032356262207\n",
      "\n",
      "Iteration: 39500\n",
      "Train Loss: 0.030358504503965378\n",
      "Average Return: 8.750462532043457\n",
      "\n",
      "Iteration: 40000\n",
      "Train Loss: 0.042969271540641785\n",
      "Average Return: 3.1263489723205566\n",
      "\n",
      "Iteration: 40500\n",
      "Train Loss: 0.03678946569561958\n",
      "Average Return: 7.44711971282959\n",
      "\n",
      "Iteration: 41000\n",
      "Train Loss: 0.018423382192850113\n",
      "Average Return: 9.532418251037598\n",
      "\n",
      "Iteration: 41500\n",
      "Train Loss: 0.02811078354716301\n",
      "Average Return: 7.586599826812744\n",
      "\n",
      "Iteration: 42000\n",
      "Train Loss: 0.06180021911859512\n",
      "Average Return: 4.295059680938721\n",
      "\n",
      "Iteration: 42500\n",
      "Train Loss: 0.02540307492017746\n",
      "Average Return: 10.113595008850098\n",
      "\n",
      "Iteration: 43000\n",
      "Train Loss: 0.02498694509267807\n",
      "Average Return: 7.870713710784912\n",
      "\n",
      "Iteration: 43500\n",
      "Train Loss: 0.043014273047447205\n",
      "Average Return: 9.419942855834961\n",
      "\n",
      "Iteration: 44000\n",
      "Train Loss: 0.026585109531879425\n",
      "Average Return: 7.461551666259766\n",
      "\n",
      "Iteration: 44500\n",
      "Train Loss: 0.02866942808032036\n",
      "Average Return: 10.638036727905273\n",
      "\n",
      "Iteration: 45000\n",
      "Train Loss: 0.03319665044546127\n",
      "Average Return: 10.592855453491211\n",
      "\n",
      "Iteration: 45500\n",
      "Train Loss: 0.0514061376452446\n",
      "Average Return: 8.2194185256958\n",
      "\n",
      "Iteration: 46000\n",
      "Train Loss: 0.018457647413015366\n",
      "Average Return: 10.278757095336914\n",
      "\n",
      "Iteration: 46500\n",
      "Train Loss: 0.05824078619480133\n",
      "Average Return: 8.19713020324707\n",
      "\n",
      "Iteration: 47000\n",
      "Train Loss: 0.05367043614387512\n",
      "Average Return: 9.561832427978516\n",
      "\n",
      "Iteration: 47500\n",
      "Train Loss: 0.03681987524032593\n",
      "Average Return: 8.87555980682373\n",
      "\n",
      "Iteration: 48000\n",
      "Train Loss: 0.02764727920293808\n",
      "Average Return: 8.83216381072998\n",
      "\n",
      "Iteration: 48500\n",
      "Train Loss: 0.028681939467787743\n",
      "Average Return: 8.929061889648438\n",
      "\n",
      "Iteration: 49000\n",
      "Train Loss: 0.022032514214515686\n",
      "Average Return: 9.260882377624512\n",
      "\n",
      "Iteration: 49500\n",
      "Train Loss: 0.03142939507961273\n",
      "Average Return: 6.529321193695068\n",
      "\n",
      "Iteration: 50000\n",
      "Train Loss: 0.05494842305779457\n",
      "Average Return: 5.626428127288818\n",
      "Collecting Initial Samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\metrics\\trading\\sortino.py:20: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(average_returns/std_downfall_returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has started...\n",
      "\n",
      "New best average return found at 26.674510955810547! Saving checkpoint at iteration 0\n",
      "\n",
      "New best average return found at 34.36062240600586! Saving checkpoint at iteration 500\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 0.014842865988612175\n",
      "Average Return: 34.36062240600586\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 0.009296132251620293\n",
      "Average Return: 33.677608489990234\n",
      "\n",
      "Iteration: 1500\n",
      "Train Loss: 0.006143459118902683\n",
      "Average Return: 33.426048278808594\n",
      "\n",
      "Iteration: 2000\n",
      "Train Loss: 0.006470206193625927\n",
      "Average Return: 32.41636657714844\n",
      "\n",
      "Iteration: 2500\n",
      "Train Loss: 0.016628220677375793\n",
      "Average Return: 31.235071182250977\n",
      "\n",
      "Iteration: 3000\n",
      "Train Loss: 0.0131447222083807\n",
      "Average Return: 34.02251434326172\n",
      "\n",
      "Iteration: 3500\n",
      "Train Loss: 0.019370481371879578\n",
      "Average Return: 32.952945709228516\n",
      "\n",
      "New best average return found at 35.579505920410156! Saving checkpoint at iteration 4000\n",
      "\n",
      "Iteration: 4000\n",
      "Train Loss: 0.02315877191722393\n",
      "Average Return: 35.579505920410156\n",
      "\n",
      "Iteration: 4500\n",
      "Train Loss: 0.021999232470989227\n",
      "Average Return: 34.02511978149414\n",
      "\n",
      "Iteration: 5000\n",
      "Train Loss: 0.011849166825413704\n",
      "Average Return: 33.50857925415039\n",
      "\n",
      "Iteration: 5500\n",
      "Train Loss: 0.028407618403434753\n",
      "Average Return: 34.56218338012695\n",
      "\n",
      "Iteration: 6000\n",
      "Train Loss: 0.00847522635012865\n",
      "Average Return: 34.51277542114258\n",
      "\n",
      "Iteration: 6500\n",
      "Train Loss: 0.03182632103562355\n",
      "Average Return: 32.695106506347656\n",
      "\n",
      "Iteration: 7000\n",
      "Train Loss: 0.011656595394015312\n",
      "Average Return: 34.98125076293945\n",
      "\n",
      "Iteration: 7500\n",
      "Train Loss: 0.009660806506872177\n",
      "Average Return: 31.62313461303711\n",
      "\n",
      "Iteration: 8000\n",
      "Train Loss: 0.0314117856323719\n",
      "Average Return: 33.56716537475586\n",
      "\n",
      "Iteration: 8500\n",
      "Train Loss: 0.03493014723062515\n",
      "Average Return: 33.97817611694336\n",
      "\n",
      "Iteration: 9000\n",
      "Train Loss: 0.04643851891160011\n",
      "Average Return: 34.746978759765625\n",
      "\n",
      "Iteration: 9500\n",
      "Train Loss: 0.03503978252410889\n",
      "Average Return: 33.81685256958008\n",
      "\n",
      "Iteration: 10000\n",
      "Train Loss: 0.021891627460718155\n",
      "Average Return: 29.650333404541016\n",
      "\n",
      "Iteration: 10500\n",
      "Train Loss: 0.022805215790867805\n",
      "Average Return: 29.088953018188477\n",
      "\n",
      "Iteration: 11000\n",
      "Train Loss: 0.04507960379123688\n",
      "Average Return: 30.216110229492188\n",
      "\n",
      "Iteration: 11500\n",
      "Train Loss: 0.02733771875500679\n",
      "Average Return: 33.06992721557617\n",
      "\n",
      "Iteration: 12000\n",
      "Train Loss: 0.056372687220573425\n",
      "Average Return: 30.357677459716797\n",
      "\n",
      "Iteration: 12500\n",
      "Train Loss: 0.02752394787967205\n",
      "Average Return: 32.55921173095703\n",
      "\n",
      "Iteration: 13000\n",
      "Train Loss: 0.017769578844308853\n",
      "Average Return: 34.2552375793457\n",
      "\n",
      "Iteration: 13500\n",
      "Train Loss: 0.02888818271458149\n",
      "Average Return: 32.96061325073242\n",
      "\n",
      "Iteration: 14000\n",
      "Train Loss: 0.03881160169839859\n",
      "Average Return: 34.74424743652344\n",
      "\n",
      "Iteration: 14500\n",
      "Train Loss: 0.013709599152207375\n",
      "Average Return: 33.130584716796875\n",
      "\n",
      "Iteration: 15000\n",
      "Train Loss: 0.040850184857845306\n",
      "Average Return: 32.45962142944336\n",
      "\n",
      "Iteration: 15500\n",
      "Train Loss: 0.05039525777101517\n",
      "Average Return: 28.701929092407227\n",
      "\n",
      "Iteration: 16000\n",
      "Train Loss: 0.03336331620812416\n",
      "Average Return: 32.648685455322266\n",
      "\n",
      "Iteration: 16500\n",
      "Train Loss: 0.03557102382183075\n",
      "Average Return: 23.558868408203125\n",
      "\n",
      "Iteration: 17000\n",
      "Train Loss: 0.0814511626958847\n",
      "Average Return: 27.89781379699707\n",
      "\n",
      "Iteration: 17500\n",
      "Train Loss: 0.03973754122853279\n",
      "Average Return: 30.257490158081055\n",
      "\n",
      "Iteration: 18000\n",
      "Train Loss: 0.02137603797018528\n",
      "Average Return: 29.149873733520508\n",
      "\n",
      "Iteration: 18500\n",
      "Train Loss: 0.06941600888967514\n",
      "Average Return: 20.33463478088379\n",
      "\n",
      "Iteration: 19000\n",
      "Train Loss: 0.0509166494011879\n",
      "Average Return: 16.09089469909668\n",
      "\n",
      "Iteration: 19500\n",
      "Train Loss: 0.04350782185792923\n",
      "Average Return: 24.78833770751953\n",
      "\n",
      "Iteration: 20000\n",
      "Train Loss: 0.030165428295731544\n",
      "Average Return: 29.02519989013672\n",
      "\n",
      "Iteration: 20500\n",
      "Train Loss: 0.05321824550628662\n",
      "Average Return: 32.478614807128906\n",
      "\n",
      "Iteration: 21000\n",
      "Train Loss: 0.058951135724782944\n",
      "Average Return: 20.856307983398438\n",
      "\n",
      "Iteration: 21500\n",
      "Train Loss: 0.022070979699492455\n",
      "Average Return: 20.029645919799805\n",
      "\n",
      "Iteration: 22000\n",
      "Train Loss: 0.05137238651514053\n",
      "Average Return: 32.525516510009766\n",
      "\n",
      "Iteration: 22500\n",
      "Train Loss: 0.06822563707828522\n",
      "Average Return: 30.00909996032715\n",
      "\n",
      "Iteration: 23000\n",
      "Train Loss: 0.04174249246716499\n",
      "Average Return: 25.82533073425293\n",
      "\n",
      "Iteration: 23500\n",
      "Train Loss: 0.0730653777718544\n",
      "Average Return: 27.547874450683594\n",
      "\n",
      "Iteration: 24000\n",
      "Train Loss: 0.059318847954273224\n",
      "Average Return: 25.95400619506836\n",
      "\n",
      "Iteration: 24500\n",
      "Train Loss: 0.08143540471792221\n",
      "Average Return: 25.117225646972656\n",
      "\n",
      "Iteration: 25000\n",
      "Train Loss: 0.04668240249156952\n",
      "Average Return: 31.091760635375977\n",
      "\n",
      "Iteration: 25500\n",
      "Train Loss: 0.07039917260408401\n",
      "Average Return: 28.372573852539062\n",
      "\n",
      "Iteration: 26000\n",
      "Train Loss: 0.02933763898909092\n",
      "Average Return: 27.743026733398438\n",
      "\n",
      "Iteration: 26500\n",
      "Train Loss: 0.055169232189655304\n",
      "Average Return: 29.194202423095703\n",
      "\n",
      "Iteration: 27000\n",
      "Train Loss: 0.07103858143091202\n",
      "Average Return: 20.428508758544922\n",
      "\n",
      "Iteration: 27500\n",
      "Train Loss: 0.07331971824169159\n",
      "Average Return: 27.27507209777832\n",
      "\n",
      "Iteration: 28000\n",
      "Train Loss: 0.04711724817752838\n",
      "Average Return: 27.673269271850586\n",
      "\n",
      "Iteration: 28500\n",
      "Train Loss: 0.028000880032777786\n",
      "Average Return: 26.36002540588379\n",
      "\n",
      "Iteration: 29000\n",
      "Train Loss: 0.04151754826307297\n",
      "Average Return: 14.05091667175293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\metrics\\trading\\sortino.py:20: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.exp(average_returns/std_downfall_returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 29500\n",
      "Train Loss: 0.039734918624162674\n",
      "Average Return: 15.872835159301758\n",
      "\n",
      "Iteration: 30000\n",
      "Train Loss: 0.0404592864215374\n",
      "Average Return: 26.861629486083984\n",
      "\n",
      "Iteration: 30500\n",
      "Train Loss: 0.03974232077598572\n",
      "Average Return: 28.027565002441406\n",
      "\n",
      "Iteration: 31000\n",
      "Train Loss: 0.12504422664642334\n",
      "Average Return: 25.17815399169922\n",
      "\n",
      "Iteration: 31500\n",
      "Train Loss: 0.11980539560317993\n",
      "Average Return: 22.144664764404297\n",
      "\n",
      "Iteration: 32000\n",
      "Train Loss: 0.03582223504781723\n",
      "Average Return: 29.08325958251953\n",
      "\n",
      "Iteration: 32500\n",
      "Train Loss: 0.06847092509269714\n",
      "Average Return: 31.308080673217773\n",
      "\n",
      "Iteration: 33000\n",
      "Train Loss: 0.045389141887426376\n",
      "Average Return: 24.97724151611328\n",
      "\n",
      "Iteration: 33500\n",
      "Train Loss: 0.053193941712379456\n",
      "Average Return: 27.473323822021484\n",
      "\n",
      "Iteration: 34000\n",
      "Train Loss: 0.026069993153214455\n",
      "Average Return: 24.416536331176758\n",
      "\n",
      "Iteration: 34500\n",
      "Train Loss: 0.026640374213457108\n",
      "Average Return: 24.8795223236084\n",
      "\n",
      "Iteration: 35000\n",
      "Train Loss: 0.06758280098438263\n",
      "Average Return: 28.57444953918457\n",
      "\n",
      "Iteration: 35500\n",
      "Train Loss: 0.03802984207868576\n",
      "Average Return: 17.991907119750977\n",
      "\n",
      "Iteration: 36000\n",
      "Train Loss: 0.03373146057128906\n",
      "Average Return: 30.290496826171875\n",
      "\n",
      "Iteration: 36500\n",
      "Train Loss: 0.06774762272834778\n",
      "Average Return: 23.382659912109375\n",
      "\n",
      "Iteration: 37000\n",
      "Train Loss: 0.06635536998510361\n",
      "Average Return: 18.79084587097168\n",
      "\n",
      "Iteration: 37500\n",
      "Train Loss: 0.13173240423202515\n",
      "Average Return: 16.010683059692383\n",
      "\n",
      "Iteration: 38000\n",
      "Train Loss: 0.08959333598613739\n",
      "Average Return: 23.3820743560791\n",
      "\n",
      "Iteration: 38500\n",
      "Train Loss: 0.0716584324836731\n",
      "Average Return: 18.551904678344727\n",
      "\n",
      "Iteration: 39000\n",
      "Train Loss: 0.09560820460319519\n",
      "Average Return: 23.810117721557617\n",
      "\n",
      "Iteration: 39500\n",
      "Train Loss: 0.05727425962686539\n",
      "Average Return: 16.650833129882812\n",
      "\n",
      "Iteration: 40000\n",
      "Train Loss: 0.13001644611358643\n",
      "Average Return: 20.407033920288086\n",
      "\n",
      "Iteration: 40500\n",
      "Train Loss: 0.09862655401229858\n",
      "Average Return: 17.443763732910156\n",
      "\n",
      "Iteration: 41000\n",
      "Train Loss: 0.07214172929525375\n",
      "Average Return: 16.13284683227539\n",
      "\n",
      "Iteration: 41500\n",
      "Train Loss: 0.05169779807329178\n",
      "Average Return: 23.30141830444336\n",
      "\n",
      "Iteration: 42000\n",
      "Train Loss: 0.17489755153656006\n",
      "Average Return: 19.121000289916992\n",
      "\n",
      "Iteration: 42500\n",
      "Train Loss: 0.07220758497714996\n",
      "Average Return: 18.448259353637695\n",
      "\n",
      "Iteration: 43000\n",
      "Train Loss: 0.055641211569309235\n",
      "Average Return: 21.340845108032227\n",
      "\n",
      "Iteration: 43500\n",
      "Train Loss: 0.0983017086982727\n",
      "Average Return: 19.208486557006836\n",
      "\n",
      "Iteration: 44000\n",
      "Train Loss: 0.07322947680950165\n",
      "Average Return: 17.394678115844727\n",
      "\n",
      "Iteration: 44500\n",
      "Train Loss: 0.07277000695466995\n",
      "Average Return: 24.44939613342285\n",
      "\n",
      "Iteration: 45000\n",
      "Train Loss: 0.08671882003545761\n",
      "Average Return: 15.791398048400879\n",
      "\n",
      "Iteration: 45500\n",
      "Train Loss: 0.05946153402328491\n",
      "Average Return: 21.91775894165039\n",
      "\n",
      "Iteration: 46000\n",
      "Train Loss: 0.033028461039066315\n",
      "Average Return: 23.337190628051758\n",
      "\n",
      "Iteration: 46500\n",
      "Train Loss: 0.08574103564023972\n",
      "Average Return: 17.536426544189453\n",
      "\n",
      "Iteration: 47000\n",
      "Train Loss: 0.11554133892059326\n",
      "Average Return: 24.52699089050293\n",
      "\n",
      "Iteration: 47500\n",
      "Train Loss: 0.07659495621919632\n",
      "Average Return: 21.903934478759766\n",
      "\n",
      "Iteration: 48000\n",
      "Train Loss: 0.07160608470439911\n",
      "Average Return: 17.420421600341797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 48500\n",
      "Train Loss: 0.05505954474210739\n",
      "Average Return: 19.10175323486328\n",
      "\n",
      "Iteration: 49000\n",
      "Train Loss: 0.05312485992908478\n",
      "Average Return: 20.285017013549805\n",
      "\n",
      "Iteration: 49500\n",
      "Train Loss: 0.07160315662622452\n",
      "Average Return: 19.44167137145996\n",
      "\n",
      "Iteration: 50000\n",
      "Train Loss: 0.1190890371799469\n",
      "Average Return: 19.467395782470703\n",
      "Collecting Initial Samples...\n",
      "Training has started...\n",
      "\n",
      "New best average return found at -44.11851119995117! Saving checkpoint at iteration 0\n",
      "\n",
      "New best average return found at 9.697697639465332! Saving checkpoint at iteration 500\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 0.0030540823936462402\n",
      "Average Return: 9.697697639465332\n",
      "\n",
      "New best average return found at 11.083016395568848! Saving checkpoint at iteration 1000\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 0.006096345372498035\n",
      "Average Return: 11.083016395568848\n",
      "\n",
      "New best average return found at 14.647705078125! Saving checkpoint at iteration 1500\n",
      "\n",
      "Iteration: 1500\n",
      "Train Loss: 0.005026837810873985\n",
      "Average Return: 14.647705078125\n",
      "\n",
      "Iteration: 2000\n",
      "Train Loss: 0.0017934980569407344\n",
      "Average Return: 13.400622367858887\n",
      "\n",
      "Iteration: 2500\n",
      "Train Loss: 0.0023113382048904896\n",
      "Average Return: 12.10470199584961\n",
      "\n",
      "New best average return found at 17.002248764038086! Saving checkpoint at iteration 3000\n",
      "\n",
      "Iteration: 3000\n",
      "Train Loss: 0.003876877948641777\n",
      "Average Return: 17.002248764038086\n",
      "\n",
      "Iteration: 3500\n",
      "Train Loss: 0.005837832577526569\n",
      "Average Return: 16.414384841918945\n",
      "\n",
      "Iteration: 4000\n",
      "Train Loss: 0.012870218604803085\n",
      "Average Return: 13.033586502075195\n",
      "\n",
      "Iteration: 4500\n",
      "Train Loss: 0.009901456534862518\n",
      "Average Return: 16.858163833618164\n",
      "\n",
      "Iteration: 5000\n",
      "Train Loss: 0.011030681431293488\n",
      "Average Return: 13.567910194396973\n",
      "\n",
      "Iteration: 5500\n",
      "Train Loss: 0.010197201743721962\n",
      "Average Return: 12.362174987792969\n",
      "\n",
      "Iteration: 6000\n",
      "Train Loss: 0.00915694423019886\n",
      "Average Return: 15.510708808898926\n",
      "\n",
      "Iteration: 6500\n",
      "Train Loss: 0.008721405640244484\n",
      "Average Return: 12.44884967803955\n",
      "\n",
      "Iteration: 7000\n",
      "Train Loss: 0.008893527090549469\n",
      "Average Return: 13.009541511535645\n",
      "\n",
      "Iteration: 7500\n",
      "Train Loss: 0.0072487881407141685\n",
      "Average Return: 12.9276762008667\n",
      "\n",
      "Iteration: 8000\n",
      "Train Loss: 0.011305312626063824\n",
      "Average Return: 14.057944297790527\n",
      "\n",
      "Iteration: 8500\n",
      "Train Loss: 0.01311388798058033\n",
      "Average Return: 15.025141716003418\n",
      "\n",
      "Iteration: 9000\n",
      "Train Loss: 0.012993981130421162\n",
      "Average Return: 15.635275840759277\n",
      "\n",
      "Iteration: 9500\n",
      "Train Loss: 0.023465869948267937\n",
      "Average Return: 11.601595878601074\n",
      "\n",
      "Iteration: 10000\n",
      "Train Loss: 0.02276882901787758\n",
      "Average Return: 12.070737838745117\n",
      "\n",
      "Iteration: 10500\n",
      "Train Loss: 0.021325014531612396\n",
      "Average Return: 16.91741943359375\n",
      "\n",
      "Iteration: 11000\n",
      "Train Loss: 0.026388272643089294\n",
      "Average Return: 13.898727416992188\n",
      "\n",
      "Iteration: 11500\n",
      "Train Loss: 0.01547106634825468\n",
      "Average Return: 14.327824592590332\n",
      "\n",
      "Iteration: 12000\n",
      "Train Loss: 0.009499987587332726\n",
      "Average Return: 14.914246559143066\n",
      "\n",
      "Iteration: 12500\n",
      "Train Loss: 0.0158954169601202\n",
      "Average Return: 15.223627090454102\n",
      "\n",
      "Iteration: 13000\n",
      "Train Loss: 0.019338414072990417\n",
      "Average Return: 15.021930694580078\n",
      "\n",
      "Iteration: 13500\n",
      "Train Loss: 0.03524928167462349\n",
      "Average Return: 15.029269218444824\n",
      "\n",
      "Iteration: 14000\n",
      "Train Loss: 0.024563997983932495\n",
      "Average Return: 11.929871559143066\n",
      "\n",
      "Iteration: 14500\n",
      "Train Loss: 0.014822220429778099\n",
      "Average Return: 15.514124870300293\n",
      "\n",
      "Iteration: 15000\n",
      "Train Loss: 0.032304223626852036\n",
      "Average Return: 14.39116382598877\n",
      "\n",
      "New best average return found at 17.122005462646484! Saving checkpoint at iteration 15500\n",
      "\n",
      "Iteration: 15500\n",
      "Train Loss: 0.01849195547401905\n",
      "Average Return: 17.122005462646484\n",
      "\n",
      "Iteration: 16000\n",
      "Train Loss: 0.02506599947810173\n",
      "Average Return: 11.12033748626709\n",
      "\n",
      "Iteration: 16500\n",
      "Train Loss: 0.013458117842674255\n",
      "Average Return: 12.077836036682129\n",
      "\n",
      "Iteration: 17000\n",
      "Train Loss: 0.0440312996506691\n",
      "Average Return: 11.099764823913574\n",
      "\n",
      "Iteration: 17500\n",
      "Train Loss: 0.05537857115268707\n",
      "Average Return: 13.010676383972168\n",
      "\n",
      "Iteration: 18000\n",
      "Train Loss: 0.01200034935027361\n",
      "Average Return: 10.366150856018066\n",
      "\n",
      "Iteration: 18500\n",
      "Train Loss: 0.02037855051457882\n",
      "Average Return: 11.925050735473633\n",
      "\n",
      "Iteration: 19000\n",
      "Train Loss: 0.020852236077189445\n",
      "Average Return: 13.446005821228027\n",
      "\n",
      "Iteration: 19500\n",
      "Train Loss: 0.0209002997726202\n",
      "Average Return: 11.58310604095459\n",
      "\n",
      "Iteration: 20000\n",
      "Train Loss: 0.07702608406543732\n",
      "Average Return: 14.511994361877441\n",
      "\n",
      "Iteration: 20500\n",
      "Train Loss: 0.028336122632026672\n",
      "Average Return: 15.19566822052002\n",
      "\n",
      "Iteration: 21000\n",
      "Train Loss: 0.030861735343933105\n",
      "Average Return: 14.643908500671387\n",
      "\n",
      "Iteration: 21500\n",
      "Train Loss: 0.010619544424116611\n",
      "Average Return: 8.155923843383789\n",
      "\n",
      "Iteration: 22000\n",
      "Train Loss: 0.010522440075874329\n",
      "Average Return: 6.85160493850708\n",
      "\n",
      "Iteration: 22500\n",
      "Train Loss: 0.03808718919754028\n",
      "Average Return: 14.541414260864258\n",
      "\n",
      "Iteration: 23000\n",
      "Train Loss: 0.01383243314921856\n",
      "Average Return: 12.706768035888672\n",
      "\n",
      "Iteration: 23500\n",
      "Train Loss: 0.01584557257592678\n",
      "Average Return: -0.5269630551338196\n",
      "\n",
      "Iteration: 24000\n",
      "Train Loss: 0.023155173286795616\n",
      "Average Return: 12.01235294342041\n",
      "\n",
      "Iteration: 24500\n",
      "Train Loss: 0.023627599701285362\n",
      "Average Return: 11.066289901733398\n",
      "\n",
      "Iteration: 25000\n",
      "Train Loss: 0.013673346489667892\n",
      "Average Return: 9.7838716506958\n",
      "\n",
      "Iteration: 25500\n",
      "Train Loss: 0.026267074048519135\n",
      "Average Return: 4.491258144378662\n",
      "\n",
      "Iteration: 26000\n",
      "Train Loss: 0.01895812712609768\n",
      "Average Return: 10.57816219329834\n",
      "\n",
      "Iteration: 26500\n",
      "Train Loss: 0.021281786262989044\n",
      "Average Return: 9.012646675109863\n",
      "\n",
      "Iteration: 27000\n",
      "Train Loss: 0.02771371603012085\n",
      "Average Return: 6.957530498504639\n",
      "\n",
      "Iteration: 27500\n",
      "Train Loss: 0.028729192912578583\n",
      "Average Return: 9.98216724395752\n",
      "\n",
      "Iteration: 28000\n",
      "Train Loss: 0.02017902582883835\n",
      "Average Return: 8.908074378967285\n",
      "\n",
      "Iteration: 28500\n",
      "Train Loss: 0.02190653793513775\n",
      "Average Return: 5.774154186248779\n",
      "\n",
      "Iteration: 29000\n",
      "Train Loss: 0.03067372739315033\n",
      "Average Return: 8.332627296447754\n",
      "\n",
      "Iteration: 29500\n",
      "Train Loss: 0.022349931299686432\n",
      "Average Return: 7.0899577140808105\n",
      "\n",
      "Iteration: 30000\n",
      "Train Loss: 0.02484118938446045\n",
      "Average Return: 10.737038612365723\n",
      "\n",
      "Iteration: 30500\n",
      "Train Loss: 0.01936274580657482\n",
      "Average Return: 5.452668190002441\n",
      "\n",
      "Iteration: 31000\n",
      "Train Loss: 0.023423943668603897\n",
      "Average Return: 6.501349449157715\n",
      "\n",
      "Iteration: 31500\n",
      "Train Loss: 0.030614688992500305\n",
      "Average Return: 7.8096160888671875\n",
      "\n",
      "Iteration: 32000\n",
      "Train Loss: 0.03244951367378235\n",
      "Average Return: 5.49692964553833\n",
      "\n",
      "Iteration: 32500\n",
      "Train Loss: 0.022387804463505745\n",
      "Average Return: 8.7847318649292\n",
      "\n",
      "Iteration: 33000\n",
      "Train Loss: 0.028850987553596497\n",
      "Average Return: 7.132216453552246\n",
      "\n",
      "Iteration: 33500\n",
      "Train Loss: 0.024001341313123703\n",
      "Average Return: 4.12261962890625\n",
      "\n",
      "Iteration: 34000\n",
      "Train Loss: 0.02350641041994095\n",
      "Average Return: 3.6213037967681885\n",
      "\n",
      "Iteration: 34500\n",
      "Train Loss: 0.022675078362226486\n",
      "Average Return: 8.201691627502441\n",
      "\n",
      "Iteration: 35000\n",
      "Train Loss: 0.01999349147081375\n",
      "Average Return: 5.397125720977783\n",
      "\n",
      "Iteration: 35500\n",
      "Train Loss: 0.031466905027627945\n",
      "Average Return: 9.177206039428711\n",
      "\n",
      "Iteration: 36000\n",
      "Train Loss: 0.03928795084357262\n",
      "Average Return: 8.142581939697266\n",
      "\n",
      "Iteration: 36500\n",
      "Train Loss: 0.0242754016071558\n",
      "Average Return: 9.3567476272583\n",
      "\n",
      "Iteration: 37000\n",
      "Train Loss: 0.026401806622743607\n",
      "Average Return: 5.168124198913574\n",
      "\n",
      "Iteration: 37500\n",
      "Train Loss: 0.015791986137628555\n",
      "Average Return: 7.511059284210205\n",
      "\n",
      "Iteration: 38000\n",
      "Train Loss: 0.03599758446216583\n",
      "Average Return: 8.564529418945312\n",
      "\n",
      "Iteration: 38500\n",
      "Train Loss: 0.025574564933776855\n",
      "Average Return: 3.299191474914551\n",
      "\n",
      "Iteration: 39000\n",
      "Train Loss: 0.027499619871377945\n",
      "Average Return: 6.083859920501709\n",
      "\n",
      "Iteration: 39500\n",
      "Train Loss: 0.0203297957777977\n",
      "Average Return: 6.865079402923584\n",
      "\n",
      "Iteration: 40000\n",
      "Train Loss: 0.0313081368803978\n",
      "Average Return: 5.636292457580566\n",
      "\n",
      "Iteration: 40500\n",
      "Train Loss: 0.021617097780108452\n",
      "Average Return: 4.650881767272949\n",
      "\n",
      "Iteration: 41000\n",
      "Train Loss: 0.0217084139585495\n",
      "Average Return: 8.366576194763184\n",
      "\n",
      "Iteration: 41500\n",
      "Train Loss: 0.03018639050424099\n",
      "Average Return: 6.985449314117432\n",
      "\n",
      "Iteration: 42000\n",
      "Train Loss: 0.02233356423676014\n",
      "Average Return: 7.349742412567139\n",
      "\n",
      "Iteration: 42500\n",
      "Train Loss: 0.03847012668848038\n",
      "Average Return: 8.266515731811523\n",
      "\n",
      "Iteration: 43000\n",
      "Train Loss: 0.02431352622807026\n",
      "Average Return: 6.24677848815918\n",
      "\n",
      "Iteration: 43500\n",
      "Train Loss: 0.023893563076853752\n",
      "Average Return: 4.990373134613037\n",
      "\n",
      "Iteration: 44000\n",
      "Train Loss: 0.018100019544363022\n",
      "Average Return: 5.306338787078857\n",
      "\n",
      "Iteration: 44500\n",
      "Train Loss: 0.023309877142310143\n",
      "Average Return: 8.498114585876465\n",
      "\n",
      "Iteration: 45000\n",
      "Train Loss: 0.022427605465054512\n",
      "Average Return: 4.3482985496521\n",
      "\n",
      "Iteration: 45500\n",
      "Train Loss: 0.027093689888715744\n",
      "Average Return: 7.085510730743408\n",
      "\n",
      "Iteration: 46000\n",
      "Train Loss: 0.025196224451065063\n",
      "Average Return: 9.535690307617188\n",
      "\n",
      "Iteration: 46500\n",
      "Train Loss: 0.02737908996641636\n",
      "Average Return: 2.893237829208374\n",
      "\n",
      "Iteration: 47000\n",
      "Train Loss: 0.019548866897821426\n",
      "Average Return: 5.655611515045166\n",
      "\n",
      "Iteration: 47500\n",
      "Train Loss: 0.01840626448392868\n",
      "Average Return: 5.24747896194458\n",
      "\n",
      "Iteration: 48000\n",
      "Train Loss: 0.023822030052542686\n",
      "Average Return: 6.774082660675049\n",
      "\n",
      "Iteration: 48500\n",
      "Train Loss: 0.021746335551142693\n",
      "Average Return: 6.355510711669922\n",
      "\n",
      "Iteration: 49000\n",
      "Train Loss: 0.03598804399371147\n",
      "Average Return: 3.1830456256866455\n",
      "\n",
      "Iteration: 49500\n",
      "Train Loss: 0.015582425519824028\n",
      "Average Return: 7.70412540435791\n",
      "\n",
      "Iteration: 50000\n",
      "Train Loss: 0.02822173200547695\n",
      "Average Return: 6.69441556930542\n",
      "Collecting Initial Samples...\n",
      "Training has started...\n",
      "\n",
      "New best average return found at -59.96932601928711! Saving checkpoint at iteration 0\n",
      "\n",
      "New best average return found at 23.308547973632812! Saving checkpoint at iteration 500\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 0.005085378419607878\n",
      "Average Return: 23.308547973632812\n",
      "\n",
      "New best average return found at 28.196086883544922! Saving checkpoint at iteration 1000\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 0.011138277128338814\n",
      "Average Return: 28.196086883544922\n",
      "\n",
      "New best average return found at 28.606473922729492! Saving checkpoint at iteration 1500\n",
      "\n",
      "Iteration: 1500\n",
      "Train Loss: 0.00447837496176362\n",
      "Average Return: 28.606473922729492\n",
      "\n",
      "Iteration: 2000\n",
      "Train Loss: 0.00262521393597126\n",
      "Average Return: 27.513072967529297\n",
      "\n",
      "New best average return found at 29.103321075439453! Saving checkpoint at iteration 2500\n",
      "\n",
      "Iteration: 2500\n",
      "Train Loss: 0.0036666749510914087\n",
      "Average Return: 29.103321075439453\n",
      "\n",
      "New best average return found at 29.525442123413086! Saving checkpoint at iteration 3000\n",
      "\n",
      "Iteration: 3000\n",
      "Train Loss: 0.006618470419198275\n",
      "Average Return: 29.525442123413086\n",
      "\n",
      "New best average return found at 30.45142936706543! Saving checkpoint at iteration 3500\n",
      "\n",
      "Iteration: 3500\n",
      "Train Loss: 0.00821087509393692\n",
      "Average Return: 30.45142936706543\n",
      "\n",
      "Iteration: 4000\n",
      "Train Loss: 0.019866885617375374\n",
      "Average Return: 28.58010482788086\n",
      "\n",
      "Iteration: 4500\n",
      "Train Loss: 0.01600138656795025\n",
      "Average Return: 29.574283599853516\n",
      "\n",
      "Iteration: 5000\n",
      "Train Loss: 0.017111744731664658\n",
      "Average Return: 29.729196548461914\n",
      "\n",
      "Iteration: 5500\n",
      "Train Loss: 0.022598005831241608\n",
      "Average Return: 29.613788604736328\n",
      "\n",
      "Iteration: 6000\n",
      "Train Loss: 0.018841350451111794\n",
      "Average Return: 29.089046478271484\n",
      "\n",
      "Iteration: 6500\n",
      "Train Loss: 0.021490465849637985\n",
      "Average Return: 30.131776809692383\n",
      "\n",
      "Iteration: 7000\n",
      "Train Loss: 0.01658443734049797\n",
      "Average Return: 28.243797302246094\n",
      "\n",
      "Iteration: 7500\n",
      "Train Loss: 0.014568492770195007\n",
      "Average Return: 28.74521255493164\n",
      "\n",
      "Iteration: 8000\n",
      "Train Loss: 0.023628605529665947\n",
      "Average Return: 19.774568557739258\n",
      "\n",
      "Iteration: 8500\n",
      "Train Loss: 0.029359307140111923\n",
      "Average Return: 29.03172492980957\n",
      "\n",
      "Iteration: 9000\n",
      "Train Loss: 0.026487160474061966\n",
      "Average Return: 29.818737030029297\n",
      "\n",
      "Iteration: 9500\n",
      "Train Loss: 0.05939247086644173\n",
      "Average Return: 25.59405517578125\n",
      "\n",
      "Iteration: 10000\n",
      "Train Loss: 0.04122224450111389\n",
      "Average Return: 27.895647048950195\n",
      "\n",
      "Iteration: 10500\n",
      "Train Loss: 0.03682142496109009\n",
      "Average Return: 30.067119598388672\n",
      "\n",
      "Iteration: 11000\n",
      "Train Loss: 0.05433892086148262\n",
      "Average Return: 27.61899757385254\n",
      "\n",
      "Iteration: 11500\n",
      "Train Loss: 0.029977872967720032\n",
      "Average Return: 28.985485076904297\n",
      "\n",
      "Iteration: 12000\n",
      "Train Loss: 0.016539201140403748\n",
      "Average Return: 28.62864875793457\n",
      "\n",
      "Iteration: 12500\n",
      "Train Loss: 0.0377531424164772\n",
      "Average Return: 26.00274658203125\n",
      "\n",
      "Iteration: 13000\n",
      "Train Loss: 0.05523458123207092\n",
      "Average Return: 26.810476303100586\n",
      "\n",
      "Iteration: 13500\n",
      "Train Loss: 0.07572461664676666\n",
      "Average Return: 27.763792037963867\n",
      "\n",
      "Iteration: 14000\n",
      "Train Loss: 0.06382755190134048\n",
      "Average Return: 21.28654670715332\n",
      "\n",
      "Iteration: 14500\n",
      "Train Loss: 0.02817440778017044\n",
      "Average Return: 27.76471710205078\n",
      "\n",
      "Iteration: 15000\n",
      "Train Loss: 0.045565102249383926\n",
      "Average Return: 25.546634674072266\n",
      "\n",
      "Iteration: 15500\n",
      "Train Loss: 0.05105698108673096\n",
      "Average Return: 24.03157615661621\n",
      "\n",
      "Iteration: 16000\n",
      "Train Loss: 0.06284958124160767\n",
      "Average Return: 26.65035629272461\n",
      "\n",
      "Iteration: 16500\n",
      "Train Loss: 0.03867068141698837\n",
      "Average Return: 17.83478546142578\n",
      "\n",
      "Iteration: 17000\n",
      "Train Loss: 0.16325868666172028\n",
      "Average Return: 25.459129333496094\n",
      "\n",
      "Iteration: 17500\n",
      "Train Loss: 0.10455098003149033\n",
      "Average Return: 25.129634857177734\n",
      "\n",
      "Iteration: 18000\n",
      "Train Loss: 0.04898891597986221\n",
      "Average Return: 14.328696250915527\n",
      "\n",
      "Iteration: 18500\n",
      "Train Loss: 0.06817153096199036\n",
      "Average Return: 25.879329681396484\n",
      "\n",
      "Iteration: 19000\n",
      "Train Loss: 0.04128324240446091\n",
      "Average Return: 27.14878273010254\n",
      "\n",
      "Iteration: 19500\n",
      "Train Loss: 0.03821380063891411\n",
      "Average Return: 13.796520233154297\n",
      "\n",
      "Iteration: 20000\n",
      "Train Loss: 0.148048996925354\n",
      "Average Return: 22.12885284423828\n",
      "\n",
      "Iteration: 20500\n",
      "Train Loss: 0.06419392675161362\n",
      "Average Return: 22.819570541381836\n",
      "\n",
      "Iteration: 21000\n",
      "Train Loss: 0.06972146034240723\n",
      "Average Return: 24.60481071472168\n",
      "\n",
      "Iteration: 21500\n",
      "Train Loss: 0.020716184750199318\n",
      "Average Return: 6.906177043914795\n",
      "\n",
      "Iteration: 22000\n",
      "Train Loss: 0.028979193419218063\n",
      "Average Return: 13.089547157287598\n",
      "\n",
      "Iteration: 22500\n",
      "Train Loss: 0.08610875904560089\n",
      "Average Return: 25.372356414794922\n",
      "\n",
      "Iteration: 23000\n",
      "Train Loss: 0.02955440804362297\n",
      "Average Return: 21.472715377807617\n",
      "\n",
      "Iteration: 23500\n",
      "Train Loss: 0.03047816827893257\n",
      "Average Return: 25.226661682128906\n",
      "\n",
      "Iteration: 24000\n",
      "Train Loss: 0.043461427092552185\n",
      "Average Return: 8.256954193115234\n",
      "\n",
      "Iteration: 24500\n",
      "Train Loss: 0.05027259141206741\n",
      "Average Return: 15.95500373840332\n",
      "\n",
      "Iteration: 25000\n",
      "Train Loss: 0.037292636930942535\n",
      "Average Return: 17.453706741333008\n",
      "\n",
      "Iteration: 25500\n",
      "Train Loss: 0.06249070167541504\n",
      "Average Return: 18.36290740966797\n",
      "\n",
      "Iteration: 26000\n",
      "Train Loss: 0.041546959429979324\n",
      "Average Return: 13.385741233825684\n",
      "\n",
      "Iteration: 26500\n",
      "Train Loss: 0.06818460673093796\n",
      "Average Return: 18.74075698852539\n",
      "\n",
      "Iteration: 27000\n",
      "Train Loss: 0.07460997253656387\n",
      "Average Return: 14.770293235778809\n",
      "\n",
      "Iteration: 27500\n",
      "Train Loss: 0.06312893331050873\n",
      "Average Return: 16.8804931640625\n",
      "\n",
      "Iteration: 28000\n",
      "Train Loss: 0.08361451327800751\n",
      "Average Return: 18.274211883544922\n",
      "\n",
      "Iteration: 28500\n",
      "Train Loss: 0.06907805055379868\n",
      "Average Return: 13.023702621459961\n",
      "\n",
      "Iteration: 29000\n",
      "Train Loss: 0.0741354376077652\n",
      "Average Return: 21.135862350463867\n",
      "\n",
      "Iteration: 29500\n",
      "Train Loss: 0.07300236076116562\n",
      "Average Return: 24.545602798461914\n",
      "\n",
      "Iteration: 30000\n",
      "Train Loss: 0.04880648851394653\n",
      "Average Return: 21.910348892211914\n",
      "\n",
      "Iteration: 30500\n",
      "Train Loss: 0.04485329985618591\n",
      "Average Return: 12.299982070922852\n",
      "\n",
      "Iteration: 31000\n",
      "Train Loss: 0.04642898961901665\n",
      "Average Return: 18.197288513183594\n",
      "\n",
      "Iteration: 31500\n",
      "Train Loss: 0.055853068828582764\n",
      "Average Return: 11.625558853149414\n",
      "\n",
      "Iteration: 32000\n",
      "Train Loss: 0.06354445964097977\n",
      "Average Return: 10.672977447509766\n",
      "\n",
      "Iteration: 32500\n",
      "Train Loss: 0.07050272822380066\n",
      "Average Return: 19.950252532958984\n",
      "\n",
      "Iteration: 33000\n",
      "Train Loss: 0.0584978386759758\n",
      "Average Return: 16.26200294494629\n",
      "\n",
      "Iteration: 33500\n",
      "Train Loss: 0.054066531360149384\n",
      "Average Return: 11.7802095413208\n",
      "\n",
      "Iteration: 34000\n",
      "Train Loss: 0.03899727016687393\n",
      "Average Return: 16.096670150756836\n",
      "\n",
      "Iteration: 34500\n",
      "Train Loss: 0.04247461259365082\n",
      "Average Return: 19.936429977416992\n",
      "\n",
      "Iteration: 35000\n",
      "Train Loss: 0.07321389019489288\n",
      "Average Return: 16.890378952026367\n",
      "\n",
      "Iteration: 35500\n",
      "Train Loss: 0.06911341845989227\n",
      "Average Return: 15.421307563781738\n",
      "\n",
      "Iteration: 36000\n",
      "Train Loss: 0.064808689057827\n",
      "Average Return: 20.774755477905273\n",
      "\n",
      "Iteration: 36500\n",
      "Train Loss: 0.04822862893342972\n",
      "Average Return: 19.01978874206543\n",
      "\n",
      "Iteration: 37000\n",
      "Train Loss: 0.05225319415330887\n",
      "Average Return: 20.58531379699707\n",
      "\n",
      "Iteration: 37500\n",
      "Train Loss: 0.029330914840102196\n",
      "Average Return: 18.41830062866211\n",
      "\n",
      "Iteration: 38000\n",
      "Train Loss: 0.0589652955532074\n",
      "Average Return: 19.56601333618164\n",
      "\n",
      "Iteration: 38500\n",
      "Train Loss: 0.060495659708976746\n",
      "Average Return: 13.484074592590332\n",
      "\n",
      "Iteration: 39000\n",
      "Train Loss: 0.07268218696117401\n",
      "Average Return: 17.20904541015625\n",
      "\n",
      "Iteration: 39500\n",
      "Train Loss: 0.04591204226016998\n",
      "Average Return: 17.488359451293945\n",
      "\n",
      "Iteration: 40000\n",
      "Train Loss: 0.06655342876911163\n",
      "Average Return: 22.39784049987793\n",
      "\n",
      "Iteration: 40500\n",
      "Train Loss: 0.05268865451216698\n",
      "Average Return: 23.457223892211914\n",
      "\n",
      "Iteration: 41000\n",
      "Train Loss: 0.04914404824376106\n",
      "Average Return: 22.127090454101562\n",
      "\n",
      "Iteration: 41500\n",
      "Train Loss: 0.06878998130559921\n",
      "Average Return: 20.585824966430664\n",
      "\n",
      "Iteration: 42000\n",
      "Train Loss: 0.06466180086135864\n",
      "Average Return: 20.118385314941406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\metrics\\trading\\sortino.py:20: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.exp(average_returns/std_downfall_returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 42500\n",
      "Train Loss: 0.07082706689834595\n",
      "Average Return: 20.564373016357422\n",
      "\n",
      "Iteration: 43000\n",
      "Train Loss: 0.07105065137147903\n",
      "Average Return: 17.692092895507812\n",
      "\n",
      "Iteration: 43500\n",
      "Train Loss: 0.09303058683872223\n",
      "Average Return: 15.988404273986816\n",
      "\n",
      "Iteration: 44000\n",
      "Train Loss: 0.03563743084669113\n",
      "Average Return: 14.372461318969727\n",
      "\n",
      "Iteration: 44500\n",
      "Train Loss: 0.0445047952234745\n",
      "Average Return: 23.42290496826172\n",
      "\n",
      "Iteration: 45000\n",
      "Train Loss: 0.044997043907642365\n",
      "Average Return: 13.917854309082031\n",
      "\n",
      "Iteration: 45500\n",
      "Train Loss: 0.030068514868617058\n",
      "Average Return: 15.403700828552246\n",
      "\n",
      "Iteration: 46000\n",
      "Train Loss: 0.039457205682992935\n",
      "Average Return: 18.324506759643555\n",
      "\n",
      "Iteration: 46500\n",
      "Train Loss: 0.054179057478904724\n",
      "Average Return: 18.779537200927734\n",
      "\n",
      "Iteration: 47000\n",
      "Train Loss: 0.051116637885570526\n",
      "Average Return: 17.13608169555664\n",
      "\n",
      "Iteration: 47500\n",
      "Train Loss: 0.05406451225280762\n",
      "Average Return: 16.363452911376953\n",
      "\n",
      "Iteration: 48000\n",
      "Train Loss: 0.03296280279755592\n",
      "Average Return: 14.398664474487305\n",
      "\n",
      "Iteration: 48500\n",
      "Train Loss: 0.04484616592526436\n",
      "Average Return: 20.545150756835938\n",
      "\n",
      "Iteration: 49000\n",
      "Train Loss: 0.10011427849531174\n",
      "Average Return: 19.96053123474121\n",
      "\n",
      "Iteration: 49500\n",
      "Train Loss: 0.09380906075239182\n",
      "Average Return: 25.32269287109375\n",
      "\n",
      "Iteration: 50000\n",
      "Train Loss: 0.05906921252608299\n",
      "Average Return: 19.855249404907227\n",
      "Collecting Initial Samples...\n",
      "Training has started...\n",
      "\n",
      "New best average return found at 18.789419174194336! Saving checkpoint at iteration 0\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 0.0016921574715524912\n",
      "Average Return: 14.9060640335083\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 0.0048326151445508\n",
      "Average Return: 18.427709579467773\n",
      "\n",
      "Iteration: 1500\n",
      "Train Loss: 0.0012794157955795527\n",
      "Average Return: 17.200483322143555\n",
      "\n",
      "Iteration: 2000\n",
      "Train Loss: 0.0013053722213953733\n",
      "Average Return: 16.295368194580078\n",
      "\n",
      "Iteration: 2500\n",
      "Train Loss: 0.015684882178902626\n",
      "Average Return: 16.496044158935547\n",
      "\n",
      "Iteration: 3000\n",
      "Train Loss: 0.0070289866998791695\n",
      "Average Return: 13.371822357177734\n",
      "\n",
      "Iteration: 3500\n",
      "Train Loss: 0.003160497872158885\n",
      "Average Return: 12.799287796020508\n",
      "\n",
      "Iteration: 4000\n",
      "Train Loss: 0.02177008055150509\n",
      "Average Return: 15.658617973327637\n",
      "\n",
      "Iteration: 4500\n",
      "Train Loss: 0.021136552095413208\n",
      "Average Return: 15.35739517211914\n",
      "\n",
      "Iteration: 5000\n",
      "Train Loss: 0.007943074218928814\n",
      "Average Return: 15.822589874267578\n",
      "\n",
      "Iteration: 5500\n",
      "Train Loss: 0.01216820627450943\n",
      "Average Return: 17.440593719482422\n",
      "\n",
      "New best average return found at 19.331584930419922! Saving checkpoint at iteration 6000\n",
      "\n",
      "Iteration: 6000\n",
      "Train Loss: 0.010554613545536995\n",
      "Average Return: 19.331584930419922\n",
      "\n",
      "Iteration: 6500\n",
      "Train Loss: 0.014682061970233917\n",
      "Average Return: 16.257797241210938\n",
      "\n",
      "Iteration: 7000\n",
      "Train Loss: 0.026716148480772972\n",
      "Average Return: 13.258204460144043\n",
      "\n",
      "Iteration: 7500\n",
      "Train Loss: 0.022333255037665367\n",
      "Average Return: 14.193395614624023\n",
      "\n",
      "Iteration: 8000\n",
      "Train Loss: 0.015435434877872467\n",
      "Average Return: 9.367835998535156\n",
      "\n",
      "Iteration: 8500\n",
      "Train Loss: 0.019639449194073677\n",
      "Average Return: 10.952973365783691\n",
      "\n",
      "Iteration: 9000\n",
      "Train Loss: 0.03331074118614197\n",
      "Average Return: 12.64367961883545\n",
      "\n",
      "Iteration: 9500\n",
      "Train Loss: 0.0042007784359157085\n",
      "Average Return: 10.685518264770508\n",
      "\n",
      "Iteration: 10000\n",
      "Train Loss: 0.014632858335971832\n",
      "Average Return: 14.917634963989258\n",
      "\n",
      "Iteration: 10500\n",
      "Train Loss: 0.009884977713227272\n",
      "Average Return: 13.44130802154541\n",
      "\n",
      "Iteration: 11000\n",
      "Train Loss: 0.013864127919077873\n",
      "Average Return: 5.522411823272705\n",
      "\n",
      "Iteration: 11500\n",
      "Train Loss: 0.01539043989032507\n",
      "Average Return: 11.14113712310791\n",
      "\n",
      "Iteration: 12000\n",
      "Train Loss: 0.045974522829055786\n",
      "Average Return: 8.137161254882812\n",
      "\n",
      "Iteration: 12500\n",
      "Train Loss: 0.028555726632475853\n",
      "Average Return: 15.28260326385498\n",
      "\n",
      "Iteration: 13000\n",
      "Train Loss: 0.02484923228621483\n",
      "Average Return: 12.906795501708984\n",
      "\n",
      "Iteration: 13500\n",
      "Train Loss: 0.029400188475847244\n",
      "Average Return: 11.480720520019531\n",
      "\n",
      "Iteration: 14000\n",
      "Train Loss: 0.050515566021203995\n",
      "Average Return: 11.094643592834473\n",
      "\n",
      "Iteration: 14500\n",
      "Train Loss: 0.026620887219905853\n",
      "Average Return: 15.219406127929688\n",
      "\n",
      "Iteration: 15000\n",
      "Train Loss: 0.06483333557844162\n",
      "Average Return: 13.235690116882324\n",
      "\n",
      "Iteration: 15500\n",
      "Train Loss: 0.03250905126333237\n",
      "Average Return: 9.199478149414062\n",
      "\n",
      "Iteration: 16000\n",
      "Train Loss: 0.0326300710439682\n",
      "Average Return: 7.690463542938232\n",
      "\n",
      "Iteration: 16500\n",
      "Train Loss: 0.02189452201128006\n",
      "Average Return: 8.55750846862793\n",
      "\n",
      "Iteration: 17000\n",
      "Train Loss: 0.022423025220632553\n",
      "Average Return: 4.3969597816467285\n",
      "\n",
      "Iteration: 17500\n",
      "Train Loss: 0.018321488052606583\n",
      "Average Return: 10.08188533782959\n",
      "\n",
      "Iteration: 18000\n",
      "Train Loss: 0.016932111233472824\n",
      "Average Return: 10.697857856750488\n",
      "\n",
      "Iteration: 18500\n",
      "Train Loss: 0.031635209918022156\n",
      "Average Return: 4.743137836456299\n",
      "\n",
      "Iteration: 19000\n",
      "Train Loss: 0.050325289368629456\n",
      "Average Return: 9.593366622924805\n",
      "\n",
      "Iteration: 19500\n",
      "Train Loss: 0.013825947418808937\n",
      "Average Return: 9.318181037902832\n",
      "\n",
      "Iteration: 20000\n",
      "Train Loss: 0.050064317882061005\n",
      "Average Return: 7.219212055206299\n",
      "\n",
      "Iteration: 20500\n",
      "Train Loss: 0.040413014590740204\n",
      "Average Return: 13.792044639587402\n",
      "\n",
      "Iteration: 21000\n",
      "Train Loss: 0.024579692631959915\n",
      "Average Return: 3.2257487773895264\n",
      "\n",
      "Iteration: 21500\n",
      "Train Loss: 0.019655616953969002\n",
      "Average Return: 11.26279067993164\n",
      "\n",
      "Iteration: 22000\n",
      "Train Loss: 0.017458023503422737\n",
      "Average Return: 5.772050380706787\n",
      "\n",
      "Iteration: 22500\n",
      "Train Loss: 0.013853411190211773\n",
      "Average Return: 9.893439292907715\n",
      "\n",
      "Iteration: 23000\n",
      "Train Loss: 0.00807960331439972\n",
      "Average Return: 14.253704071044922\n",
      "\n",
      "Iteration: 23500\n",
      "Train Loss: 0.009422041475772858\n",
      "Average Return: 12.653514862060547\n",
      "\n",
      "Iteration: 24000\n",
      "Train Loss: 0.01571056805551052\n",
      "Average Return: 11.49100399017334\n",
      "\n",
      "Iteration: 24500\n",
      "Train Loss: 0.016629746183753014\n",
      "Average Return: 11.574601173400879\n",
      "\n",
      "Iteration: 25000\n",
      "Train Loss: 0.03333015367388725\n",
      "Average Return: 6.897262096405029\n",
      "\n",
      "Iteration: 25500\n",
      "Train Loss: 0.021010149270296097\n",
      "Average Return: 12.900124549865723\n",
      "\n",
      "Iteration: 26000\n",
      "Train Loss: 0.030970077961683273\n",
      "Average Return: 11.445958137512207\n",
      "\n",
      "Iteration: 26500\n",
      "Train Loss: 0.046299055218696594\n",
      "Average Return: 7.798789024353027\n",
      "\n",
      "Iteration: 27000\n",
      "Train Loss: 0.01557313185185194\n",
      "Average Return: 10.259819984436035\n",
      "\n",
      "Iteration: 27500\n",
      "Train Loss: 0.045786160975694656\n",
      "Average Return: 6.717820167541504\n",
      "\n",
      "Iteration: 28000\n",
      "Train Loss: 0.03835804760456085\n",
      "Average Return: 12.38538932800293\n",
      "\n",
      "Iteration: 28500\n",
      "Train Loss: 0.023814544081687927\n",
      "Average Return: 10.839722633361816\n",
      "\n",
      "Iteration: 29000\n",
      "Train Loss: 0.028153907507658005\n",
      "Average Return: 7.716271877288818\n",
      "\n",
      "Iteration: 29500\n",
      "Train Loss: 0.04455713927745819\n",
      "Average Return: 12.48973274230957\n",
      "\n",
      "Iteration: 30000\n",
      "Train Loss: 0.016198815777897835\n",
      "Average Return: 10.905515670776367\n",
      "\n",
      "Iteration: 30500\n",
      "Train Loss: 0.01988566666841507\n",
      "Average Return: 7.0996904373168945\n",
      "\n",
      "Iteration: 31000\n",
      "Train Loss: 0.031236525624990463\n",
      "Average Return: 6.998242378234863\n",
      "\n",
      "Iteration: 31500\n",
      "Train Loss: 0.03237299621105194\n",
      "Average Return: 6.760446548461914\n",
      "\n",
      "Iteration: 32000\n",
      "Train Loss: 0.028894683346152306\n",
      "Average Return: 8.699671745300293\n",
      "\n",
      "Iteration: 32500\n",
      "Train Loss: 0.016278192400932312\n",
      "Average Return: 7.516851425170898\n",
      "\n",
      "Iteration: 33000\n",
      "Train Loss: 0.03837154433131218\n",
      "Average Return: 3.707742691040039\n",
      "\n",
      "Iteration: 33500\n",
      "Train Loss: 0.020024534314870834\n",
      "Average Return: 7.491837024688721\n",
      "\n",
      "Iteration: 34000\n",
      "Train Loss: 0.061938509345054626\n",
      "Average Return: 9.766465187072754\n",
      "\n",
      "Iteration: 34500\n",
      "Train Loss: 0.019076712429523468\n",
      "Average Return: 7.467236042022705\n",
      "\n",
      "Iteration: 35000\n",
      "Train Loss: 0.061052244156599045\n",
      "Average Return: 5.36392068862915\n",
      "\n",
      "Iteration: 35500\n",
      "Train Loss: 0.03803953528404236\n",
      "Average Return: 7.179227352142334\n",
      "\n",
      "Iteration: 36000\n",
      "Train Loss: 0.0452568382024765\n",
      "Average Return: 5.062232494354248\n",
      "\n",
      "Iteration: 36500\n",
      "Train Loss: 0.03055907040834427\n",
      "Average Return: 10.710384368896484\n",
      "\n",
      "Iteration: 37000\n",
      "Train Loss: 0.031712040305137634\n",
      "Average Return: 6.195154190063477\n",
      "\n",
      "Iteration: 37500\n",
      "Train Loss: 0.035467036068439484\n",
      "Average Return: 2.750307559967041\n",
      "\n",
      "Iteration: 38000\n",
      "Train Loss: 0.028440743684768677\n",
      "Average Return: 1.3755608797073364\n",
      "\n",
      "Iteration: 38500\n",
      "Train Loss: 0.036649562418460846\n",
      "Average Return: 3.570354700088501\n",
      "\n",
      "Iteration: 39000\n",
      "Train Loss: 0.017250915989279747\n",
      "Average Return: 5.977653503417969\n",
      "\n",
      "Iteration: 39500\n",
      "Train Loss: 0.024098996073007584\n",
      "Average Return: 6.180083274841309\n",
      "\n",
      "Iteration: 40000\n",
      "Train Loss: 0.02491685375571251\n",
      "Average Return: 4.771190166473389\n",
      "\n",
      "Iteration: 40500\n",
      "Train Loss: 0.021884948015213013\n",
      "Average Return: 3.2150113582611084\n",
      "\n",
      "Iteration: 41000\n",
      "Train Loss: 0.027589330449700356\n",
      "Average Return: 7.362988471984863\n",
      "\n",
      "Iteration: 41500\n",
      "Train Loss: 0.02097076177597046\n",
      "Average Return: 5.480828285217285\n",
      "\n",
      "Iteration: 42000\n",
      "Train Loss: 0.025486167520284653\n",
      "Average Return: -0.16394032537937164\n",
      "\n",
      "Iteration: 42500\n",
      "Train Loss: 0.01258038729429245\n",
      "Average Return: 2.8468191623687744\n",
      "\n",
      "Iteration: 43000\n",
      "Train Loss: 0.049134038388729095\n",
      "Average Return: 2.2510318756103516\n",
      "\n",
      "Iteration: 43500\n",
      "Train Loss: 0.017848428338766098\n",
      "Average Return: 6.547842979431152\n",
      "\n",
      "Iteration: 44000\n",
      "Train Loss: 0.016090869903564453\n",
      "Average Return: 4.686088562011719\n",
      "\n",
      "Iteration: 44500\n",
      "Train Loss: 0.01891404762864113\n",
      "Average Return: 4.187236785888672\n",
      "\n",
      "Iteration: 45000\n",
      "Train Loss: 0.015487872995436192\n",
      "Average Return: 6.768958568572998\n",
      "\n",
      "Iteration: 45500\n",
      "Train Loss: 0.03353719785809517\n",
      "Average Return: 6.37083625793457\n",
      "\n",
      "Iteration: 46000\n",
      "Train Loss: 0.06075591221451759\n",
      "Average Return: 2.0240588188171387\n",
      "\n",
      "Iteration: 46500\n",
      "Train Loss: 0.031929027289152145\n",
      "Average Return: 2.7716217041015625\n",
      "\n",
      "Iteration: 47000\n",
      "Train Loss: 0.019455399364233017\n",
      "Average Return: 6.800055027008057\n",
      "\n",
      "Iteration: 47500\n",
      "Train Loss: 0.019989725202322006\n",
      "Average Return: 2.248434543609619\n",
      "\n",
      "Iteration: 48000\n",
      "Train Loss: 0.021674666553735733\n",
      "Average Return: 7.048463344573975\n",
      "\n",
      "Iteration: 48500\n",
      "Train Loss: 0.022761300206184387\n",
      "Average Return: 3.5517687797546387\n",
      "\n",
      "Iteration: 49000\n",
      "Train Loss: 0.022192731499671936\n",
      "Average Return: 2.388579845428467\n",
      "\n",
      "Iteration: 49500\n",
      "Train Loss: 0.02974877879023552\n",
      "Average Return: 1.1948224306106567\n",
      "\n",
      "Iteration: 50000\n",
      "Train Loss: 0.01138025987893343\n",
      "Average Return: 4.781389236450195\n",
      "Collecting Initial Samples...\n",
      "Training has started...\n",
      "\n",
      "New best average return found at 34.10908508300781! Saving checkpoint at iteration 0\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 0.002537816297262907\n",
      "Average Return: 31.200023651123047\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 0.006933087483048439\n",
      "Average Return: 34.06086730957031\n",
      "\n",
      "Iteration: 1500\n",
      "Train Loss: 0.0026125642471015453\n",
      "Average Return: 33.45402145385742\n",
      "\n",
      "New best average return found at 37.77055740356445! Saving checkpoint at iteration 2000\n",
      "\n",
      "Iteration: 2000\n",
      "Train Loss: 0.001969135133549571\n",
      "Average Return: 37.77055740356445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\metrics\\trading\\sortino.py:20: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.exp(average_returns/std_downfall_returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best average return found at 39.94670104980469! Saving checkpoint at iteration 2500\n",
      "\n",
      "Iteration: 2500\n",
      "Train Loss: 0.034266166388988495\n",
      "Average Return: 39.94670104980469\n",
      "\n",
      "Iteration: 3000\n",
      "Train Loss: 0.005774769000709057\n",
      "Average Return: 29.007732391357422\n",
      "\n",
      "Iteration: 3500\n",
      "Train Loss: 0.007667203433811665\n",
      "Average Return: 29.335298538208008\n",
      "\n",
      "Iteration: 4000\n",
      "Train Loss: 0.06139087677001953\n",
      "Average Return: 32.82987976074219\n",
      "\n",
      "Iteration: 4500\n",
      "Train Loss: 0.024734649807214737\n",
      "Average Return: 34.1562614440918\n",
      "\n",
      "Iteration: 5000\n",
      "Train Loss: 0.013552075251936913\n",
      "Average Return: 32.62443542480469\n",
      "\n",
      "Iteration: 5500\n",
      "Train Loss: 0.028917577117681503\n",
      "Average Return: 32.91023635864258\n",
      "\n",
      "Iteration: 6000\n",
      "Train Loss: 0.03204839304089546\n",
      "Average Return: 34.35884475708008\n",
      "\n",
      "Iteration: 6500\n",
      "Train Loss: 0.0349116176366806\n",
      "Average Return: 34.153526306152344\n",
      "\n",
      "Iteration: 7000\n",
      "Train Loss: 0.04254339635372162\n",
      "Average Return: 28.3703670501709\n",
      "\n",
      "Iteration: 7500\n",
      "Train Loss: 0.052191294729709625\n",
      "Average Return: 30.228717803955078\n",
      "\n",
      "Iteration: 8000\n",
      "Train Loss: 0.04991018399596214\n",
      "Average Return: 25.82896614074707\n",
      "\n",
      "Iteration: 8500\n",
      "Train Loss: 0.048340607434511185\n",
      "Average Return: 29.7249698638916\n",
      "\n",
      "Iteration: 9000\n",
      "Train Loss: 0.06404318660497665\n",
      "Average Return: 31.020275115966797\n",
      "\n",
      "Iteration: 9500\n",
      "Train Loss: 0.016021136194467545\n",
      "Average Return: 30.696508407592773\n",
      "\n",
      "Iteration: 10000\n",
      "Train Loss: 0.026559285819530487\n",
      "Average Return: 31.063308715820312\n",
      "\n",
      "Iteration: 10500\n",
      "Train Loss: 0.0239415243268013\n",
      "Average Return: 29.14960479736328\n",
      "\n",
      "Iteration: 11000\n",
      "Train Loss: 0.02289266139268875\n",
      "Average Return: 25.966856002807617\n",
      "\n",
      "Iteration: 11500\n",
      "Train Loss: 0.024493448436260223\n",
      "Average Return: 25.786426544189453\n",
      "\n",
      "Iteration: 12000\n",
      "Train Loss: 0.08023775368928909\n",
      "Average Return: 27.808897018432617\n",
      "\n",
      "Iteration: 12500\n",
      "Train Loss: 0.06974662095308304\n",
      "Average Return: 29.306150436401367\n",
      "\n",
      "Iteration: 13000\n",
      "Train Loss: 0.05996284261345863\n",
      "Average Return: 29.572917938232422\n",
      "\n",
      "Iteration: 13500\n",
      "Train Loss: 0.06344173103570938\n",
      "Average Return: 30.617090225219727\n",
      "\n",
      "Iteration: 14000\n",
      "Train Loss: 0.07959487289190292\n",
      "Average Return: 30.85843276977539\n",
      "\n",
      "Iteration: 14500\n",
      "Train Loss: 0.03033963218331337\n",
      "Average Return: 31.23116683959961\n",
      "\n",
      "Iteration: 15000\n",
      "Train Loss: 0.08771201968193054\n",
      "Average Return: 29.86888885498047\n",
      "\n",
      "Iteration: 15500\n",
      "Train Loss: 0.057593148201704025\n",
      "Average Return: 28.6575984954834\n",
      "\n",
      "Iteration: 16000\n",
      "Train Loss: 0.08125333487987518\n",
      "Average Return: 25.232393264770508\n",
      "\n",
      "Iteration: 16500\n",
      "Train Loss: 0.03299815207719803\n",
      "Average Return: 32.03684997558594\n",
      "\n",
      "Iteration: 17000\n",
      "Train Loss: 0.05263492092490196\n",
      "Average Return: 28.461441040039062\n",
      "\n",
      "Iteration: 17500\n",
      "Train Loss: 0.037757690995931625\n",
      "Average Return: 27.044292449951172\n",
      "\n",
      "Iteration: 18000\n",
      "Train Loss: 0.058498565107584\n",
      "Average Return: 4.127450942993164\n",
      "\n",
      "Iteration: 18500\n",
      "Train Loss: 0.07105731964111328\n",
      "Average Return: 18.592140197753906\n",
      "\n",
      "Iteration: 19000\n",
      "Train Loss: 0.07369571924209595\n",
      "Average Return: 24.90118408203125\n",
      "\n",
      "Iteration: 19500\n",
      "Train Loss: 0.0502157099545002\n",
      "Average Return: 15.759587287902832\n",
      "\n",
      "Iteration: 20000\n",
      "Train Loss: 0.14619594812393188\n",
      "Average Return: 17.388025283813477\n",
      "\n",
      "Iteration: 20500\n",
      "Train Loss: 0.06954896450042725\n",
      "Average Return: 26.81870460510254\n",
      "\n",
      "Iteration: 21000\n",
      "Train Loss: 0.06699179112911224\n",
      "Average Return: 23.731571197509766\n",
      "\n",
      "Iteration: 21500\n",
      "Train Loss: 0.05207670480012894\n",
      "Average Return: 21.41900062561035\n",
      "\n",
      "Iteration: 22000\n",
      "Train Loss: 0.03304599970579147\n",
      "Average Return: 13.506303787231445\n",
      "\n",
      "Iteration: 22500\n",
      "Train Loss: 0.01594848558306694\n",
      "Average Return: 23.503259658813477\n",
      "\n",
      "Iteration: 23000\n",
      "Train Loss: 0.03577682375907898\n",
      "Average Return: 25.357751846313477\n",
      "\n",
      "Iteration: 23500\n",
      "Train Loss: 0.024920377880334854\n",
      "Average Return: 15.63839340209961\n",
      "\n",
      "Iteration: 24000\n",
      "Train Loss: 0.036864008754491806\n",
      "Average Return: 19.830184936523438\n",
      "\n",
      "Iteration: 24500\n",
      "Train Loss: 0.061706945300102234\n",
      "Average Return: 13.751503944396973\n",
      "\n",
      "Iteration: 25000\n",
      "Train Loss: 0.046818334609270096\n",
      "Average Return: 17.239166259765625\n",
      "\n",
      "Iteration: 25500\n",
      "Train Loss: 0.04574889317154884\n",
      "Average Return: 14.632869720458984\n",
      "\n",
      "Iteration: 26000\n",
      "Train Loss: 0.07438023388385773\n",
      "Average Return: 13.273036003112793\n",
      "\n",
      "Iteration: 26500\n",
      "Train Loss: 0.06216013804078102\n",
      "Average Return: 22.725046157836914\n",
      "\n",
      "Iteration: 27000\n",
      "Train Loss: 0.043651387095451355\n",
      "Average Return: 17.57353973388672\n",
      "\n",
      "Iteration: 27500\n",
      "Train Loss: 0.08980917930603027\n",
      "Average Return: 17.97040367126465\n",
      "\n",
      "Iteration: 28000\n",
      "Train Loss: 0.06379285454750061\n",
      "Average Return: 15.827261924743652\n",
      "\n",
      "Iteration: 28500\n",
      "Train Loss: 0.04863153398036957\n",
      "Average Return: 20.340059280395508\n",
      "\n",
      "Iteration: 29000\n",
      "Train Loss: 0.06230044364929199\n",
      "Average Return: 23.73661231994629\n",
      "\n",
      "Iteration: 29500\n",
      "Train Loss: 0.06871373951435089\n",
      "Average Return: 19.979610443115234\n",
      "\n",
      "Iteration: 30000\n",
      "Train Loss: 0.03844873607158661\n",
      "Average Return: 16.344757080078125\n",
      "\n",
      "Iteration: 30500\n",
      "Train Loss: 0.053226396441459656\n",
      "Average Return: 17.743999481201172\n",
      "\n",
      "Iteration: 31000\n",
      "Train Loss: 0.06486459076404572\n",
      "Average Return: 14.00668716430664\n",
      "\n",
      "Iteration: 31500\n",
      "Train Loss: 0.05507196485996246\n",
      "Average Return: 16.05389976501465\n",
      "\n",
      "Iteration: 32000\n",
      "Train Loss: 0.05904246121644974\n",
      "Average Return: 19.068511962890625\n",
      "\n",
      "Iteration: 32500\n",
      "Train Loss: 0.04116172343492508\n",
      "Average Return: 19.019872665405273\n",
      "\n",
      "Iteration: 33000\n",
      "Train Loss: 0.036570578813552856\n",
      "Average Return: 21.443361282348633\n",
      "\n",
      "Iteration: 33500\n",
      "Train Loss: 0.056958314031362534\n",
      "Average Return: 11.531023025512695\n",
      "\n",
      "Iteration: 34000\n",
      "Train Loss: 0.11646363139152527\n",
      "Average Return: 20.404600143432617\n",
      "\n",
      "Iteration: 34500\n",
      "Train Loss: 0.04192769154906273\n",
      "Average Return: 21.796239852905273\n",
      "\n",
      "Iteration: 35000\n",
      "Train Loss: 0.0959651991724968\n",
      "Average Return: 16.83807945251465\n",
      "\n",
      "Iteration: 35500\n",
      "Train Loss: 0.07592760771512985\n",
      "Average Return: 23.302209854125977\n",
      "\n",
      "Iteration: 36000\n",
      "Train Loss: 0.0485270731151104\n",
      "Average Return: 19.741628646850586\n",
      "\n",
      "Iteration: 36500\n",
      "Train Loss: 0.07924202084541321\n",
      "Average Return: 13.48375415802002\n",
      "\n",
      "Iteration: 37000\n",
      "Train Loss: 0.04375246912240982\n",
      "Average Return: 13.591341018676758\n",
      "\n",
      "Iteration: 37500\n",
      "Train Loss: 0.06209485977888107\n",
      "Average Return: 16.498003005981445\n",
      "\n",
      "Iteration: 38000\n",
      "Train Loss: 0.056244514882564545\n",
      "Average Return: 9.23832893371582\n",
      "\n",
      "Iteration: 38500\n",
      "Train Loss: 0.06988231092691422\n",
      "Average Return: 16.196929931640625\n",
      "\n",
      "Iteration: 39000\n",
      "Train Loss: 0.03785201907157898\n",
      "Average Return: 20.797269821166992\n",
      "\n",
      "Iteration: 39500\n",
      "Train Loss: 0.03135398402810097\n",
      "Average Return: 16.402835845947266\n",
      "\n",
      "Iteration: 40000\n",
      "Train Loss: 0.032313402742147446\n",
      "Average Return: 9.930054664611816\n",
      "\n",
      "Iteration: 40500\n",
      "Train Loss: 0.05194808915257454\n",
      "Average Return: 16.715988159179688\n",
      "\n",
      "Iteration: 41000\n",
      "Train Loss: 0.06636090576648712\n",
      "Average Return: 20.471113204956055\n",
      "\n",
      "Iteration: 41500\n",
      "Train Loss: 0.037946391850709915\n",
      "Average Return: 16.978837966918945\n",
      "\n",
      "Iteration: 42000\n",
      "Train Loss: 0.03947664424777031\n",
      "Average Return: 8.906356811523438\n",
      "\n",
      "Iteration: 42500\n",
      "Train Loss: 0.03765684366226196\n",
      "Average Return: 18.508569717407227\n",
      "\n",
      "Iteration: 43000\n",
      "Train Loss: 0.07225344330072403\n",
      "Average Return: 19.092105865478516\n",
      "\n",
      "Iteration: 43500\n",
      "Train Loss: 0.04369201138615608\n",
      "Average Return: 20.171039581298828\n",
      "\n",
      "Iteration: 44000\n",
      "Train Loss: 0.038076091557741165\n",
      "Average Return: 22.157569885253906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\metrics\\trading\\sortino.py:20: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(average_returns/std_downfall_returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 44500\n",
      "Train Loss: 0.04125214368104935\n",
      "Average Return: 15.408001899719238\n",
      "\n",
      "Iteration: 45000\n",
      "Train Loss: 0.02894483134150505\n",
      "Average Return: 21.040027618408203\n",
      "\n",
      "Iteration: 45500\n",
      "Train Loss: 0.04462943598628044\n",
      "Average Return: 19.959476470947266\n",
      "\n",
      "Iteration: 46000\n",
      "Train Loss: 0.08680948615074158\n",
      "Average Return: 20.964540481567383\n",
      "\n",
      "Iteration: 46500\n",
      "Train Loss: 0.042590994387865067\n",
      "Average Return: 14.944565773010254\n",
      "\n",
      "Iteration: 47000\n",
      "Train Loss: 0.03898043930530548\n",
      "Average Return: 18.49053955078125\n",
      "\n",
      "Iteration: 47500\n",
      "Train Loss: 0.03178520128130913\n",
      "Average Return: 19.17402458190918\n",
      "\n",
      "Iteration: 48000\n",
      "Train Loss: 0.0307757668197155\n",
      "Average Return: 16.51491355895996\n",
      "\n",
      "Iteration: 48500\n",
      "Train Loss: 0.028123019263148308\n",
      "Average Return: 20.195476531982422\n",
      "\n",
      "Iteration: 49000\n",
      "Train Loss: 0.030516181141138077\n",
      "Average Return: 19.901241302490234\n",
      "\n",
      "Iteration: 49500\n",
      "Train Loss: 0.046743009239435196\n",
      "Average Return: 20.680295944213867\n",
      "\n",
      "Iteration: 50000\n",
      "Train Loss: 0.02945210412144661\n",
      "Average Return: 15.680879592895508\n",
      "Collecting Initial Samples...\n",
      "Training has started...\n",
      "\n",
      "New best average return found at 11.935018539428711! Saving checkpoint at iteration 0\n",
      "\n",
      "New best average return found at 14.955881118774414! Saving checkpoint at iteration 500\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 0.004523931536823511\n",
      "Average Return: 14.955881118774414\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 0.0043364474549889565\n",
      "Average Return: 11.483203887939453\n",
      "\n",
      "Iteration: 1500\n",
      "Train Loss: 0.0021362323313951492\n",
      "Average Return: 13.195969581604004\n",
      "\n",
      "Iteration: 2000\n",
      "Train Loss: 0.002057366305962205\n",
      "Average Return: 11.474380493164062\n",
      "\n",
      "Iteration: 2500\n",
      "Train Loss: 0.004311296157538891\n",
      "Average Return: 12.524772644042969\n",
      "\n",
      "Iteration: 3000\n",
      "Train Loss: 0.003811764530837536\n",
      "Average Return: 14.369969367980957\n",
      "\n",
      "Iteration: 3500\n",
      "Train Loss: 0.0019423624034970999\n",
      "Average Return: 13.7164945602417\n",
      "\n",
      "Iteration: 4000\n",
      "Train Loss: 0.0034672371111810207\n",
      "Average Return: 11.090452194213867\n",
      "\n",
      "Iteration: 4500\n",
      "Train Loss: 0.004702760837972164\n",
      "Average Return: 12.864175796508789\n",
      "\n",
      "Iteration: 5000\n",
      "Train Loss: 0.003424848197028041\n",
      "Average Return: 11.992714881896973\n",
      "\n",
      "Iteration: 5500\n",
      "Train Loss: 0.005517955869436264\n",
      "Average Return: 11.956507682800293\n",
      "\n",
      "Iteration: 6000\n",
      "Train Loss: 0.005635446403175592\n",
      "Average Return: 11.324484825134277\n",
      "\n",
      "Iteration: 6500\n",
      "Train Loss: 0.007668604142963886\n",
      "Average Return: 9.967278480529785\n",
      "\n",
      "Iteration: 7000\n",
      "Train Loss: 0.004516273736953735\n",
      "Average Return: 11.718586921691895\n",
      "\n",
      "Iteration: 7500\n",
      "Train Loss: 0.005217655561864376\n",
      "Average Return: 9.197980880737305\n",
      "\n",
      "Iteration: 8000\n",
      "Train Loss: 0.015565330162644386\n",
      "Average Return: 10.15944766998291\n",
      "\n",
      "Iteration: 8500\n",
      "Train Loss: 0.010868363082408905\n",
      "Average Return: 8.416643142700195\n",
      "\n",
      "Iteration: 9000\n",
      "Train Loss: 0.017606016248464584\n",
      "Average Return: 9.984190940856934\n",
      "\n",
      "Iteration: 9500\n",
      "Train Loss: 0.008170416578650475\n",
      "Average Return: 3.705024480819702\n",
      "\n",
      "Iteration: 10000\n",
      "Train Loss: 0.006825306452810764\n",
      "Average Return: 10.7954683303833\n",
      "\n",
      "Iteration: 10500\n",
      "Train Loss: 0.01768549717962742\n",
      "Average Return: 9.141828536987305\n",
      "\n",
      "Iteration: 11000\n",
      "Train Loss: 0.011330166831612587\n",
      "Average Return: 8.795599937438965\n",
      "\n",
      "Iteration: 11500\n",
      "Train Loss: 0.005865578074008226\n",
      "Average Return: 7.274608612060547\n",
      "\n",
      "Iteration: 12000\n",
      "Train Loss: 0.011772388592362404\n",
      "Average Return: 10.265366554260254\n",
      "\n",
      "Iteration: 12500\n",
      "Train Loss: 0.00655579287558794\n",
      "Average Return: 9.287690162658691\n",
      "\n",
      "Iteration: 13000\n",
      "Train Loss: 0.005719625391066074\n",
      "Average Return: 9.916077613830566\n",
      "\n",
      "Iteration: 13500\n",
      "Train Loss: 0.012934715487062931\n",
      "Average Return: 8.673670768737793\n",
      "\n",
      "Iteration: 14000\n",
      "Train Loss: 0.01755649968981743\n",
      "Average Return: 9.15993881225586\n",
      "\n",
      "Iteration: 14500\n",
      "Train Loss: 0.005259310826659203\n",
      "Average Return: 10.550538063049316\n",
      "\n",
      "Iteration: 15000\n",
      "Train Loss: 0.018925294280052185\n",
      "Average Return: 7.690305233001709\n",
      "\n",
      "Iteration: 15500\n",
      "Train Loss: 0.03170425444841385\n",
      "Average Return: 9.070964813232422\n",
      "\n",
      "Iteration: 16000\n",
      "Train Loss: 0.013568374328315258\n",
      "Average Return: 8.883942604064941\n",
      "\n",
      "Iteration: 16500\n",
      "Train Loss: 0.011713391169905663\n",
      "Average Return: 6.018958568572998\n",
      "\n",
      "Iteration: 17000\n",
      "Train Loss: 0.04398383945226669\n",
      "Average Return: 5.26186990737915\n",
      "\n",
      "Iteration: 17500\n",
      "Train Loss: 0.01820768043398857\n",
      "Average Return: 9.200323104858398\n",
      "\n",
      "Iteration: 18000\n",
      "Train Loss: 0.016726817935705185\n",
      "Average Return: 8.299177169799805\n",
      "\n",
      "Iteration: 18500\n",
      "Train Loss: 0.029975971207022667\n",
      "Average Return: 8.649502754211426\n",
      "\n",
      "Iteration: 19000\n",
      "Train Loss: 0.007474865298718214\n",
      "Average Return: 11.952736854553223\n",
      "\n",
      "Iteration: 19500\n",
      "Train Loss: 0.008752954192459583\n",
      "Average Return: 6.4291605949401855\n",
      "\n",
      "Iteration: 20000\n",
      "Train Loss: 0.02443256974220276\n",
      "Average Return: 9.49043083190918\n",
      "\n",
      "Iteration: 20500\n",
      "Train Loss: 0.01857660338282585\n",
      "Average Return: 4.098020553588867\n",
      "\n",
      "Iteration: 21000\n",
      "Train Loss: 0.008810177445411682\n",
      "Average Return: 6.922904968261719\n",
      "\n",
      "Iteration: 21500\n",
      "Train Loss: 0.028315242379903793\n",
      "Average Return: -9.9949951171875\n",
      "\n",
      "Iteration: 22000\n",
      "Train Loss: 0.0395999401807785\n",
      "Average Return: 6.265463829040527\n",
      "\n",
      "Iteration: 22500\n",
      "Train Loss: 0.01675633154809475\n",
      "Average Return: 2.474407434463501\n",
      "\n",
      "Iteration: 23000\n",
      "Train Loss: 0.01907462254166603\n",
      "Average Return: 6.662038326263428\n",
      "\n",
      "Iteration: 23500\n",
      "Train Loss: 0.022198442369699478\n",
      "Average Return: 1.4069600105285645\n",
      "\n",
      "Iteration: 24000\n",
      "Train Loss: 0.03817672282457352\n",
      "Average Return: -8.681843757629395\n",
      "\n",
      "Iteration: 24500\n",
      "Train Loss: 0.028130266815423965\n",
      "Average Return: 5.800032615661621\n",
      "\n",
      "Iteration: 25000\n",
      "Train Loss: 0.023287003859877586\n",
      "Average Return: -0.8850982189178467\n",
      "\n",
      "Iteration: 25500\n",
      "Train Loss: 0.017569556832313538\n",
      "Average Return: 9.190768241882324\n",
      "\n",
      "Iteration: 26000\n",
      "Train Loss: 0.013367587700486183\n",
      "Average Return: 3.6475210189819336\n",
      "\n",
      "Iteration: 26500\n",
      "Train Loss: 0.024872152134776115\n",
      "Average Return: 7.003885269165039\n",
      "\n",
      "Iteration: 27000\n",
      "Train Loss: 0.018682245165109634\n",
      "Average Return: 2.8780357837677\n",
      "\n",
      "Iteration: 27500\n",
      "Train Loss: 0.030803140252828598\n",
      "Average Return: -0.36283010244369507\n",
      "\n",
      "Iteration: 28000\n",
      "Train Loss: 0.08452705293893814\n",
      "Average Return: 5.237058639526367\n",
      "\n",
      "Iteration: 28500\n",
      "Train Loss: 0.025858137756586075\n",
      "Average Return: 8.685718536376953\n",
      "\n",
      "Iteration: 29000\n",
      "Train Loss: 0.019223686307668686\n",
      "Average Return: 7.10528039932251\n",
      "\n",
      "Iteration: 29500\n",
      "Train Loss: 0.018856938928365707\n",
      "Average Return: 2.7191593647003174\n",
      "\n",
      "Iteration: 30000\n",
      "Train Loss: 0.03732678294181824\n",
      "Average Return: 4.699405193328857\n",
      "\n",
      "Iteration: 30500\n",
      "Train Loss: 0.02939692512154579\n",
      "Average Return: 8.681607246398926\n",
      "\n",
      "Iteration: 31000\n",
      "Train Loss: 0.031175412237644196\n",
      "Average Return: 10.346556663513184\n",
      "\n",
      "Iteration: 31500\n",
      "Train Loss: 0.030018359422683716\n",
      "Average Return: 8.5420503616333\n",
      "\n",
      "Iteration: 32000\n",
      "Train Loss: 0.03836410120129585\n",
      "Average Return: 6.446831226348877\n",
      "\n",
      "Iteration: 32500\n",
      "Train Loss: 0.024859363213181496\n",
      "Average Return: 10.593304634094238\n",
      "\n",
      "Iteration: 33000\n",
      "Train Loss: 0.014882061630487442\n",
      "Average Return: 3.1744236946105957\n",
      "\n",
      "Iteration: 33500\n",
      "Train Loss: 0.022778434678912163\n",
      "Average Return: 7.2318854331970215\n",
      "\n",
      "Iteration: 34000\n",
      "Train Loss: 0.026423968374729156\n",
      "Average Return: 6.482220649719238\n",
      "\n",
      "Iteration: 34500\n",
      "Train Loss: 0.029350273311138153\n",
      "Average Return: 5.277362823486328\n",
      "\n",
      "Iteration: 35000\n",
      "Train Loss: 0.03225778043270111\n",
      "Average Return: 8.726433753967285\n",
      "\n",
      "Iteration: 35500\n",
      "Train Loss: 0.044116463512182236\n",
      "Average Return: 9.8250732421875\n",
      "\n",
      "Iteration: 36000\n",
      "Train Loss: 0.039680786430835724\n",
      "Average Return: 7.584718704223633\n",
      "\n",
      "Iteration: 36500\n",
      "Train Loss: 0.02963854745030403\n",
      "Average Return: 7.226746082305908\n",
      "\n",
      "Iteration: 37000\n",
      "Train Loss: 0.04276840388774872\n",
      "Average Return: 6.533945083618164\n",
      "\n",
      "Iteration: 37500\n",
      "Train Loss: 0.032217130064964294\n",
      "Average Return: 5.296351432800293\n",
      "\n",
      "Iteration: 38000\n",
      "Train Loss: 0.0295342355966568\n",
      "Average Return: 6.289473533630371\n",
      "\n",
      "Iteration: 38500\n",
      "Train Loss: 0.028687141835689545\n",
      "Average Return: 5.687506675720215\n",
      "\n",
      "Iteration: 39000\n",
      "Train Loss: 0.04811646044254303\n",
      "Average Return: -2.1398327350616455\n",
      "\n",
      "Iteration: 39500\n",
      "Train Loss: 0.020557891577482224\n",
      "Average Return: 8.988733291625977\n",
      "\n",
      "Iteration: 40000\n",
      "Train Loss: 0.014385644346475601\n",
      "Average Return: 7.714887619018555\n",
      "\n",
      "Iteration: 40500\n",
      "Train Loss: 0.02002379298210144\n",
      "Average Return: 7.515458583831787\n",
      "\n",
      "Iteration: 41000\n",
      "Train Loss: 0.022753974422812462\n",
      "Average Return: 6.630287170410156\n",
      "\n",
      "Iteration: 41500\n",
      "Train Loss: 0.013340256176888943\n",
      "Average Return: 2.9476444721221924\n",
      "\n",
      "Iteration: 42000\n",
      "Train Loss: 0.02095528319478035\n",
      "Average Return: 7.230216026306152\n",
      "\n",
      "Iteration: 42500\n",
      "Train Loss: 0.02288445457816124\n",
      "Average Return: 5.850903511047363\n",
      "\n",
      "Iteration: 43000\n",
      "Train Loss: 0.027925092726945877\n",
      "Average Return: 6.360163688659668\n",
      "\n",
      "Iteration: 43500\n",
      "Train Loss: 0.024000905454158783\n",
      "Average Return: 1.349226951599121\n",
      "\n",
      "Iteration: 44000\n",
      "Train Loss: 0.028155945241451263\n",
      "Average Return: 4.979063987731934\n",
      "\n",
      "Iteration: 44500\n",
      "Train Loss: 0.019845249131321907\n",
      "Average Return: 0.6627621650695801\n",
      "\n",
      "Iteration: 45000\n",
      "Train Loss: 0.024258844554424286\n",
      "Average Return: 7.1783013343811035\n",
      "\n",
      "Iteration: 45500\n",
      "Train Loss: 0.01332064252346754\n",
      "Average Return: 8.176642417907715\n",
      "\n",
      "Iteration: 46000\n",
      "Train Loss: 0.040803875774145126\n",
      "Average Return: 6.289683818817139\n",
      "\n",
      "Iteration: 46500\n",
      "Train Loss: 0.02271556854248047\n",
      "Average Return: 5.3038010597229\n",
      "\n",
      "Iteration: 47000\n",
      "Train Loss: 0.012511570006608963\n",
      "Average Return: 2.063358783721924\n",
      "\n",
      "Iteration: 47500\n",
      "Train Loss: 0.023795846849679947\n",
      "Average Return: 5.720069408416748\n",
      "\n",
      "Iteration: 48000\n",
      "Train Loss: 0.026180755347013474\n",
      "Average Return: 5.365420818328857\n",
      "\n",
      "Iteration: 48500\n",
      "Train Loss: 0.029155313968658447\n",
      "Average Return: 3.712216854095459\n",
      "\n",
      "Iteration: 49000\n",
      "Train Loss: 0.02559235505759716\n",
      "Average Return: 2.8599853515625\n",
      "\n",
      "Iteration: 49500\n",
      "Train Loss: 0.033464767038822174\n",
      "Average Return: 4.907556533813477\n",
      "\n",
      "Iteration: 50000\n",
      "Train Loss: 0.01893753558397293\n",
      "Average Return: 3.368246078491211\n",
      "Collecting Initial Samples...\n",
      "Training has started...\n",
      "\n",
      "New best average return found at 25.54416847229004! Saving checkpoint at iteration 0\n",
      "\n",
      "New best average return found at 30.108598709106445! Saving checkpoint at iteration 500\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 0.006008012220263481\n",
      "Average Return: 30.108598709106445\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 0.006200199015438557\n",
      "Average Return: 26.367063522338867\n",
      "\n",
      "Iteration: 1500\n",
      "Train Loss: 0.003251817775890231\n",
      "Average Return: 28.47507667541504\n",
      "\n",
      "Iteration: 2000\n",
      "Train Loss: 0.0031571865547448397\n",
      "Average Return: 26.430326461791992\n",
      "\n",
      "Iteration: 2500\n",
      "Train Loss: 0.008410974405705929\n",
      "Average Return: 29.109498977661133\n",
      "\n",
      "Iteration: 3000\n",
      "Train Loss: 0.008266575634479523\n",
      "Average Return: 29.233440399169922\n",
      "\n",
      "Iteration: 3500\n",
      "Train Loss: 0.005418046377599239\n",
      "Average Return: 28.856721878051758\n",
      "\n",
      "Iteration: 4000\n",
      "Train Loss: 0.011693181470036507\n",
      "Average Return: 28.31305694580078\n",
      "\n",
      "Iteration: 4500\n",
      "Train Loss: 0.014131656847894192\n",
      "Average Return: 27.28030776977539\n",
      "\n",
      "Iteration: 5000\n",
      "Train Loss: 0.008319733664393425\n",
      "Average Return: 27.75088119506836\n",
      "\n",
      "Iteration: 5500\n",
      "Train Loss: 0.01365104503929615\n",
      "Average Return: 27.785552978515625\n",
      "\n",
      "Iteration: 6000\n",
      "Train Loss: 0.01387118548154831\n",
      "Average Return: 27.148052215576172\n",
      "\n",
      "Iteration: 6500\n",
      "Train Loss: 0.013925958424806595\n",
      "Average Return: 26.341236114501953\n",
      "\n",
      "Iteration: 7000\n",
      "Train Loss: 0.01825311779975891\n",
      "Average Return: 25.711849212646484\n",
      "\n",
      "Iteration: 7500\n",
      "Train Loss: 0.010841649025678635\n",
      "Average Return: 21.496152877807617\n",
      "\n",
      "Iteration: 8000\n",
      "Train Loss: 0.046357411891222\n",
      "Average Return: 24.186269760131836\n",
      "\n",
      "Iteration: 8500\n",
      "Train Loss: 0.029383473098278046\n",
      "Average Return: 23.98967933654785\n",
      "\n",
      "Iteration: 9000\n",
      "Train Loss: 0.03886538743972778\n",
      "Average Return: 27.3649845123291\n",
      "\n",
      "Iteration: 9500\n",
      "Train Loss: 0.028364624828100204\n",
      "Average Return: 17.52958106994629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\metrics\\trading\\sortino.py:20: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.exp(average_returns/std_downfall_returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 10000\n",
      "Train Loss: 0.023667778819799423\n",
      "Average Return: 25.605226516723633\n",
      "\n",
      "Iteration: 10500\n",
      "Train Loss: 0.04919448122382164\n",
      "Average Return: 22.970861434936523\n",
      "\n",
      "Iteration: 11000\n",
      "Train Loss: 0.04298529401421547\n",
      "Average Return: 25.2415771484375\n",
      "\n",
      "Iteration: 11500\n",
      "Train Loss: 0.014252390712499619\n",
      "Average Return: 20.440105438232422\n",
      "\n",
      "Iteration: 12000\n",
      "Train Loss: 0.04621461033821106\n",
      "Average Return: 24.700584411621094\n",
      "\n",
      "Iteration: 12500\n",
      "Train Loss: 0.01949145272374153\n",
      "Average Return: 21.308048248291016\n",
      "\n",
      "Iteration: 13000\n",
      "Train Loss: 0.020260967314243317\n",
      "Average Return: 22.767114639282227\n",
      "\n",
      "Iteration: 13500\n",
      "Train Loss: 0.05006321519613266\n",
      "Average Return: 25.912738800048828\n",
      "\n",
      "Iteration: 14000\n",
      "Train Loss: 0.052836641669273376\n",
      "Average Return: 22.558948516845703\n",
      "\n",
      "Iteration: 14500\n",
      "Train Loss: 0.013921651989221573\n",
      "Average Return: 22.873308181762695\n",
      "\n",
      "Iteration: 15000\n",
      "Train Loss: 0.05262694135308266\n",
      "Average Return: 21.38837432861328\n",
      "\n",
      "Iteration: 15500\n",
      "Train Loss: 0.06189469248056412\n",
      "Average Return: 20.95713996887207\n",
      "\n",
      "Iteration: 16000\n",
      "Train Loss: 0.03962692245841026\n",
      "Average Return: 23.08103370666504\n",
      "\n",
      "Iteration: 16500\n",
      "Train Loss: 0.01411520130932331\n",
      "Average Return: 19.695003509521484\n",
      "\n",
      "Iteration: 17000\n",
      "Train Loss: 0.04801567271351814\n",
      "Average Return: 16.476301193237305\n",
      "\n",
      "Iteration: 17500\n",
      "Train Loss: 0.06899601221084595\n",
      "Average Return: 23.701974868774414\n",
      "\n",
      "Iteration: 18000\n",
      "Train Loss: 0.047886744141578674\n",
      "Average Return: 23.96906280517578\n",
      "\n",
      "Iteration: 18500\n",
      "Train Loss: 0.09249784797430038\n",
      "Average Return: 24.773950576782227\n",
      "\n",
      "Iteration: 19000\n",
      "Train Loss: 0.020909367129206657\n",
      "Average Return: 20.658533096313477\n",
      "\n",
      "Iteration: 19500\n",
      "Train Loss: 0.017536889761686325\n",
      "Average Return: 26.422901153564453\n",
      "\n",
      "Iteration: 20000\n",
      "Train Loss: 0.05451538413763046\n",
      "Average Return: 26.731966018676758\n",
      "\n",
      "Iteration: 20500\n",
      "Train Loss: 0.06687653064727783\n",
      "Average Return: 23.17949676513672\n",
      "\n",
      "Iteration: 21000\n",
      "Train Loss: 0.0227024108171463\n",
      "Average Return: 20.576751708984375\n",
      "\n",
      "Iteration: 21500\n",
      "Train Loss: 0.08165816962718964\n",
      "Average Return: -0.5503893494606018\n",
      "\n",
      "Iteration: 22000\n",
      "Train Loss: 0.11399029195308685\n",
      "Average Return: 26.32588768005371\n",
      "\n",
      "Iteration: 22500\n",
      "Train Loss: 0.044801194220781326\n",
      "Average Return: 24.08243751525879\n",
      "\n",
      "Iteration: 23000\n",
      "Train Loss: 0.05754027143120766\n",
      "Average Return: 24.124481201171875\n",
      "\n",
      "Iteration: 23500\n",
      "Train Loss: 0.053080447018146515\n",
      "Average Return: 21.76993751525879\n",
      "\n",
      "Iteration: 24000\n",
      "Train Loss: 0.09018775820732117\n",
      "Average Return: 22.637182235717773\n",
      "\n",
      "Iteration: 24500\n",
      "Train Loss: 0.06765824556350708\n",
      "Average Return: 12.420309066772461\n",
      "\n",
      "Iteration: 25000\n",
      "Train Loss: 0.08261337131261826\n",
      "Average Return: 20.958637237548828\n",
      "\n",
      "Iteration: 25500\n",
      "Train Loss: 0.05115184187889099\n",
      "Average Return: 17.464914321899414\n",
      "\n",
      "Iteration: 26000\n",
      "Train Loss: 0.03099171258509159\n",
      "Average Return: 20.440309524536133\n",
      "\n",
      "Iteration: 26500\n",
      "Train Loss: 0.06088459491729736\n",
      "Average Return: 23.2351016998291\n",
      "\n",
      "Iteration: 27000\n",
      "Train Loss: 0.047718554735183716\n",
      "Average Return: 8.544875144958496\n",
      "\n",
      "Iteration: 27500\n",
      "Train Loss: 0.07693937420845032\n",
      "Average Return: 15.210553169250488\n",
      "\n",
      "Iteration: 28000\n",
      "Train Loss: 0.09743107110261917\n",
      "Average Return: 24.947853088378906\n",
      "\n",
      "Iteration: 28500\n",
      "Train Loss: 0.06416995823383331\n",
      "Average Return: 25.14026641845703\n",
      "\n",
      "Iteration: 29000\n",
      "Train Loss: 0.023637518286705017\n",
      "Average Return: 13.210358619689941\n",
      "\n",
      "Iteration: 29500\n",
      "Train Loss: 0.030356287956237793\n",
      "Average Return: 16.924835205078125\n",
      "\n",
      "Iteration: 30000\n",
      "Train Loss: 0.0926276296377182\n",
      "Average Return: 3.568572521209717\n",
      "\n",
      "Iteration: 30500\n",
      "Train Loss: 0.06274880468845367\n",
      "Average Return: 16.85628318786621\n",
      "\n",
      "Iteration: 31000\n",
      "Train Loss: 0.04871189594268799\n",
      "Average Return: 15.385248184204102\n",
      "\n",
      "Iteration: 31500\n",
      "Train Loss: 0.11328762024641037\n",
      "Average Return: 23.895822525024414\n",
      "\n",
      "Iteration: 32000\n",
      "Train Loss: 0.07497890293598175\n",
      "Average Return: 17.665422439575195\n",
      "\n",
      "Iteration: 32500\n",
      "Train Loss: 0.0591481551527977\n",
      "Average Return: 13.058837890625\n",
      "\n",
      "Iteration: 33000\n",
      "Train Loss: 0.043995775282382965\n",
      "Average Return: 23.01688003540039\n",
      "\n",
      "Iteration: 33500\n",
      "Train Loss: 0.07801143079996109\n",
      "Average Return: 11.827924728393555\n",
      "\n",
      "Iteration: 34000\n",
      "Train Loss: 0.06825833022594452\n",
      "Average Return: 15.178179740905762\n",
      "\n",
      "Iteration: 34500\n",
      "Train Loss: 0.08153066039085388\n",
      "Average Return: 20.271520614624023\n",
      "\n",
      "Iteration: 35000\n",
      "Train Loss: 0.06703118979930878\n",
      "Average Return: 20.840354919433594\n",
      "\n",
      "Iteration: 35500\n",
      "Train Loss: 0.10555929690599442\n",
      "Average Return: 18.109844207763672\n",
      "\n",
      "Iteration: 36000\n",
      "Train Loss: 0.07161255180835724\n",
      "Average Return: 21.041200637817383\n",
      "\n",
      "Iteration: 36500\n",
      "Train Loss: 0.059207841753959656\n",
      "Average Return: 19.15926742553711\n",
      "\n",
      "Iteration: 37000\n",
      "Train Loss: 0.05943681672215462\n",
      "Average Return: 17.00664710998535\n",
      "\n",
      "Iteration: 37500\n",
      "Train Loss: 0.10150827467441559\n",
      "Average Return: 17.870187759399414\n",
      "\n",
      "Iteration: 38000\n",
      "Train Loss: 0.08457468450069427\n",
      "Average Return: 18.582740783691406\n",
      "\n",
      "Iteration: 38500\n",
      "Train Loss: 0.09199181199073792\n",
      "Average Return: 21.947669982910156\n",
      "\n",
      "Iteration: 39000\n",
      "Train Loss: 0.11629793047904968\n",
      "Average Return: 23.295156478881836\n",
      "\n",
      "Iteration: 39500\n",
      "Train Loss: 0.05503971129655838\n",
      "Average Return: 11.00740909576416\n",
      "\n",
      "Iteration: 40000\n",
      "Train Loss: 0.038802728056907654\n",
      "Average Return: 19.30893898010254\n",
      "\n",
      "Iteration: 40500\n",
      "Train Loss: 0.037870265543460846\n",
      "Average Return: 20.97408676147461\n",
      "\n",
      "Iteration: 41000\n",
      "Train Loss: 0.0335526317358017\n",
      "Average Return: 18.656982421875\n",
      "\n",
      "Iteration: 41500\n",
      "Train Loss: 0.030056796967983246\n",
      "Average Return: 21.34276008605957\n",
      "\n",
      "Iteration: 42000\n",
      "Train Loss: 0.04007428511977196\n",
      "Average Return: 19.37005043029785\n",
      "\n",
      "Iteration: 42500\n",
      "Train Loss: 0.05814077705144882\n",
      "Average Return: 17.033239364624023\n",
      "\n",
      "Iteration: 43000\n",
      "Train Loss: 0.0730142816901207\n",
      "Average Return: 18.570173263549805\n",
      "\n",
      "Iteration: 43500\n",
      "Train Loss: 0.07483215630054474\n",
      "Average Return: 17.153127670288086\n",
      "\n",
      "Iteration: 44000\n",
      "Train Loss: 0.05178770422935486\n",
      "Average Return: 17.37790870666504\n",
      "\n",
      "Iteration: 44500\n",
      "Train Loss: 0.04668435826897621\n",
      "Average Return: 15.57682991027832\n",
      "\n",
      "Iteration: 45000\n",
      "Train Loss: 0.030175071209669113\n",
      "Average Return: 15.961201667785645\n",
      "\n",
      "Iteration: 45500\n",
      "Train Loss: 0.03481091558933258\n",
      "Average Return: 17.325620651245117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\metrics\\trading\\sortino.py:20: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(average_returns/std_downfall_returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 46000\n",
      "Train Loss: 0.08623255044221878\n",
      "Average Return: 19.810997009277344\n",
      "\n",
      "Iteration: 46500\n",
      "Train Loss: 0.0432465597987175\n",
      "Average Return: 17.709928512573242\n",
      "\n",
      "Iteration: 47000\n",
      "Train Loss: 0.02915484458208084\n",
      "Average Return: 14.909781455993652\n",
      "\n",
      "Iteration: 47500\n",
      "Train Loss: 0.05309150367975235\n",
      "Average Return: 19.572769165039062\n",
      "\n",
      "Iteration: 48000\n",
      "Train Loss: 0.06309910863637924\n",
      "Average Return: 19.79684829711914\n",
      "\n",
      "Iteration: 48500\n",
      "Train Loss: 0.051611725240945816\n",
      "Average Return: 20.55994415283203\n",
      "\n",
      "Iteration: 49000\n",
      "Train Loss: 0.0620298907160759\n",
      "Average Return: 17.001340866088867\n",
      "\n",
      "Iteration: 49500\n",
      "Train Loss: 0.06673221290111542\n",
      "Average Return: 20.259021759033203\n",
      "\n",
      "Iteration: 50000\n",
      "Train Loss: 0.04690238833427429\n",
      "Average Return: 17.817033767700195\n",
      "Training has started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\metrics\\trading\\sharpe.py:20: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.exp(average_returns/std_returns)\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best average return found at -24.56235694885254! Saving checkpoint at iteration 0\n",
      "WARNING:tensorflow:From C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\agents\\tfagents\\ppo.py:152: ReplayBuffer.gather_all (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=True)` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\agents\\tfagents\\ppo.py:152: ReplayBuffer.gather_all (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=True)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best average return found at -9.082357406616211! Saving checkpoint at iteration 10\n",
      "\n",
      "Iteration: 10\n",
      "Train Loss: 76.96861267089844\n",
      "Average Return: -9.082357406616211\n",
      "\n",
      "New best average return found at -7.86840295791626! Saving checkpoint at iteration 20\n",
      "\n",
      "Iteration: 20\n",
      "Train Loss: 103.05207824707031\n",
      "Average Return: -7.86840295791626\n",
      "\n",
      "New best average return found at -2.489945888519287! Saving checkpoint at iteration 30\n",
      "\n",
      "Iteration: 30\n",
      "Train Loss: 2.7145440578460693\n",
      "Average Return: -2.489945888519287\n",
      "\n",
      "Iteration: 40\n",
      "Train Loss: 35.020503997802734\n",
      "Average Return: -3.2149956226348877\n",
      "\n",
      "New best average return found at -2.186483860015869! Saving checkpoint at iteration 50\n",
      "\n",
      "Iteration: 50\n",
      "Train Loss: 10.051138877868652\n",
      "Average Return: -2.186483860015869\n",
      "\n",
      "Iteration: 60\n",
      "Train Loss: 3.69128155708313\n",
      "Average Return: -2.2240147590637207\n",
      "\n",
      "New best average return found at -0.3787486255168915! Saving checkpoint at iteration 70\n",
      "\n",
      "Iteration: 70\n",
      "Train Loss: 5.146501064300537\n",
      "Average Return: -0.3787486255168915\n",
      "\n",
      "Iteration: 80\n",
      "Train Loss: 7.714166641235352\n",
      "Average Return: -2.5715386867523193\n",
      "\n",
      "Iteration: 90\n",
      "Train Loss: 14.636516571044922\n",
      "Average Return: -1.476660132408142\n",
      "\n",
      "Iteration: 100\n",
      "Train Loss: 4.605318069458008\n",
      "Average Return: -2.641249895095825\n",
      "\n",
      "Iteration: 110\n",
      "Train Loss: 43.96084213256836\n",
      "Average Return: -2.272692918777466\n",
      "\n",
      "Iteration: 120\n",
      "Train Loss: 17.04018211364746\n",
      "Average Return: -2.979212522506714\n",
      "\n",
      "Iteration: 130\n",
      "Train Loss: 7.790109634399414\n",
      "Average Return: -5.098542213439941\n",
      "\n",
      "Iteration: 140\n",
      "Train Loss: 12.899239540100098\n",
      "Average Return: -5.818873405456543\n",
      "\n",
      "Iteration: 150\n",
      "Train Loss: 4.841335773468018\n",
      "Average Return: -6.034965515136719\n",
      "\n",
      "Iteration: 160\n",
      "Train Loss: 3.559432029724121\n",
      "Average Return: -6.343156814575195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\metrics\\trading\\sortino.py:20: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.exp(average_returns/std_downfall_returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 170\n",
      "Train Loss: 52.83454513549805\n",
      "Average Return: -5.25177001953125\n",
      "\n",
      "Iteration: 180\n",
      "Train Loss: 4.874590873718262\n",
      "Average Return: -5.434153079986572\n",
      "\n",
      "Iteration: 190\n",
      "Train Loss: 6.426185607910156\n",
      "Average Return: -4.4396185874938965\n",
      "\n",
      "Iteration: 200\n",
      "Train Loss: 19.11383056640625\n",
      "Average Return: -4.696266174316406\n",
      "\n",
      "Iteration: 210\n",
      "Train Loss: 22.2358341217041\n",
      "Average Return: -3.511338710784912\n",
      "\n",
      "Iteration: 220\n",
      "Train Loss: 5.90990686416626\n",
      "Average Return: -3.7045938968658447\n",
      "\n",
      "Iteration: 230\n",
      "Train Loss: 63.00525665283203\n",
      "Average Return: -4.930360794067383\n",
      "\n",
      "Iteration: 240\n",
      "Train Loss: 29.741174697875977\n",
      "Average Return: -5.014069080352783\n",
      "\n",
      "Iteration: 250\n",
      "Train Loss: 8.847372055053711\n",
      "Average Return: -5.560515880584717\n",
      "\n",
      "Iteration: 260\n",
      "Train Loss: 13.609344482421875\n",
      "Average Return: -4.795721054077148\n",
      "\n",
      "Iteration: 270\n",
      "Train Loss: 11.227121353149414\n",
      "Average Return: -3.153813362121582\n",
      "\n",
      "Iteration: 280\n",
      "Train Loss: 8.795845031738281\n",
      "Average Return: -2.8686413764953613\n",
      "\n",
      "Iteration: 290\n",
      "Train Loss: 8.698908805847168\n",
      "Average Return: -3.1341447830200195\n",
      "\n",
      "Iteration: 300\n",
      "Train Loss: 21.29794692993164\n",
      "Average Return: -2.510129690170288\n",
      "\n",
      "Iteration: 310\n",
      "Train Loss: 25.955799102783203\n",
      "Average Return: -2.5931782722473145\n",
      "\n",
      "Iteration: 320\n",
      "Train Loss: 15.148880004882812\n",
      "Average Return: -3.000152826309204\n",
      "\n",
      "Iteration: 330\n",
      "Train Loss: 7.687265396118164\n",
      "Average Return: -1.2292410135269165\n",
      "\n",
      "Iteration: 340\n",
      "Train Loss: 29.113643646240234\n",
      "Average Return: -3.2740182876586914\n",
      "\n",
      "Iteration: 350\n",
      "Train Loss: 158.9314727783203\n",
      "Average Return: -2.7843995094299316\n",
      "\n",
      "Iteration: 360\n",
      "Train Loss: 4.4328460693359375\n",
      "Average Return: -2.1432418823242188\n",
      "\n",
      "Iteration: 370\n",
      "Train Loss: 28.14387321472168\n",
      "Average Return: -3.4337971210479736\n",
      "\n",
      "Iteration: 380\n",
      "Train Loss: 21.377809524536133\n",
      "Average Return: -2.1959352493286133\n",
      "\n",
      "Iteration: 390\n",
      "Train Loss: 16.413562774658203\n",
      "Average Return: -1.678128719329834\n",
      "\n",
      "Iteration: 400\n",
      "Train Loss: 39.3933219909668\n",
      "Average Return: -0.6533772945404053\n",
      "\n",
      "New best average return found at 0.28350386023521423! Saving checkpoint at iteration 410\n",
      "\n",
      "Iteration: 410\n",
      "Train Loss: 95.40131378173828\n",
      "Average Return: 0.28350386023521423\n",
      "\n",
      "New best average return found at 0.5001004338264465! Saving checkpoint at iteration 420\n",
      "\n",
      "Iteration: 420\n",
      "Train Loss: 23.34811782836914\n",
      "Average Return: 0.5001004338264465\n",
      "\n",
      "New best average return found at 0.5810246467590332! Saving checkpoint at iteration 430\n",
      "\n",
      "Iteration: 430\n",
      "Train Loss: 4.612255096435547\n",
      "Average Return: 0.5810246467590332\n",
      "\n",
      "New best average return found at 0.6418925523757935! Saving checkpoint at iteration 440\n",
      "\n",
      "Iteration: 440\n",
      "Train Loss: 6.331028461456299\n",
      "Average Return: 0.6418925523757935\n",
      "\n",
      "Iteration: 450\n",
      "Train Loss: 71.53086853027344\n",
      "Average Return: 0.5809102058410645\n",
      "\n",
      "Iteration: 460\n",
      "Train Loss: 71.24479675292969\n",
      "Average Return: 0.60104900598526\n",
      "\n",
      "Iteration: 470\n",
      "Train Loss: 50.6640625\n",
      "Average Return: 0.5340389013290405\n",
      "\n",
      "New best average return found at 0.7989462018013! Saving checkpoint at iteration 480\n",
      "\n",
      "Iteration: 480\n",
      "Train Loss: 39.69065856933594\n",
      "Average Return: 0.7989462018013\n",
      "\n",
      "Iteration: 490\n",
      "Train Loss: 207.419921875\n",
      "Average Return: 0.2900558412075043\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 47.78886413574219\n",
      "Average Return: -0.23796981573104858\n",
      "\n",
      "Iteration: 510\n",
      "Train Loss: 62.78119659423828\n",
      "Average Return: -0.12176059186458588\n",
      "\n",
      "Iteration: 520\n",
      "Train Loss: 7.738028049468994\n",
      "Average Return: 0.1561206728219986\n",
      "\n",
      "Iteration: 530\n",
      "Train Loss: 9.554657936096191\n",
      "Average Return: -0.05584300681948662\n",
      "\n",
      "Iteration: 540\n",
      "Train Loss: 19.11729621887207\n",
      "Average Return: 0.27164095640182495\n",
      "\n",
      "Iteration: 550\n",
      "Train Loss: 7.025333881378174\n",
      "Average Return: 0.45202434062957764\n",
      "\n",
      "New best average return found at 1.0565056800842285! Saving checkpoint at iteration 560\n",
      "\n",
      "Iteration: 560\n",
      "Train Loss: 76.80644226074219\n",
      "Average Return: 1.0565056800842285\n",
      "\n",
      "Iteration: 570\n",
      "Train Loss: 12.071346282958984\n",
      "Average Return: 0.9637866020202637\n",
      "\n",
      "Iteration: 580\n",
      "Train Loss: 24.180994033813477\n",
      "Average Return: 0.8505176901817322\n",
      "\n",
      "New best average return found at 1.1861913204193115! Saving checkpoint at iteration 590\n",
      "\n",
      "Iteration: 590\n",
      "Train Loss: 29.128887176513672\n",
      "Average Return: 1.1861913204193115\n",
      "\n",
      "Iteration: 600\n",
      "Train Loss: 37.74159240722656\n",
      "Average Return: 1.1508574485778809\n",
      "\n",
      "Iteration: 610\n",
      "Train Loss: 23.531126022338867\n",
      "Average Return: 1.1411428451538086\n",
      "\n",
      "Iteration: 620\n",
      "Train Loss: 91.14016723632812\n",
      "Average Return: 0.8884205222129822\n",
      "\n",
      "Iteration: 630\n",
      "Train Loss: 27.092588424682617\n",
      "Average Return: 0.6634224653244019\n",
      "\n",
      "Iteration: 640\n",
      "Train Loss: 8.918779373168945\n",
      "Average Return: 0.8624441027641296\n",
      "\n",
      "Iteration: 650\n",
      "Train Loss: 11.25179672241211\n",
      "Average Return: 0.7983794808387756\n",
      "\n",
      "Iteration: 660\n",
      "Train Loss: 91.21965026855469\n",
      "Average Return: 0.8495761156082153\n",
      "\n",
      "Iteration: 670\n",
      "Train Loss: 37.50503921508789\n",
      "Average Return: 0.8211631178855896\n",
      "\n",
      "Iteration: 680\n",
      "Train Loss: 17.408924102783203\n",
      "Average Return: 0.8076856732368469\n",
      "\n",
      "Iteration: 690\n",
      "Train Loss: 12.404682159423828\n",
      "Average Return: 0.9276540875434875\n",
      "\n",
      "Iteration: 700\n",
      "Train Loss: 62.65018081665039\n",
      "Average Return: 0.7200886011123657\n",
      "\n",
      "Iteration: 710\n",
      "Train Loss: 25.665054321289062\n",
      "Average Return: 0.7028533816337585\n",
      "\n",
      "Iteration: 720\n",
      "Train Loss: 24.87986946105957\n",
      "Average Return: 0.7028533816337585\n",
      "\n",
      "Iteration: 730\n",
      "Train Loss: 22.488130569458008\n",
      "Average Return: 0.7028533816337585\n",
      "\n",
      "Iteration: 740\n",
      "Train Loss: 31.87248992919922\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 750\n",
      "Train Loss: 9.546257972717285\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 760\n",
      "Train Loss: 78.25407409667969\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 770\n",
      "Train Loss: 33.170623779296875\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 780\n",
      "Train Loss: 38.20109176635742\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 790\n",
      "Train Loss: 26.3485164642334\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 800\n",
      "Train Loss: 57.470340728759766\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 810\n",
      "Train Loss: 18.246341705322266\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 820\n",
      "Train Loss: 21.441255569458008\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 830\n",
      "Train Loss: 7.249356269836426\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 840\n",
      "Train Loss: 24.77081298828125\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 850\n",
      "Train Loss: 7.622143745422363\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 860\n",
      "Train Loss: 38.73369598388672\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 870\n",
      "Train Loss: 21.08527946472168\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 880\n",
      "Train Loss: 14.135252952575684\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 890\n",
      "Train Loss: 21.23375701904297\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 900\n",
      "Train Loss: 23.677074432373047\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 910\n",
      "Train Loss: 20.716991424560547\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 920\n",
      "Train Loss: 13.974364280700684\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 930\n",
      "Train Loss: 6.238272190093994\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 940\n",
      "Train Loss: 11.640891075134277\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 950\n",
      "Train Loss: 14.324173927307129\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 960\n",
      "Train Loss: 10.014177322387695\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 970\n",
      "Train Loss: 7.167002201080322\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 980\n",
      "Train Loss: 17.620346069335938\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 990\n",
      "Train Loss: 18.15631675720215\n",
      "Average Return: 0.6806155443191528\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 14.69659423828125\n",
      "Average Return: 0.6806155443191528\n",
      "Training has started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\metrics\\trading\\sharpe.py:20: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.exp(average_returns/std_returns)\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best average return found at -34.00912094116211! Saving checkpoint at iteration 0\n",
      "\n",
      "New best average return found at 5.621676921844482! Saving checkpoint at iteration 10\n",
      "\n",
      "Iteration: 10\n",
      "Train Loss: 0.46018218994140625\n",
      "Average Return: 5.621676921844482\n",
      "\n",
      "Iteration: 20\n",
      "Train Loss: 1.4222354888916016\n",
      "Average Return: 4.73570442199707\n",
      "\n",
      "Iteration: 30\n",
      "Train Loss: 0.04966241866350174\n",
      "Average Return: 2.025658369064331\n",
      "\n",
      "New best average return found at 6.530117511749268! Saving checkpoint at iteration 40\n",
      "\n",
      "Iteration: 40\n",
      "Train Loss: 0.918865442276001\n",
      "Average Return: 6.530117511749268\n",
      "\n",
      "New best average return found at 7.718655586242676! Saving checkpoint at iteration 50\n",
      "\n",
      "Iteration: 50\n",
      "Train Loss: 0.5210205316543579\n",
      "Average Return: 7.718655586242676\n",
      "\n",
      "Iteration: 60\n",
      "Train Loss: 0.8800739049911499\n",
      "Average Return: 1.753743052482605\n",
      "\n",
      "Iteration: 70\n",
      "Train Loss: 1.7316622734069824\n",
      "Average Return: 1.1272082328796387\n",
      "\n",
      "Iteration: 80\n",
      "Train Loss: 1.0698623657226562\n",
      "Average Return: -0.36870265007019043\n",
      "\n",
      "Iteration: 90\n",
      "Train Loss: 0.9235286712646484\n",
      "Average Return: -4.855419635772705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\metrics\\trading\\sortino.py:20: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.exp(average_returns/std_downfall_returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 100\n",
      "Train Loss: 0.5158969163894653\n",
      "Average Return: -6.293537616729736\n",
      "\n",
      "Iteration: 110\n",
      "Train Loss: 1.699774980545044\n",
      "Average Return: 6.281260967254639\n",
      "\n",
      "New best average return found at 9.275382041931152! Saving checkpoint at iteration 120\n",
      "\n",
      "Iteration: 120\n",
      "Train Loss: 2.34633469581604\n",
      "Average Return: 9.275382041931152\n",
      "\n",
      "Iteration: 130\n",
      "Train Loss: 0.3292926847934723\n",
      "Average Return: 8.948965072631836\n",
      "\n",
      "New best average return found at 9.87955093383789! Saving checkpoint at iteration 140\n",
      "\n",
      "Iteration: 140\n",
      "Train Loss: 3.070344924926758\n",
      "Average Return: 9.87955093383789\n",
      "\n",
      "New best average return found at 9.947980880737305! Saving checkpoint at iteration 150\n",
      "\n",
      "Iteration: 150\n",
      "Train Loss: 0.9939121603965759\n",
      "Average Return: 9.947980880737305\n",
      "\n",
      "Iteration: 160\n",
      "Train Loss: 0.20012550055980682\n",
      "Average Return: 5.086240768432617\n",
      "\n",
      "Iteration: 170\n",
      "Train Loss: 2.1696181297302246\n",
      "Average Return: 6.095435619354248\n",
      "\n",
      "Iteration: 180\n",
      "Train Loss: 0.7126433849334717\n",
      "Average Return: 2.5526211261749268\n",
      "\n",
      "Iteration: 190\n",
      "Train Loss: 0.8078312873840332\n",
      "Average Return: 6.689222812652588\n",
      "\n",
      "Iteration: 200\n",
      "Train Loss: 4.016073703765869\n",
      "Average Return: 5.39107608795166\n",
      "\n",
      "New best average return found at 10.527050018310547! Saving checkpoint at iteration 210\n",
      "\n",
      "Iteration: 210\n",
      "Train Loss: 0.9372744560241699\n",
      "Average Return: 10.527050018310547\n",
      "\n",
      "New best average return found at 12.505449295043945! Saving checkpoint at iteration 220\n",
      "\n",
      "Iteration: 220\n",
      "Train Loss: 0.6434412598609924\n",
      "Average Return: 12.505449295043945\n",
      "\n",
      "Iteration: 230\n",
      "Train Loss: 2.980860948562622\n",
      "Average Return: 10.740884780883789\n",
      "\n",
      "Iteration: 240\n",
      "Train Loss: 3.256269931793213\n",
      "Average Return: 10.141983985900879\n",
      "\n",
      "Iteration: 250\n",
      "Train Loss: 0.38364988565444946\n",
      "Average Return: 9.981307983398438\n",
      "\n",
      "Iteration: 260\n",
      "Train Loss: 1.3579128980636597\n",
      "Average Return: 9.75897216796875\n",
      "\n",
      "Iteration: 270\n",
      "Train Loss: 1.3260302543640137\n",
      "Average Return: 0.7910831570625305\n",
      "\n",
      "Iteration: 280\n",
      "Train Loss: 1.000317096710205\n",
      "Average Return: -2.1213927268981934\n",
      "\n",
      "Iteration: 290\n",
      "Train Loss: 0.6373936533927917\n",
      "Average Return: 2.95107364654541\n",
      "\n",
      "Iteration: 300\n",
      "Train Loss: 6.1883673667907715\n",
      "Average Return: 8.94428539276123\n",
      "\n",
      "Iteration: 310\n",
      "Train Loss: 5.55111026763916\n",
      "Average Return: 9.046117782592773\n",
      "\n",
      "Iteration: 320\n",
      "Train Loss: 1.167473554611206\n",
      "Average Return: 10.162379264831543\n",
      "\n",
      "Iteration: 330\n",
      "Train Loss: 0.8800109624862671\n",
      "Average Return: 10.758902549743652\n",
      "\n",
      "Iteration: 340\n",
      "Train Loss: 0.6361181735992432\n",
      "Average Return: 12.259510040283203\n",
      "\n",
      "Iteration: 350\n",
      "Train Loss: 4.549870014190674\n",
      "Average Return: 10.266068458557129\n",
      "\n",
      "Iteration: 360\n",
      "Train Loss: 0.19489072263240814\n",
      "Average Return: 9.617091178894043\n",
      "\n",
      "Iteration: 370\n",
      "Train Loss: 2.8878848552703857\n",
      "Average Return: 11.192334175109863\n",
      "\n",
      "Iteration: 380\n",
      "Train Loss: 1.301479697227478\n",
      "Average Return: 10.968390464782715\n",
      "\n",
      "Iteration: 390\n",
      "Train Loss: 0.6124669313430786\n",
      "Average Return: 10.890403747558594\n",
      "\n",
      "Iteration: 400\n",
      "Train Loss: 1.582131266593933\n",
      "Average Return: 9.791131019592285\n",
      "\n",
      "Iteration: 410\n",
      "Train Loss: 5.670928478240967\n",
      "Average Return: 7.912402153015137\n",
      "\n",
      "Iteration: 420\n",
      "Train Loss: 2.4588210582733154\n",
      "Average Return: 7.171885013580322\n",
      "\n",
      "Iteration: 430\n",
      "Train Loss: 0.36150696873664856\n",
      "Average Return: 7.360751152038574\n",
      "\n",
      "Iteration: 440\n",
      "Train Loss: 0.5380896925926208\n",
      "Average Return: 6.9998779296875\n",
      "\n",
      "Iteration: 450\n",
      "Train Loss: 1.5169901847839355\n",
      "Average Return: 6.87376594543457\n",
      "\n",
      "Iteration: 460\n",
      "Train Loss: 1.425597071647644\n",
      "Average Return: 8.558099746704102\n",
      "\n",
      "Iteration: 470\n",
      "Train Loss: 1.1049805879592896\n",
      "Average Return: 9.073735237121582\n",
      "\n",
      "Iteration: 480\n",
      "Train Loss: 1.6372697353363037\n",
      "Average Return: 8.424783706665039\n",
      "\n",
      "Iteration: 490\n",
      "Train Loss: 1.3553760051727295\n",
      "Average Return: 8.668168067932129\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 1.7402092218399048\n",
      "Average Return: 8.863760948181152\n",
      "\n",
      "Iteration: 510\n",
      "Train Loss: 1.7438392639160156\n",
      "Average Return: 7.295434951782227\n",
      "\n",
      "Iteration: 520\n",
      "Train Loss: 0.20053686201572418\n",
      "Average Return: 6.932361602783203\n",
      "\n",
      "Iteration: 530\n",
      "Train Loss: 0.44036802649497986\n",
      "Average Return: -6.796670436859131\n",
      "\n",
      "Iteration: 540\n",
      "Train Loss: 0.9038903713226318\n",
      "Average Return: 3.1532833576202393\n",
      "\n",
      "Iteration: 550\n",
      "Train Loss: 0.4577334523200989\n",
      "Average Return: 1.546255111694336\n",
      "\n",
      "Iteration: 560\n",
      "Train Loss: 4.765012264251709\n",
      "Average Return: 1.8264591693878174\n",
      "\n",
      "Iteration: 570\n",
      "Train Loss: 1.1065975427627563\n",
      "Average Return: 4.001802444458008\n",
      "\n",
      "Iteration: 580\n",
      "Train Loss: 2.2140650749206543\n",
      "Average Return: 4.423840045928955\n",
      "\n",
      "Iteration: 590\n",
      "Train Loss: 0.9418071508407593\n",
      "Average Return: 5.389755725860596\n",
      "\n",
      "Iteration: 600\n",
      "Train Loss: 1.6607697010040283\n",
      "Average Return: 7.278899669647217\n",
      "\n",
      "Iteration: 610\n",
      "Train Loss: 1.2938389778137207\n",
      "Average Return: 9.465849876403809\n",
      "\n",
      "Iteration: 620\n",
      "Train Loss: 4.9273576736450195\n",
      "Average Return: 10.39370346069336\n",
      "\n",
      "Iteration: 630\n",
      "Train Loss: 1.0661789178848267\n",
      "Average Return: 10.769539833068848\n",
      "\n",
      "Iteration: 640\n",
      "Train Loss: 1.0846298933029175\n",
      "Average Return: 3.940326452255249\n",
      "\n",
      "Iteration: 650\n",
      "Train Loss: 2.721555709838867\n",
      "Average Return: 6.013312816619873\n",
      "\n",
      "Iteration: 660\n",
      "Train Loss: 3.9472692012786865\n",
      "Average Return: 7.20182991027832\n",
      "\n",
      "Iteration: 670\n",
      "Train Loss: 1.6085110902786255\n",
      "Average Return: 11.439067840576172\n",
      "\n",
      "Iteration: 680\n",
      "Train Loss: 1.9575456380844116\n",
      "Average Return: 12.0609130859375\n",
      "\n",
      "Iteration: 690\n",
      "Train Loss: 28.95587730407715\n",
      "Average Return: 12.458250045776367\n",
      "\n",
      "Iteration: 700\n",
      "Train Loss: 2.794055223464966\n",
      "Average Return: 12.3156156539917\n",
      "\n",
      "Iteration: 710\n",
      "Train Loss: 1.7342349290847778\n",
      "Average Return: 12.405421257019043\n",
      "\n",
      "Iteration: 720\n",
      "Train Loss: 5.763828277587891\n",
      "Average Return: 11.919886589050293\n",
      "\n",
      "Iteration: 730\n",
      "Train Loss: 4.218903541564941\n",
      "Average Return: 12.143970489501953\n",
      "\n",
      "New best average return found at 12.780410766601562! Saving checkpoint at iteration 740\n",
      "\n",
      "Iteration: 740\n",
      "Train Loss: 1.775517225265503\n",
      "Average Return: 12.780410766601562\n",
      "\n",
      "New best average return found at 12.997045516967773! Saving checkpoint at iteration 750\n",
      "\n",
      "Iteration: 750\n",
      "Train Loss: 0.6332044005393982\n",
      "Average Return: 12.997045516967773\n",
      "\n",
      "New best average return found at 13.173492431640625! Saving checkpoint at iteration 760\n",
      "\n",
      "Iteration: 760\n",
      "Train Loss: 9.013433456420898\n",
      "Average Return: 13.173492431640625\n",
      "\n",
      "Iteration: 770\n",
      "Train Loss: 2.008563280105591\n",
      "Average Return: 12.987317085266113\n",
      "\n",
      "Iteration: 780\n",
      "Train Loss: 3.708951950073242\n",
      "Average Return: 12.478092193603516\n",
      "\n",
      "Iteration: 790\n",
      "Train Loss: 7.902163982391357\n",
      "Average Return: 12.095540046691895\n",
      "\n",
      "Iteration: 800\n",
      "Train Loss: 18.538366317749023\n",
      "Average Return: 12.564424514770508\n",
      "\n",
      "Iteration: 810\n",
      "Train Loss: 1.7583279609680176\n",
      "Average Return: 12.411053657531738\n",
      "\n",
      "Iteration: 820\n",
      "Train Loss: 3.5432965755462646\n",
      "Average Return: 12.271063804626465\n",
      "\n",
      "Iteration: 830\n",
      "Train Loss: 1.963843822479248\n",
      "Average Return: 10.73397445678711\n",
      "\n",
      "Iteration: 840\n",
      "Train Loss: 3.036127805709839\n",
      "Average Return: 8.687288284301758\n",
      "\n",
      "Iteration: 850\n",
      "Train Loss: 2.4571449756622314\n",
      "Average Return: 8.97044563293457\n",
      "\n",
      "Iteration: 860\n",
      "Train Loss: 6.999133110046387\n",
      "Average Return: 9.023170471191406\n",
      "\n",
      "Iteration: 870\n",
      "Train Loss: 0.8072725534439087\n",
      "Average Return: 9.174371719360352\n",
      "\n",
      "Iteration: 880\n",
      "Train Loss: 4.564720630645752\n",
      "Average Return: 8.933643341064453\n",
      "\n",
      "Iteration: 890\n",
      "Train Loss: 3.443577766418457\n",
      "Average Return: 9.205968856811523\n",
      "\n",
      "Iteration: 900\n",
      "Train Loss: 3.6654584407806396\n",
      "Average Return: 9.648324012756348\n",
      "\n",
      "Iteration: 910\n",
      "Train Loss: 1.3540693521499634\n",
      "Average Return: 10.184487342834473\n",
      "\n",
      "Iteration: 920\n",
      "Train Loss: 1.5237454175949097\n",
      "Average Return: 10.150748252868652\n",
      "\n",
      "Iteration: 930\n",
      "Train Loss: 1.0827600955963135\n",
      "Average Return: 9.902350425720215\n",
      "\n",
      "Iteration: 940\n",
      "Train Loss: 0.5144187808036804\n",
      "Average Return: 9.606514930725098\n",
      "\n",
      "Iteration: 950\n",
      "Train Loss: 1.1961697340011597\n",
      "Average Return: 9.115482330322266\n",
      "\n",
      "Iteration: 960\n",
      "Train Loss: 1.9504454135894775\n",
      "Average Return: 9.390527725219727\n",
      "\n",
      "Iteration: 970\n",
      "Train Loss: 0.9219921827316284\n",
      "Average Return: 9.864007949829102\n",
      "\n",
      "Iteration: 980\n",
      "Train Loss: 1.6463274955749512\n",
      "Average Return: 9.741819381713867\n",
      "\n",
      "Iteration: 990\n",
      "Train Loss: 2.9681906700134277\n",
      "Average Return: 8.528011322021484\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 4.240189075469971\n",
      "Average Return: 8.111739158630371\n",
      "Training has started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\metrics\\trading\\sharpe.py:20: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.exp(average_returns/std_returns)\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best average return found at -48.64369201660156! Saving checkpoint at iteration 0\n",
      "\n",
      "New best average return found at 0.40528222918510437! Saving checkpoint at iteration 10\n",
      "\n",
      "Iteration: 10\n",
      "Train Loss: 70.51688385009766\n",
      "Average Return: 0.40528222918510437\n",
      "\n",
      "New best average return found at 13.098921775817871! Saving checkpoint at iteration 20\n",
      "\n",
      "Iteration: 20\n",
      "Train Loss: 17.186201095581055\n",
      "Average Return: 13.098921775817871\n",
      "\n",
      "Iteration: 30\n",
      "Train Loss: 13.850643157958984\n",
      "Average Return: 9.405078887939453\n",
      "\n",
      "Iteration: 40\n",
      "Train Loss: 5.288665771484375\n",
      "Average Return: 6.276732921600342\n",
      "\n",
      "Iteration: 50\n",
      "Train Loss: 3.398315668106079\n",
      "Average Return: 10.637572288513184\n",
      "\n",
      "Iteration: 60\n",
      "Train Loss: 6.0303144454956055\n",
      "Average Return: 7.900406837463379\n",
      "\n",
      "Iteration: 70\n",
      "Train Loss: 176.51661682128906\n",
      "Average Return: -0.5921937823295593\n",
      "\n",
      "Iteration: 80\n",
      "Train Loss: 16.269285202026367\n",
      "Average Return: 9.368404388427734\n",
      "\n",
      "Iteration: 90\n",
      "Train Loss: 31.27381706237793\n",
      "Average Return: 7.797119617462158\n",
      "\n",
      "Iteration: 100\n",
      "Train Loss: 5.907190322875977\n",
      "Average Return: 4.064102649688721\n",
      "\n",
      "Iteration: 110\n",
      "Train Loss: 27.76999282836914\n",
      "Average Return: 9.748353004455566\n",
      "\n",
      "Iteration: 120\n",
      "Train Loss: 36.59880065917969\n",
      "Average Return: 11.362984657287598\n",
      "\n",
      "New best average return found at 14.477336883544922! Saving checkpoint at iteration 130\n",
      "\n",
      "Iteration: 130\n",
      "Train Loss: 15.209576606750488\n",
      "Average Return: 14.477336883544922\n",
      "\n",
      "Iteration: 140\n",
      "Train Loss: 13.481925010681152\n",
      "Average Return: 13.451435089111328\n",
      "\n",
      "Iteration: 150\n",
      "Train Loss: 3.2325491905212402\n",
      "Average Return: 10.19921875\n",
      "\n",
      "New best average return found at 16.323253631591797! Saving checkpoint at iteration 160\n",
      "\n",
      "Iteration: 160\n",
      "Train Loss: 40.64710998535156\n",
      "Average Return: 16.323253631591797\n",
      "\n",
      "New best average return found at 17.237350463867188! Saving checkpoint at iteration 170\n",
      "\n",
      "Iteration: 170\n",
      "Train Loss: 67.00071716308594\n",
      "Average Return: 17.237350463867188\n",
      "\n",
      "Iteration: 180\n",
      "Train Loss: 40.41860580444336\n",
      "Average Return: 11.36484432220459\n",
      "\n",
      "Iteration: 190\n",
      "Train Loss: 9.41111946105957\n",
      "Average Return: 12.980070114135742\n",
      "\n",
      "Iteration: 200\n",
      "Train Loss: 20.174304962158203\n",
      "Average Return: 10.592402458190918\n",
      "\n",
      "Iteration: 210\n",
      "Train Loss: 9.405241966247559\n",
      "Average Return: 9.38394832611084\n",
      "\n",
      "Iteration: 220\n",
      "Train Loss: 69.78494262695312\n",
      "Average Return: 9.660141944885254\n",
      "\n",
      "Iteration: 230\n",
      "Train Loss: 117.31067657470703\n",
      "Average Return: 10.359187126159668\n",
      "\n",
      "Iteration: 240\n",
      "Train Loss: 36.898590087890625\n",
      "Average Return: 9.455463409423828\n",
      "\n",
      "Iteration: 250\n",
      "Train Loss: 39.623958587646484\n",
      "Average Return: 9.738256454467773\n",
      "\n",
      "Iteration: 260\n",
      "Train Loss: 13.377257347106934\n",
      "Average Return: 14.638679504394531\n",
      "\n",
      "Iteration: 270\n",
      "Train Loss: 22.756986618041992\n",
      "Average Return: 14.64754581451416\n",
      "\n",
      "Iteration: 280\n",
      "Train Loss: 7.186678886413574\n",
      "Average Return: 8.176916122436523\n",
      "\n",
      "Iteration: 290\n",
      "Train Loss: 18.54865837097168\n",
      "Average Return: 8.99118423461914\n",
      "\n",
      "Iteration: 300\n",
      "Train Loss: 315.8956604003906\n",
      "Average Return: 9.742212295532227\n",
      "\n",
      "Iteration: 310\n",
      "Train Loss: 75.15176391601562\n",
      "Average Return: 9.503044128417969\n",
      "\n",
      "Iteration: 320\n",
      "Train Loss: 30.70663070678711\n",
      "Average Return: 10.141403198242188\n",
      "\n",
      "Iteration: 330\n",
      "Train Loss: 78.73900604248047\n",
      "Average Return: 12.437777519226074\n",
      "\n",
      "Iteration: 340\n",
      "Train Loss: 11.991265296936035\n",
      "Average Return: 15.400308609008789\n",
      "\n",
      "Iteration: 350\n",
      "Train Loss: 14.202271461486816\n",
      "Average Return: 13.615148544311523\n",
      "\n",
      "Iteration: 360\n",
      "Train Loss: 29.176586151123047\n",
      "Average Return: 10.251774787902832\n",
      "\n",
      "Iteration: 370\n",
      "Train Loss: 52.7169189453125\n",
      "Average Return: 11.907227516174316\n",
      "\n",
      "Iteration: 380\n",
      "Train Loss: 8.080817222595215\n",
      "Average Return: 13.038358688354492\n",
      "\n",
      "Iteration: 390\n",
      "Train Loss: 19.110942840576172\n",
      "Average Return: 15.052552223205566\n",
      "\n",
      "Iteration: 400\n",
      "Train Loss: 268.1810607910156\n",
      "Average Return: 14.353374481201172\n",
      "\n",
      "Iteration: 410\n",
      "Train Loss: 52.95894241333008\n",
      "Average Return: 14.204291343688965\n",
      "\n",
      "Iteration: 420\n",
      "Train Loss: 49.830955505371094\n",
      "Average Return: 14.203502655029297\n",
      "\n",
      "Iteration: 430\n",
      "Train Loss: 44.06730270385742\n",
      "Average Return: 13.440032005310059\n",
      "\n",
      "Iteration: 440\n",
      "Train Loss: 18.608251571655273\n",
      "Average Return: 14.669541358947754\n",
      "\n",
      "Iteration: 450\n",
      "Train Loss: 10.094923973083496\n",
      "Average Return: 14.891908645629883\n",
      "\n",
      "Iteration: 460\n",
      "Train Loss: 33.14834976196289\n",
      "Average Return: 13.28481388092041\n",
      "\n",
      "Iteration: 470\n",
      "Train Loss: 15.78686237335205\n",
      "Average Return: 16.611223220825195\n",
      "\n",
      "Iteration: 480\n",
      "Train Loss: 21.554187774658203\n",
      "Average Return: 15.223891258239746\n",
      "\n",
      "Iteration: 490\n",
      "Train Loss: 7.7765326499938965\n",
      "Average Return: 15.506791114807129\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 90.6447982788086\n",
      "Average Return: 11.284123420715332\n",
      "\n",
      "Iteration: 510\n",
      "Train Loss: 101.11314392089844\n",
      "Average Return: 15.330053329467773\n",
      "\n",
      "Iteration: 520\n",
      "Train Loss: 5.928909778594971\n",
      "Average Return: 15.814460754394531\n",
      "\n",
      "Iteration: 530\n",
      "Train Loss: 32.41335678100586\n",
      "Average Return: 15.17403507232666\n",
      "\n",
      "Iteration: 540\n",
      "Train Loss: 37.4069709777832\n",
      "Average Return: 15.721959114074707\n",
      "\n",
      "Iteration: 550\n",
      "Train Loss: 4.905808925628662\n",
      "Average Return: 15.114433288574219\n",
      "\n",
      "Iteration: 560\n",
      "Train Loss: 17.374425888061523\n",
      "Average Return: 15.250676155090332\n",
      "\n",
      "Iteration: 570\n",
      "Train Loss: 17.750829696655273\n",
      "Average Return: 16.088266372680664\n",
      "\n",
      "Iteration: 580\n",
      "Train Loss: 10.331793785095215\n",
      "Average Return: 16.55600929260254\n",
      "\n",
      "Iteration: 590\n",
      "Train Loss: 64.00845336914062\n",
      "Average Return: 15.834319114685059\n",
      "\n",
      "Iteration: 600\n",
      "Train Loss: 17.66707992553711\n",
      "Average Return: 16.10398292541504\n",
      "\n",
      "Iteration: 610\n",
      "Train Loss: 47.39250946044922\n",
      "Average Return: 15.843664169311523\n",
      "\n",
      "Iteration: 620\n",
      "Train Loss: 94.4460220336914\n",
      "Average Return: 15.928228378295898\n",
      "\n",
      "Iteration: 630\n",
      "Train Loss: 25.092815399169922\n",
      "Average Return: 15.432907104492188\n",
      "\n",
      "Iteration: 640\n",
      "Train Loss: 23.832284927368164\n",
      "Average Return: 15.632694244384766\n",
      "\n",
      "Iteration: 650\n",
      "Train Loss: 62.416866302490234\n",
      "Average Return: 15.551401138305664\n",
      "\n",
      "Iteration: 660\n",
      "Train Loss: 12.814144134521484\n",
      "Average Return: 15.628491401672363\n",
      "\n",
      "Iteration: 670\n",
      "Train Loss: 12.117777824401855\n",
      "Average Return: 15.25532341003418\n",
      "\n",
      "Iteration: 680\n",
      "Train Loss: 27.770143508911133\n",
      "Average Return: 15.459183692932129\n",
      "\n",
      "Iteration: 690\n",
      "Train Loss: 23.795391082763672\n",
      "Average Return: 15.358742713928223\n",
      "\n",
      "Iteration: 700\n",
      "Train Loss: 39.69482421875\n",
      "Average Return: 14.510679244995117\n",
      "\n",
      "Iteration: 710\n",
      "Train Loss: 15.45863151550293\n",
      "Average Return: 14.965194702148438\n",
      "\n",
      "Iteration: 720\n",
      "Train Loss: 15.12614917755127\n",
      "Average Return: 15.317116737365723\n",
      "\n",
      "Iteration: 730\n",
      "Train Loss: 4.743220806121826\n",
      "Average Return: 14.530899047851562\n",
      "\n",
      "Iteration: 740\n",
      "Train Loss: 10.957758903503418\n",
      "Average Return: 13.987821578979492\n",
      "\n",
      "Iteration: 750\n",
      "Train Loss: 6.334837913513184\n",
      "Average Return: 13.765722274780273\n",
      "\n",
      "Iteration: 760\n",
      "Train Loss: 68.28917694091797\n",
      "Average Return: 14.537203788757324\n",
      "\n",
      "Iteration: 770\n",
      "Train Loss: 28.005815505981445\n",
      "Average Return: 15.33570671081543\n",
      "\n",
      "Iteration: 780\n",
      "Train Loss: 17.992568969726562\n",
      "Average Return: 16.077905654907227\n",
      "\n",
      "Iteration: 790\n",
      "Train Loss: 27.914411544799805\n",
      "Average Return: 15.096424102783203\n",
      "\n",
      "Iteration: 800\n",
      "Train Loss: 25.746688842773438\n",
      "Average Return: 15.585188865661621\n",
      "\n",
      "Iteration: 810\n",
      "Train Loss: 31.728227615356445\n",
      "Average Return: 15.736492156982422\n",
      "\n",
      "Iteration: 820\n",
      "Train Loss: 23.80767059326172\n",
      "Average Return: 14.859793663024902\n",
      "\n",
      "Iteration: 830\n",
      "Train Loss: 11.794922828674316\n",
      "Average Return: 14.360954284667969\n",
      "\n",
      "Iteration: 840\n",
      "Train Loss: 6.8247480392456055\n",
      "Average Return: 14.999972343444824\n",
      "\n",
      "Iteration: 850\n",
      "Train Loss: 5.136873722076416\n",
      "Average Return: 14.074951171875\n",
      "\n",
      "Iteration: 860\n",
      "Train Loss: 26.967348098754883\n",
      "Average Return: 12.65369701385498\n",
      "\n",
      "Iteration: 870\n",
      "Train Loss: 22.27107048034668\n",
      "Average Return: 13.15499496459961\n",
      "\n",
      "Iteration: 880\n",
      "Train Loss: 3.6819939613342285\n",
      "Average Return: 14.083919525146484\n",
      "\n",
      "Iteration: 890\n",
      "Train Loss: 22.084394454956055\n",
      "Average Return: 15.430845260620117\n",
      "\n",
      "Iteration: 900\n",
      "Train Loss: 76.78983306884766\n",
      "Average Return: 15.258252143859863\n",
      "\n",
      "Iteration: 910\n",
      "Train Loss: 65.9464111328125\n",
      "Average Return: 15.503181457519531\n",
      "\n",
      "Iteration: 920\n",
      "Train Loss: 14.784893035888672\n",
      "Average Return: 15.282883644104004\n",
      "\n",
      "Iteration: 930\n",
      "Train Loss: 28.001907348632812\n",
      "Average Return: 15.74380874633789\n",
      "\n",
      "Iteration: 940\n",
      "Train Loss: 8.153138160705566\n",
      "Average Return: 15.626052856445312\n",
      "\n",
      "Iteration: 950\n",
      "Train Loss: 12.763715744018555\n",
      "Average Return: 15.594131469726562\n",
      "\n",
      "Iteration: 960\n",
      "Train Loss: 12.172502517700195\n",
      "Average Return: 15.575934410095215\n",
      "\n",
      "Iteration: 970\n",
      "Train Loss: 10.796942710876465\n",
      "Average Return: 15.541716575622559\n",
      "\n",
      "Iteration: 980\n",
      "Train Loss: 46.809104919433594\n",
      "Average Return: 15.672518730163574\n",
      "\n",
      "Iteration: 990\n",
      "Train Loss: 23.2572021484375\n",
      "Average Return: 15.752315521240234\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 40.22966384887695\n",
      "Average Return: 15.851975440979004\n",
      "Training has started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\metrics\\trading\\sharpe.py:20: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.exp(average_returns/std_returns)\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best average return found at -65.24463653564453! Saving checkpoint at iteration 0\n",
      "\n",
      "New best average return found at 31.52944564819336! Saving checkpoint at iteration 10\n",
      "\n",
      "Iteration: 10\n",
      "Train Loss: 30.882476806640625\n",
      "Average Return: 31.52944564819336\n",
      "\n",
      "Iteration: 20\n",
      "Train Loss: 3.7421491146087646\n",
      "Average Return: 12.899675369262695\n",
      "\n",
      "New best average return found at 31.550079345703125! Saving checkpoint at iteration 30\n",
      "\n",
      "Iteration: 30\n",
      "Train Loss: 15.2725248336792\n",
      "Average Return: 31.550079345703125\n",
      "\n",
      "Iteration: 40\n",
      "Train Loss: 14.822991371154785\n",
      "Average Return: 18.04451560974121\n",
      "\n",
      "Iteration: 50\n",
      "Train Loss: 1.8637970685958862\n",
      "Average Return: 25.020660400390625\n",
      "\n",
      "Iteration: 60\n",
      "Train Loss: 5.907175064086914\n",
      "Average Return: 26.850509643554688\n",
      "\n",
      "Iteration: 70\n",
      "Train Loss: 128.55764770507812\n",
      "Average Return: 24.307260513305664\n",
      "\n",
      "Iteration: 80\n",
      "Train Loss: 13.327314376831055\n",
      "Average Return: 28.896926879882812\n",
      "\n",
      "Iteration: 90\n",
      "Train Loss: 13.951639175415039\n",
      "Average Return: 28.272659301757812\n",
      "\n",
      "Iteration: 100\n",
      "Train Loss: 13.461362838745117\n",
      "Average Return: 28.649049758911133\n",
      "\n",
      "Iteration: 110\n",
      "Train Loss: 81.45703887939453\n",
      "Average Return: 27.466114044189453\n",
      "\n",
      "Iteration: 120\n",
      "Train Loss: 17.483793258666992\n",
      "Average Return: 30.684663772583008\n",
      "\n",
      "Iteration: 130\n",
      "Train Loss: 12.184462547302246\n",
      "Average Return: 29.226682662963867\n",
      "\n",
      "Iteration: 140\n",
      "Train Loss: 11.641938209533691\n",
      "Average Return: 30.966657638549805\n",
      "\n",
      "Iteration: 150\n",
      "Train Loss: 3.848705768585205\n",
      "Average Return: 31.187238693237305\n",
      "\n",
      "Iteration: 160\n",
      "Train Loss: 18.00472640991211\n",
      "Average Return: 30.76483154296875\n",
      "\n",
      "Iteration: 170\n",
      "Train Loss: 117.42777252197266\n",
      "Average Return: 28.143169403076172\n",
      "\n",
      "Iteration: 180\n",
      "Train Loss: 7.290651798248291\n",
      "Average Return: 29.22522735595703\n",
      "\n",
      "Iteration: 190\n",
      "Train Loss: 2.4841177463531494\n",
      "Average Return: 22.175596237182617\n",
      "\n",
      "New best average return found at 33.86387252807617! Saving checkpoint at iteration 200\n",
      "\n",
      "Iteration: 200\n",
      "Train Loss: 53.94358444213867\n",
      "Average Return: 33.86387252807617\n",
      "\n",
      "New best average return found at 34.20793914794922! Saving checkpoint at iteration 210\n",
      "\n",
      "Iteration: 210\n",
      "Train Loss: 36.31894302368164\n",
      "Average Return: 34.20793914794922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\metrics\\trading\\sortino.py:20: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.exp(average_returns/std_downfall_returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best average return found at 34.63430404663086! Saving checkpoint at iteration 220\n",
      "\n",
      "Iteration: 220\n",
      "Train Loss: 17.594863891601562\n",
      "Average Return: 34.63430404663086\n",
      "\n",
      "Iteration: 230\n",
      "Train Loss: 86.07440948486328\n",
      "Average Return: 33.724552154541016\n",
      "\n",
      "Iteration: 240\n",
      "Train Loss: 30.18636131286621\n",
      "Average Return: 31.809232711791992\n",
      "\n",
      "Iteration: 250\n",
      "Train Loss: 23.029687881469727\n",
      "Average Return: 31.708229064941406\n",
      "\n",
      "Iteration: 260\n",
      "Train Loss: 19.543594360351562\n",
      "Average Return: 29.641963958740234\n",
      "\n",
      "Iteration: 270\n",
      "Train Loss: 9.848798751831055\n",
      "Average Return: 29.528423309326172\n",
      "\n",
      "Iteration: 280\n",
      "Train Loss: 16.52969741821289\n",
      "Average Return: 23.857404708862305\n",
      "\n",
      "Iteration: 290\n",
      "Train Loss: 8.878270149230957\n",
      "Average Return: 27.526376724243164\n",
      "\n",
      "Iteration: 300\n",
      "Train Loss: 56.107269287109375\n",
      "Average Return: 27.303577423095703\n",
      "\n",
      "Iteration: 310\n",
      "Train Loss: 109.92997741699219\n",
      "Average Return: 27.830976486206055\n",
      "\n",
      "Iteration: 320\n",
      "Train Loss: 7.256505489349365\n",
      "Average Return: 29.732585906982422\n",
      "\n",
      "Iteration: 330\n",
      "Train Loss: 27.49474334716797\n",
      "Average Return: 30.981199264526367\n",
      "\n",
      "Iteration: 340\n",
      "Train Loss: 4.358847618103027\n",
      "Average Return: 33.44369125366211\n",
      "\n",
      "Iteration: 350\n",
      "Train Loss: 18.19790267944336\n",
      "Average Return: 33.18916320800781\n",
      "\n",
      "Iteration: 360\n",
      "Train Loss: 13.342705726623535\n",
      "Average Return: 33.80364990234375\n",
      "\n",
      "Iteration: 370\n",
      "Train Loss: 22.530595779418945\n",
      "Average Return: 32.12744903564453\n",
      "\n",
      "Iteration: 380\n",
      "Train Loss: 8.37434196472168\n",
      "Average Return: 34.18915557861328\n",
      "\n",
      "New best average return found at 34.64027786254883! Saving checkpoint at iteration 390\n",
      "\n",
      "Iteration: 390\n",
      "Train Loss: 16.18828582763672\n",
      "Average Return: 34.64027786254883\n",
      "\n",
      "Iteration: 400\n",
      "Train Loss: 71.27008056640625\n",
      "Average Return: 34.443382263183594\n",
      "\n",
      "Iteration: 410\n",
      "Train Loss: 82.73260498046875\n",
      "Average Return: 34.48111343383789\n",
      "\n",
      "New best average return found at 34.85139846801758! Saving checkpoint at iteration 420\n",
      "\n",
      "Iteration: 420\n",
      "Train Loss: 50.09584426879883\n",
      "Average Return: 34.85139846801758\n",
      "\n",
      "Iteration: 430\n",
      "Train Loss: 52.580413818359375\n",
      "Average Return: 34.295928955078125\n",
      "\n",
      "New best average return found at 35.139381408691406! Saving checkpoint at iteration 440\n",
      "\n",
      "Iteration: 440\n",
      "Train Loss: 24.149263381958008\n",
      "Average Return: 35.139381408691406\n",
      "\n",
      "Iteration: 450\n",
      "Train Loss: 23.79131507873535\n",
      "Average Return: 34.60171890258789\n",
      "\n",
      "Iteration: 460\n",
      "Train Loss: 10.812557220458984\n",
      "Average Return: 32.71406936645508\n",
      "\n",
      "Iteration: 470\n",
      "Train Loss: 13.419293403625488\n",
      "Average Return: 32.96894073486328\n",
      "\n",
      "Iteration: 480\n",
      "Train Loss: 22.07828140258789\n",
      "Average Return: 32.984771728515625\n",
      "\n",
      "Iteration: 490\n",
      "Train Loss: 27.838268280029297\n",
      "Average Return: 33.26411056518555\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 57.35713577270508\n",
      "Average Return: 33.21994400024414\n",
      "\n",
      "Iteration: 510\n",
      "Train Loss: 39.358341217041016\n",
      "Average Return: 33.110347747802734\n",
      "\n",
      "Iteration: 520\n",
      "Train Loss: 14.144861221313477\n",
      "Average Return: 32.80844497680664\n",
      "\n",
      "Iteration: 530\n",
      "Train Loss: 28.791397094726562\n",
      "Average Return: 33.0429801940918\n",
      "\n",
      "Iteration: 540\n",
      "Train Loss: 9.576562881469727\n",
      "Average Return: 32.938720703125\n",
      "\n",
      "Iteration: 550\n",
      "Train Loss: 13.822351455688477\n",
      "Average Return: 32.64287185668945\n",
      "\n",
      "Iteration: 560\n",
      "Train Loss: 13.906435012817383\n",
      "Average Return: 31.879732131958008\n",
      "\n",
      "Iteration: 570\n",
      "Train Loss: 6.1575798988342285\n",
      "Average Return: 29.006444931030273\n",
      "\n",
      "Iteration: 580\n",
      "Train Loss: 3.395341157913208\n",
      "Average Return: 31.042457580566406\n",
      "\n",
      "Iteration: 590\n",
      "Train Loss: 39.709781646728516\n",
      "Average Return: 31.43828773498535\n",
      "\n",
      "Iteration: 600\n",
      "Train Loss: 12.463497161865234\n",
      "Average Return: 31.728906631469727\n",
      "\n",
      "Iteration: 610\n",
      "Train Loss: 20.645261764526367\n",
      "Average Return: 32.067718505859375\n",
      "\n",
      "Iteration: 620\n",
      "Train Loss: 38.85012435913086\n",
      "Average Return: 33.257816314697266\n",
      "\n",
      "Iteration: 630\n",
      "Train Loss: 17.00210952758789\n",
      "Average Return: 33.07275390625\n",
      "\n",
      "Iteration: 640\n",
      "Train Loss: 24.432052612304688\n",
      "Average Return: 32.935482025146484\n",
      "\n",
      "Iteration: 650\n",
      "Train Loss: 13.098490715026855\n",
      "Average Return: 32.170989990234375\n",
      "\n",
      "Iteration: 660\n",
      "Train Loss: 7.297677516937256\n",
      "Average Return: 32.2563591003418\n",
      "\n",
      "Iteration: 670\n",
      "Train Loss: 11.53781795501709\n",
      "Average Return: 32.52931213378906\n",
      "\n",
      "Iteration: 680\n",
      "Train Loss: 9.829011917114258\n",
      "Average Return: 33.481136322021484\n",
      "\n",
      "Iteration: 690\n",
      "Train Loss: 14.636565208435059\n",
      "Average Return: 32.87835693359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\metrics\\trading\\sortino.py:20: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(average_returns/std_downfall_returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 700\n",
      "Train Loss: 18.130949020385742\n",
      "Average Return: 32.86817169189453\n",
      "\n",
      "Iteration: 710\n",
      "Train Loss: 13.423917770385742\n",
      "Average Return: 33.56792449951172\n",
      "\n",
      "Iteration: 720\n",
      "Train Loss: 20.193578720092773\n",
      "Average Return: 33.57172393798828\n",
      "\n",
      "Iteration: 730\n",
      "Train Loss: 9.036803245544434\n",
      "Average Return: 34.95358657836914\n",
      "\n",
      "Iteration: 740\n",
      "Train Loss: 10.679986953735352\n",
      "Average Return: 34.899288177490234\n",
      "\n",
      "New best average return found at 35.236839294433594! Saving checkpoint at iteration 750\n",
      "\n",
      "Iteration: 750\n",
      "Train Loss: 5.837555885314941\n",
      "Average Return: 35.236839294433594\n",
      "\n",
      "Iteration: 760\n",
      "Train Loss: 23.15612030029297\n",
      "Average Return: 35.08951187133789\n",
      "\n",
      "Iteration: 770\n",
      "Train Loss: 17.101131439208984\n",
      "Average Return: 33.62620162963867\n",
      "\n",
      "Iteration: 780\n",
      "Train Loss: 13.467620849609375\n",
      "Average Return: 33.17713165283203\n",
      "\n",
      "Iteration: 790\n",
      "Train Loss: 17.285812377929688\n",
      "Average Return: 33.3601188659668\n",
      "\n",
      "Iteration: 800\n",
      "Train Loss: 10.91231632232666\n",
      "Average Return: 33.16476821899414\n",
      "\n",
      "Iteration: 810\n",
      "Train Loss: 11.670588493347168\n",
      "Average Return: 33.446624755859375\n",
      "\n",
      "Iteration: 820\n",
      "Train Loss: 10.414992332458496\n",
      "Average Return: 33.91112518310547\n",
      "\n",
      "Iteration: 830\n",
      "Train Loss: 11.09591007232666\n",
      "Average Return: 34.49987030029297\n",
      "\n",
      "Iteration: 840\n",
      "Train Loss: 9.005280494689941\n",
      "Average Return: 35.0429801940918\n",
      "\n",
      "Iteration: 850\n",
      "Train Loss: 5.764828205108643\n",
      "Average Return: 34.82085037231445\n",
      "\n",
      "Iteration: 860\n",
      "Train Loss: 11.062323570251465\n",
      "Average Return: 35.161354064941406\n",
      "\n",
      "New best average return found at 35.333805084228516! Saving checkpoint at iteration 870\n",
      "\n",
      "Iteration: 870\n",
      "Train Loss: 10.338889122009277\n",
      "Average Return: 35.333805084228516\n",
      "\n",
      "Iteration: 880\n",
      "Train Loss: 5.21715784072876\n",
      "Average Return: 34.54840850830078\n",
      "\n",
      "Iteration: 890\n",
      "Train Loss: 7.831847190856934\n",
      "Average Return: 35.12067413330078\n",
      "\n",
      "New best average return found at 35.414276123046875! Saving checkpoint at iteration 900\n",
      "\n",
      "Iteration: 900\n",
      "Train Loss: 9.946601867675781\n",
      "Average Return: 35.414276123046875\n",
      "\n",
      "Iteration: 910\n",
      "Train Loss: 3.3769288063049316\n",
      "Average Return: 35.34549331665039\n",
      "\n",
      "New best average return found at 35.43244171142578! Saving checkpoint at iteration 920\n",
      "\n",
      "Iteration: 920\n",
      "Train Loss: 6.837220668792725\n",
      "Average Return: 35.43244171142578\n",
      "\n",
      "Iteration: 930\n",
      "Train Loss: 5.062716007232666\n",
      "Average Return: 34.7514762878418\n",
      "\n",
      "Iteration: 940\n",
      "Train Loss: 3.4981844425201416\n",
      "Average Return: 34.684425354003906\n",
      "\n",
      "Iteration: 950\n",
      "Train Loss: 6.303434371948242\n",
      "Average Return: 34.7243766784668\n",
      "\n",
      "Iteration: 960\n",
      "Train Loss: 4.331771373748779\n",
      "Average Return: 34.95503234863281\n",
      "\n",
      "Iteration: 970\n",
      "Train Loss: 4.285712242126465\n",
      "Average Return: 35.40803527832031\n",
      "\n",
      "Iteration: 980\n",
      "Train Loss: 4.873270511627197\n",
      "Average Return: 35.302391052246094\n",
      "\n",
      "Iteration: 990\n",
      "Train Loss: 4.342246055603027\n",
      "Average Return: 35.2971076965332\n",
      "\n",
      "New best average return found at 35.80829620361328! Saving checkpoint at iteration 1000\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 7.940550327301025\n",
      "Average Return: 35.80829620361328\n",
      "Training has started...\n",
      "\n",
      "New best average return found at 12.387986183166504! Saving checkpoint at iteration 0\n",
      "\n",
      "Iteration: 10\n",
      "Train Loss: 9.734615325927734\n",
      "Average Return: 6.324771404266357\n",
      "\n",
      "New best average return found at 12.38946533203125! Saving checkpoint at iteration 20\n",
      "\n",
      "Iteration: 20\n",
      "Train Loss: 5.700501441955566\n",
      "Average Return: 12.38946533203125\n",
      "\n",
      "Iteration: 30\n",
      "Train Loss: 12.833500862121582\n",
      "Average Return: 9.817073822021484\n",
      "\n",
      "Iteration: 40\n",
      "Train Loss: 112.48849487304688\n",
      "Average Return: 9.292096138000488\n",
      "\n",
      "Iteration: 50\n",
      "Train Loss: 14.270296096801758\n",
      "Average Return: 11.656820297241211\n",
      "\n",
      "Iteration: 60\n",
      "Train Loss: 25.058443069458008\n",
      "Average Return: 8.613386154174805\n",
      "\n",
      "Iteration: 70\n",
      "Train Loss: 89.57261657714844\n",
      "Average Return: 6.194524765014648\n",
      "\n",
      "Iteration: 80\n",
      "Train Loss: 14.664582252502441\n",
      "Average Return: 1.1726016998291016\n",
      "\n",
      "Iteration: 90\n",
      "Train Loss: 11.679254531860352\n",
      "Average Return: 7.5606489181518555\n",
      "\n",
      "Iteration: 100\n",
      "Train Loss: 73.18016815185547\n",
      "Average Return: 7.083733558654785\n",
      "\n",
      "Iteration: 110\n",
      "Train Loss: 64.74309539794922\n",
      "Average Return: 4.9943132400512695\n",
      "\n",
      "Iteration: 120\n",
      "Train Loss: 9.20265007019043\n",
      "Average Return: 6.615514755249023\n",
      "\n",
      "Iteration: 130\n",
      "Train Loss: 62.50098419189453\n",
      "Average Return: 8.841489791870117\n",
      "\n",
      "Iteration: 140\n",
      "Train Loss: 87.09117126464844\n",
      "Average Return: 9.846911430358887\n",
      "\n",
      "Iteration: 150\n",
      "Train Loss: 5.010437965393066\n",
      "Average Return: 5.366451263427734\n",
      "\n",
      "Iteration: 160\n",
      "Train Loss: 23.055767059326172\n",
      "Average Return: 6.7518630027771\n",
      "\n",
      "Iteration: 170\n",
      "Train Loss: 9.991392135620117\n",
      "Average Return: 5.514848709106445\n",
      "\n",
      "Iteration: 180\n",
      "Train Loss: 52.430599212646484\n",
      "Average Return: 6.41800594329834\n",
      "\n",
      "Iteration: 190\n",
      "Train Loss: 16.12810707092285\n",
      "Average Return: 7.791877746582031\n",
      "\n",
      "Iteration: 200\n",
      "Train Loss: 37.198341369628906\n",
      "Average Return: 10.52617359161377\n",
      "\n",
      "Iteration: 210\n",
      "Train Loss: 13.10595703125\n",
      "Average Return: 10.853344917297363\n",
      "\n",
      "Iteration: 220\n",
      "Train Loss: 29.92548942565918\n",
      "Average Return: 9.87091064453125\n",
      "\n",
      "Iteration: 230\n",
      "Train Loss: 12.737277030944824\n",
      "Average Return: 9.12649154663086\n",
      "\n",
      "Iteration: 240\n",
      "Train Loss: 39.27561569213867\n",
      "Average Return: 8.596186637878418\n",
      "\n",
      "Iteration: 250\n",
      "Train Loss: 30.490407943725586\n",
      "Average Return: 5.984303951263428\n",
      "\n",
      "Iteration: 260\n",
      "Train Loss: 6.570268630981445\n",
      "Average Return: 6.785016059875488\n",
      "\n",
      "Iteration: 270\n",
      "Train Loss: 42.94374465942383\n",
      "Average Return: 8.772828102111816\n",
      "\n",
      "Iteration: 280\n",
      "Train Loss: 59.552677154541016\n",
      "Average Return: 8.270500183105469\n",
      "\n",
      "Iteration: 290\n",
      "Train Loss: 40.990562438964844\n",
      "Average Return: 4.164639949798584\n",
      "\n",
      "Iteration: 300\n",
      "Train Loss: 31.386674880981445\n",
      "Average Return: 7.381196022033691\n",
      "\n",
      "Iteration: 310\n",
      "Train Loss: 11.015488624572754\n",
      "Average Return: 9.24734115600586\n",
      "\n",
      "Iteration: 320\n",
      "Train Loss: 9.719016075134277\n",
      "Average Return: 8.20833683013916\n",
      "\n",
      "Iteration: 330\n",
      "Train Loss: 151.96328735351562\n",
      "Average Return: 7.096576690673828\n",
      "\n",
      "Iteration: 340\n",
      "Train Loss: 34.23219299316406\n",
      "Average Return: 8.633668899536133\n",
      "\n",
      "Iteration: 350\n",
      "Train Loss: 55.66449737548828\n",
      "Average Return: 9.107199668884277\n",
      "\n",
      "Iteration: 360\n",
      "Train Loss: 29.865997314453125\n",
      "Average Return: 11.205289840698242\n",
      "\n",
      "Iteration: 370\n",
      "Train Loss: 52.29066848754883\n",
      "Average Return: 8.147139549255371\n",
      "\n",
      "Iteration: 380\n",
      "Train Loss: 35.48752212524414\n",
      "Average Return: 8.794378280639648\n",
      "\n",
      "Iteration: 390\n",
      "Train Loss: 11.74750804901123\n",
      "Average Return: 2.4702773094177246\n",
      "\n",
      "Iteration: 400\n",
      "Train Loss: 9.42131519317627\n",
      "Average Return: 5.373027801513672\n",
      "\n",
      "Iteration: 410\n",
      "Train Loss: 41.606998443603516\n",
      "Average Return: 2.7253916263580322\n",
      "\n",
      "Iteration: 420\n",
      "Train Loss: 17.100929260253906\n",
      "Average Return: 8.56369686126709\n",
      "\n",
      "Iteration: 430\n",
      "Train Loss: 37.74264907836914\n",
      "Average Return: 4.300886631011963\n",
      "\n",
      "Iteration: 440\n",
      "Train Loss: 44.09876251220703\n",
      "Average Return: 2.9934909343719482\n",
      "\n",
      "Iteration: 450\n",
      "Train Loss: 55.37938690185547\n",
      "Average Return: 4.050841331481934\n",
      "\n",
      "Iteration: 460\n",
      "Train Loss: 18.160541534423828\n",
      "Average Return: 0.24403250217437744\n",
      "\n",
      "Iteration: 470\n",
      "Train Loss: 23.537796020507812\n",
      "Average Return: 8.407265663146973\n",
      "\n",
      "Iteration: 480\n",
      "Train Loss: 42.443145751953125\n",
      "Average Return: 8.125900268554688\n",
      "\n",
      "Iteration: 490\n",
      "Train Loss: 11.1820068359375\n",
      "Average Return: 10.589775085449219\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 15.365436553955078\n",
      "Average Return: 9.621981620788574\n",
      "\n",
      "Iteration: 510\n",
      "Train Loss: 10.569419860839844\n",
      "Average Return: 9.934077262878418\n",
      "\n",
      "Iteration: 520\n",
      "Train Loss: 5.637443542480469\n",
      "Average Return: 10.896292686462402\n",
      "\n",
      "Iteration: 530\n",
      "Train Loss: 14.679670333862305\n",
      "Average Return: 10.089902877807617\n",
      "\n",
      "Iteration: 540\n",
      "Train Loss: 15.168339729309082\n",
      "Average Return: 9.248032569885254\n",
      "\n",
      "Iteration: 550\n",
      "Train Loss: 14.95659351348877\n",
      "Average Return: 7.929337978363037\n",
      "\n",
      "Iteration: 560\n",
      "Train Loss: 14.861862182617188\n",
      "Average Return: 9.218280792236328\n",
      "\n",
      "Iteration: 570\n",
      "Train Loss: 18.059106826782227\n",
      "Average Return: 10.77879810333252\n",
      "\n",
      "Iteration: 580\n",
      "Train Loss: 5.378176689147949\n",
      "Average Return: 10.299612998962402\n",
      "\n",
      "Iteration: 590\n",
      "Train Loss: 10.277790069580078\n",
      "Average Return: 8.304496765136719\n",
      "\n",
      "Iteration: 600\n",
      "Train Loss: 9.205901145935059\n",
      "Average Return: 10.834150314331055\n",
      "\n",
      "Iteration: 610\n",
      "Train Loss: 13.631746292114258\n",
      "Average Return: 11.490830421447754\n",
      "\n",
      "Iteration: 620\n",
      "Train Loss: 24.41046142578125\n",
      "Average Return: 11.026652336120605\n",
      "\n",
      "Iteration: 630\n",
      "Train Loss: 7.94770622253418\n",
      "Average Return: 11.622056007385254\n",
      "\n",
      "Iteration: 640\n",
      "Train Loss: 7.6404709815979\n",
      "Average Return: 11.756563186645508\n",
      "\n",
      "Iteration: 650\n",
      "Train Loss: 4.900465965270996\n",
      "Average Return: 12.046732902526855\n",
      "\n",
      "Iteration: 660\n",
      "Train Loss: 10.522607803344727\n",
      "Average Return: 12.067577362060547\n",
      "\n",
      "Iteration: 670\n",
      "Train Loss: 5.472782611846924\n",
      "Average Return: 11.978981018066406\n",
      "\n",
      "Iteration: 680\n",
      "Train Loss: 9.633713722229004\n",
      "Average Return: 11.817182540893555\n",
      "\n",
      "New best average return found at 12.50438117980957! Saving checkpoint at iteration 690\n",
      "\n",
      "Iteration: 690\n",
      "Train Loss: 8.369182586669922\n",
      "Average Return: 12.50438117980957\n",
      "\n",
      "New best average return found at 12.70651912689209! Saving checkpoint at iteration 700\n",
      "\n",
      "Iteration: 700\n",
      "Train Loss: 6.775252342224121\n",
      "Average Return: 12.70651912689209\n",
      "\n",
      "New best average return found at 13.072072982788086! Saving checkpoint at iteration 710\n",
      "\n",
      "Iteration: 710\n",
      "Train Loss: 5.072950839996338\n",
      "Average Return: 13.072072982788086\n",
      "\n",
      "New best average return found at 13.908658027648926! Saving checkpoint at iteration 720\n",
      "\n",
      "Iteration: 720\n",
      "Train Loss: 3.509801149368286\n",
      "Average Return: 13.908658027648926\n",
      "\n",
      "New best average return found at 14.296161651611328! Saving checkpoint at iteration 730\n",
      "\n",
      "Iteration: 730\n",
      "Train Loss: 25.731985092163086\n",
      "Average Return: 14.296161651611328\n",
      "\n",
      "Iteration: 740\n",
      "Train Loss: 7.733785152435303\n",
      "Average Return: 13.422900199890137\n",
      "\n",
      "Iteration: 750\n",
      "Train Loss: 6.6203389167785645\n",
      "Average Return: 13.13252067565918\n",
      "\n",
      "Iteration: 760\n",
      "Train Loss: 7.831892490386963\n",
      "Average Return: 13.911388397216797\n",
      "\n",
      "New best average return found at 14.827301025390625! Saving checkpoint at iteration 770\n",
      "\n",
      "Iteration: 770\n",
      "Train Loss: 3.8435781002044678\n",
      "Average Return: 14.827301025390625\n",
      "\n",
      "Iteration: 780\n",
      "Train Loss: 11.323591232299805\n",
      "Average Return: 13.641878128051758\n",
      "\n",
      "Iteration: 790\n",
      "Train Loss: 14.413021087646484\n",
      "Average Return: 13.303524017333984\n",
      "\n",
      "Iteration: 800\n",
      "Train Loss: 5.11098051071167\n",
      "Average Return: 13.01645565032959\n",
      "\n",
      "Iteration: 810\n",
      "Train Loss: 4.781370639801025\n",
      "Average Return: 13.507345199584961\n",
      "\n",
      "Iteration: 820\n",
      "Train Loss: 6.949954986572266\n",
      "Average Return: 13.802309036254883\n",
      "\n",
      "Iteration: 830\n",
      "Train Loss: 6.10225248336792\n",
      "Average Return: 14.019679069519043\n",
      "\n",
      "New best average return found at 14.934133529663086! Saving checkpoint at iteration 840\n",
      "\n",
      "Iteration: 840\n",
      "Train Loss: 4.50235652923584\n",
      "Average Return: 14.934133529663086\n",
      "\n",
      "Iteration: 850\n",
      "Train Loss: 6.085288047790527\n",
      "Average Return: 14.219408988952637\n",
      "\n",
      "Iteration: 860\n",
      "Train Loss: 3.1369845867156982\n",
      "Average Return: 14.389153480529785\n",
      "\n",
      "Iteration: 870\n",
      "Train Loss: 3.9513726234436035\n",
      "Average Return: 14.49252700805664\n",
      "\n",
      "Iteration: 880\n",
      "Train Loss: 4.270667552947998\n",
      "Average Return: 14.621201515197754\n",
      "\n",
      "Iteration: 890\n",
      "Train Loss: 3.7906277179718018\n",
      "Average Return: 14.39078140258789\n",
      "\n",
      "Iteration: 900\n",
      "Train Loss: 2.478670120239258\n",
      "Average Return: 14.021153450012207\n",
      "\n",
      "Iteration: 910\n",
      "Train Loss: 1.8894983530044556\n",
      "Average Return: 13.037583351135254\n",
      "\n",
      "Iteration: 920\n",
      "Train Loss: 3.3100123405456543\n",
      "Average Return: 13.497762680053711\n",
      "\n",
      "Iteration: 930\n",
      "Train Loss: 5.501938343048096\n",
      "Average Return: 12.927680969238281\n",
      "\n",
      "Iteration: 940\n",
      "Train Loss: 2.2191684246063232\n",
      "Average Return: 13.121758460998535\n",
      "\n",
      "Iteration: 950\n",
      "Train Loss: 1.8438928127288818\n",
      "Average Return: 13.067819595336914\n",
      "\n",
      "Iteration: 960\n",
      "Train Loss: 2.5712850093841553\n",
      "Average Return: 14.287580490112305\n",
      "\n",
      "Iteration: 970\n",
      "Train Loss: 1.1203941106796265\n",
      "Average Return: 14.253854751586914\n",
      "\n",
      "Iteration: 980\n",
      "Train Loss: 1.2937248945236206\n",
      "Average Return: 14.143376350402832\n",
      "\n",
      "Iteration: 990\n",
      "Train Loss: 2.237954616546631\n",
      "Average Return: 14.38394832611084\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 1.0868829488754272\n",
      "Average Return: 14.519984245300293\n",
      "Training has started...\n",
      "\n",
      "New best average return found at 27.00905418395996! Saving checkpoint at iteration 0\n",
      "\n",
      "Iteration: 10\n",
      "Train Loss: 10.988387107849121\n",
      "Average Return: 19.906253814697266\n",
      "\n",
      "Iteration: 20\n",
      "Train Loss: 9.484980583190918\n",
      "Average Return: 14.414373397827148\n",
      "\n",
      "Iteration: 30\n",
      "Train Loss: 24.08005142211914\n",
      "Average Return: 20.030261993408203\n",
      "\n",
      "Iteration: 40\n",
      "Train Loss: 16.56072998046875\n",
      "Average Return: 20.852895736694336\n",
      "\n",
      "Iteration: 50\n",
      "Train Loss: 6.905545234680176\n",
      "Average Return: 15.168685913085938\n",
      "\n",
      "New best average return found at 29.30465316772461! Saving checkpoint at iteration 60\n",
      "\n",
      "Iteration: 60\n",
      "Train Loss: 10.630707740783691\n",
      "Average Return: 29.30465316772461\n",
      "\n",
      "Iteration: 70\n",
      "Train Loss: 317.9141540527344\n",
      "Average Return: 26.88119125366211\n",
      "\n",
      "Iteration: 80\n",
      "Train Loss: 82.42691802978516\n",
      "Average Return: 26.334577560424805\n",
      "\n",
      "Iteration: 90\n",
      "Train Loss: 41.540306091308594\n",
      "Average Return: 25.140981674194336\n",
      "\n",
      "Iteration: 100\n",
      "Train Loss: 60.188140869140625\n",
      "Average Return: 24.854217529296875\n",
      "\n",
      "Iteration: 110\n",
      "Train Loss: 138.69822692871094\n",
      "Average Return: 23.647720336914062\n",
      "\n",
      "Iteration: 120\n",
      "Train Loss: 56.7470588684082\n",
      "Average Return: 25.160608291625977\n",
      "\n",
      "Iteration: 130\n",
      "Train Loss: 222.20787048339844\n",
      "Average Return: 25.917415618896484\n",
      "\n",
      "Iteration: 140\n",
      "Train Loss: 68.6972885131836\n",
      "Average Return: 24.378942489624023\n",
      "\n",
      "Iteration: 150\n",
      "Train Loss: 23.47235679626465\n",
      "Average Return: 25.280094146728516\n",
      "\n",
      "Iteration: 160\n",
      "Train Loss: 17.472492218017578\n",
      "Average Return: 26.06428337097168\n",
      "\n",
      "Iteration: 170\n",
      "Train Loss: 12.107809066772461\n",
      "Average Return: 27.354036331176758\n",
      "\n",
      "Iteration: 180\n",
      "Train Loss: 84.81118774414062\n",
      "Average Return: 27.48707389831543\n",
      "\n",
      "Iteration: 190\n",
      "Train Loss: 24.706457138061523\n",
      "Average Return: 23.127605438232422\n",
      "\n",
      "Iteration: 200\n",
      "Train Loss: 37.50743865966797\n",
      "Average Return: 24.38865089416504\n",
      "\n",
      "Iteration: 210\n",
      "Train Loss: 30.871620178222656\n",
      "Average Return: 26.000690460205078\n",
      "\n",
      "Iteration: 220\n",
      "Train Loss: 18.749298095703125\n",
      "Average Return: 26.516393661499023\n",
      "\n",
      "Iteration: 230\n",
      "Train Loss: 45.879547119140625\n",
      "Average Return: 25.262744903564453\n",
      "\n",
      "Iteration: 240\n",
      "Train Loss: 49.41960906982422\n",
      "Average Return: 23.718111038208008\n",
      "\n",
      "Iteration: 250\n",
      "Train Loss: 88.681396484375\n",
      "Average Return: 24.79686164855957\n",
      "\n",
      "Iteration: 260\n",
      "Train Loss: 11.753690719604492\n",
      "Average Return: 25.967527389526367\n",
      "\n",
      "Iteration: 270\n",
      "Train Loss: 92.4933853149414\n",
      "Average Return: 25.920394897460938\n",
      "\n",
      "Iteration: 280\n",
      "Train Loss: 19.805910110473633\n",
      "Average Return: 24.942636489868164\n",
      "\n",
      "Iteration: 290\n",
      "Train Loss: 58.846412658691406\n",
      "Average Return: 24.480161666870117\n",
      "\n",
      "Iteration: 300\n",
      "Train Loss: 22.253971099853516\n",
      "Average Return: 25.2476863861084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\metrics\\trading\\sortino.py:20: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.exp(average_returns/std_downfall_returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 310\n",
      "Train Loss: 26.124069213867188\n",
      "Average Return: 26.255859375\n",
      "\n",
      "Iteration: 320\n",
      "Train Loss: 41.37175369262695\n",
      "Average Return: 26.070159912109375\n",
      "\n",
      "Iteration: 330\n",
      "Train Loss: 106.23200988769531\n",
      "Average Return: 26.699647903442383\n",
      "\n",
      "Iteration: 340\n",
      "Train Loss: 47.64500045776367\n",
      "Average Return: 26.317041397094727\n",
      "\n",
      "Iteration: 350\n",
      "Train Loss: 19.235759735107422\n",
      "Average Return: 25.577198028564453\n",
      "\n",
      "Iteration: 360\n",
      "Train Loss: 19.773860931396484\n",
      "Average Return: 25.15020179748535\n",
      "\n",
      "Iteration: 370\n",
      "Train Loss: 14.424314498901367\n",
      "Average Return: 26.3021240234375\n",
      "\n",
      "Iteration: 380\n",
      "Train Loss: 40.704532623291016\n",
      "Average Return: 25.834980010986328\n",
      "\n",
      "Iteration: 390\n",
      "Train Loss: 21.924833297729492\n",
      "Average Return: 25.750274658203125\n",
      "\n",
      "Iteration: 400\n",
      "Train Loss: 26.356698989868164\n",
      "Average Return: 26.177915573120117\n",
      "\n",
      "Iteration: 410\n",
      "Train Loss: 50.576873779296875\n",
      "Average Return: 25.78809928894043\n",
      "\n",
      "Iteration: 420\n",
      "Train Loss: 17.180110931396484\n",
      "Average Return: 24.58803367614746\n",
      "\n",
      "Iteration: 430\n",
      "Train Loss: 41.69828796386719\n",
      "Average Return: 25.8304443359375\n",
      "\n",
      "Iteration: 440\n",
      "Train Loss: 26.128070831298828\n",
      "Average Return: 26.250988006591797\n",
      "\n",
      "Iteration: 450\n",
      "Train Loss: 30.332155227661133\n",
      "Average Return: 27.16217041015625\n",
      "\n",
      "Iteration: 460\n",
      "Train Loss: 9.37160873413086\n",
      "Average Return: 26.083694458007812\n",
      "\n",
      "Iteration: 470\n",
      "Train Loss: 11.872611999511719\n",
      "Average Return: 25.353179931640625\n",
      "\n",
      "Iteration: 480\n",
      "Train Loss: 22.57798957824707\n",
      "Average Return: 25.960500717163086\n",
      "\n",
      "Iteration: 490\n",
      "Train Loss: 18.483505249023438\n",
      "Average Return: 25.141563415527344\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 11.118756294250488\n",
      "Average Return: 24.303131103515625\n",
      "\n",
      "Iteration: 510\n",
      "Train Loss: 10.379581451416016\n",
      "Average Return: 19.831262588500977\n",
      "\n",
      "Iteration: 520\n",
      "Train Loss: 23.256105422973633\n",
      "Average Return: 14.759359359741211\n",
      "\n",
      "Iteration: 530\n",
      "Train Loss: 20.516651153564453\n",
      "Average Return: 11.02569580078125\n",
      "\n",
      "Iteration: 540\n",
      "Train Loss: 42.729286193847656\n",
      "Average Return: 6.099861145019531\n",
      "\n",
      "Iteration: 550\n",
      "Train Loss: 53.2620849609375\n",
      "Average Return: 2.9046638011932373\n",
      "\n",
      "Iteration: 560\n",
      "Train Loss: 24.040098190307617\n",
      "Average Return: 4.149067401885986\n",
      "\n",
      "Iteration: 570\n",
      "Train Loss: 16.507919311523438\n",
      "Average Return: 6.770331859588623\n",
      "\n",
      "Iteration: 580\n",
      "Train Loss: 25.768362045288086\n",
      "Average Return: 10.718779563903809\n",
      "\n",
      "Iteration: 590\n",
      "Train Loss: 7.627699375152588\n",
      "Average Return: 17.801025390625\n",
      "\n",
      "Iteration: 600\n",
      "Train Loss: 12.80518627166748\n",
      "Average Return: 17.46986961364746\n",
      "\n",
      "Iteration: 610\n",
      "Train Loss: 16.900066375732422\n",
      "Average Return: 21.329519271850586\n",
      "\n",
      "Iteration: 620\n",
      "Train Loss: 15.866259574890137\n",
      "Average Return: 24.35507583618164\n",
      "\n",
      "Iteration: 630\n",
      "Train Loss: 27.878093719482422\n",
      "Average Return: 27.320453643798828\n",
      "\n",
      "Iteration: 640\n",
      "Train Loss: 12.372045516967773\n",
      "Average Return: 28.67407989501953\n",
      "\n",
      "Iteration: 650\n",
      "Train Loss: 20.244245529174805\n",
      "Average Return: 26.423898696899414\n",
      "\n",
      "Iteration: 660\n",
      "Train Loss: 9.521018028259277\n",
      "Average Return: 27.241195678710938\n",
      "\n",
      "Iteration: 670\n",
      "Train Loss: 4.357295513153076\n",
      "Average Return: 25.92558479309082\n",
      "\n",
      "Iteration: 680\n",
      "Train Loss: 11.517594337463379\n",
      "Average Return: 25.91876983642578\n",
      "\n",
      "Iteration: 690\n",
      "Train Loss: 10.240943908691406\n",
      "Average Return: 25.358070373535156\n",
      "\n",
      "Iteration: 700\n",
      "Train Loss: 8.475423812866211\n",
      "Average Return: 26.799795150756836\n",
      "\n",
      "Iteration: 710\n",
      "Train Loss: 11.31973934173584\n",
      "Average Return: 26.791738510131836\n",
      "\n",
      "Iteration: 720\n",
      "Train Loss: 6.709681987762451\n",
      "Average Return: 27.906688690185547\n",
      "\n",
      "Iteration: 730\n",
      "Train Loss: 9.901198387145996\n",
      "Average Return: 28.47719383239746\n",
      "\n",
      "Iteration: 740\n",
      "Train Loss: 6.517220497131348\n",
      "Average Return: 27.9520263671875\n",
      "\n",
      "Iteration: 750\n",
      "Train Loss: 6.712682247161865\n",
      "Average Return: 28.339635848999023\n",
      "\n",
      "Iteration: 760\n",
      "Train Loss: 12.6432466506958\n",
      "Average Return: 28.733781814575195\n",
      "\n",
      "Iteration: 770\n",
      "Train Loss: 5.642937183380127\n",
      "Average Return: 28.71134376525879\n",
      "\n",
      "Iteration: 780\n",
      "Train Loss: 9.212284088134766\n",
      "Average Return: 28.429811477661133\n",
      "\n",
      "Iteration: 790\n",
      "Train Loss: 5.287350654602051\n",
      "Average Return: 28.874496459960938\n",
      "\n",
      "Iteration: 800\n",
      "Train Loss: 3.9271249771118164\n",
      "Average Return: 28.97882080078125\n",
      "\n",
      "Iteration: 810\n",
      "Train Loss: 4.53220272064209\n",
      "Average Return: 28.937942504882812\n",
      "\n",
      "Iteration: 820\n",
      "Train Loss: 3.8802330493927\n",
      "Average Return: 29.01706314086914\n",
      "\n",
      "Iteration: 830\n",
      "Train Loss: 4.345235347747803\n",
      "Average Return: 29.08653450012207\n",
      "\n",
      "Iteration: 840\n",
      "Train Loss: 2.8347830772399902\n",
      "Average Return: 28.598085403442383\n",
      "\n",
      "Iteration: 850\n",
      "Train Loss: 5.2381157875061035\n",
      "Average Return: 28.50426483154297\n",
      "\n",
      "Iteration: 860\n",
      "Train Loss: 4.795260906219482\n",
      "Average Return: 28.219253540039062\n",
      "\n",
      "Iteration: 870\n",
      "Train Loss: 2.2008917331695557\n",
      "Average Return: 28.37421989440918\n",
      "\n",
      "Iteration: 880\n",
      "Train Loss: 3.2575342655181885\n",
      "Average Return: 28.15793228149414\n",
      "\n",
      "Iteration: 890\n",
      "Train Loss: 3.9045474529266357\n",
      "Average Return: 28.376123428344727\n",
      "\n",
      "Iteration: 900\n",
      "Train Loss: 2.7103729248046875\n",
      "Average Return: 28.343299865722656\n",
      "\n",
      "Iteration: 910\n",
      "Train Loss: 4.150096893310547\n",
      "Average Return: 28.256107330322266\n",
      "\n",
      "Iteration: 920\n",
      "Train Loss: 5.487105369567871\n",
      "Average Return: 28.195894241333008\n",
      "\n",
      "Iteration: 930\n",
      "Train Loss: 3.693150281906128\n",
      "Average Return: 28.06866455078125\n",
      "\n",
      "Iteration: 940\n",
      "Train Loss: 2.2318179607391357\n",
      "Average Return: 28.399097442626953\n",
      "\n",
      "Iteration: 950\n",
      "Train Loss: 1.3928866386413574\n",
      "Average Return: 28.27189826965332\n",
      "\n",
      "Iteration: 960\n",
      "Train Loss: 5.517866611480713\n",
      "Average Return: 28.43947982788086\n",
      "\n",
      "Iteration: 970\n",
      "Train Loss: 3.52541184425354\n",
      "Average Return: 28.174768447875977\n",
      "\n",
      "Iteration: 980\n",
      "Train Loss: 4.458736419677734\n",
      "Average Return: 28.286046981811523\n",
      "\n",
      "Iteration: 990\n",
      "Train Loss: 4.503303527832031\n",
      "Average Return: 28.500886917114258\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 2.081676959991455\n",
      "Average Return: 28.33383560180664\n",
      "Training has started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\metrics\\trading\\sharpe.py:20: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.exp(average_returns/std_returns)\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best average return found at -53.16938018798828! Saving checkpoint at iteration 0\n",
      "\n",
      "New best average return found at 7.0066633224487305! Saving checkpoint at iteration 10\n",
      "\n",
      "Iteration: 10\n",
      "Train Loss: 16.020870208740234\n",
      "Average Return: 7.0066633224487305\n",
      "\n",
      "New best average return found at 9.603474617004395! Saving checkpoint at iteration 20\n",
      "\n",
      "Iteration: 20\n",
      "Train Loss: 21.447486877441406\n",
      "Average Return: 9.603474617004395\n",
      "\n",
      "Iteration: 30\n",
      "Train Loss: 7.572327613830566\n",
      "Average Return: 5.982873916625977\n",
      "\n",
      "Iteration: 40\n",
      "Train Loss: 240.0513153076172\n",
      "Average Return: 0.8460615277290344\n",
      "\n",
      "Iteration: 50\n",
      "Train Loss: 3.418628454208374\n",
      "Average Return: 5.343705177307129\n",
      "\n",
      "New best average return found at 11.387880325317383! Saving checkpoint at iteration 60\n",
      "\n",
      "Iteration: 60\n",
      "Train Loss: 9.410750389099121\n",
      "Average Return: 11.387880325317383\n",
      "\n",
      "New best average return found at 13.292974472045898! Saving checkpoint at iteration 70\n",
      "\n",
      "Iteration: 70\n",
      "Train Loss: 249.70150756835938\n",
      "Average Return: 13.292974472045898\n",
      "\n",
      "New best average return found at 14.04511833190918! Saving checkpoint at iteration 80\n",
      "\n",
      "Iteration: 80\n",
      "Train Loss: 7.3858208656311035\n",
      "Average Return: 14.04511833190918\n",
      "\n",
      "New best average return found at 17.461009979248047! Saving checkpoint at iteration 90\n",
      "\n",
      "Iteration: 90\n",
      "Train Loss: 13.18637466430664\n",
      "Average Return: 17.461009979248047\n",
      "\n",
      "Iteration: 100\n",
      "Train Loss: 82.8076171875\n",
      "Average Return: 13.01596450805664\n",
      "\n",
      "New best average return found at 18.713031768798828! Saving checkpoint at iteration 110\n",
      "\n",
      "Iteration: 110\n",
      "Train Loss: 111.19894409179688\n",
      "Average Return: 18.713031768798828\n",
      "\n",
      "New best average return found at 20.42278480529785! Saving checkpoint at iteration 120\n",
      "\n",
      "Iteration: 120\n",
      "Train Loss: 8.605650901794434\n",
      "Average Return: 20.42278480529785\n",
      "\n",
      "Iteration: 130\n",
      "Train Loss: 75.46639251708984\n",
      "Average Return: 13.291757583618164\n",
      "\n",
      "Iteration: 140\n",
      "Train Loss: 5.9943013191223145\n",
      "Average Return: 13.853906631469727\n",
      "\n",
      "Iteration: 150\n",
      "Train Loss: 7.575812816619873\n",
      "Average Return: 12.474851608276367\n",
      "\n",
      "Iteration: 160\n",
      "Train Loss: 59.804805755615234\n",
      "Average Return: 8.316776275634766\n",
      "\n",
      "Iteration: 170\n",
      "Train Loss: 32.1663932800293\n",
      "Average Return: 11.963289260864258\n",
      "\n",
      "Iteration: 180\n",
      "Train Loss: 106.55752563476562\n",
      "Average Return: 14.123245239257812\n",
      "\n",
      "Iteration: 190\n",
      "Train Loss: 4.1144795417785645\n",
      "Average Return: 15.843711853027344\n",
      "\n",
      "Iteration: 200\n",
      "Train Loss: 21.381988525390625\n",
      "Average Return: 16.506206512451172\n",
      "\n",
      "New best average return found at 21.790746688842773! Saving checkpoint at iteration 210\n",
      "\n",
      "Iteration: 210\n",
      "Train Loss: 61.46971130371094\n",
      "Average Return: 21.790746688842773\n",
      "\n",
      "Iteration: 220\n",
      "Train Loss: 2.4932804107666016\n",
      "Average Return: 13.607128143310547\n",
      "\n",
      "Iteration: 230\n",
      "Train Loss: 70.04993438720703\n",
      "Average Return: 11.62747859954834\n",
      "\n",
      "Iteration: 240\n",
      "Train Loss: 69.51183319091797\n",
      "Average Return: 15.63131332397461\n",
      "\n",
      "Iteration: 250\n",
      "Train Loss: 8.132939338684082\n",
      "Average Return: 18.795915603637695\n",
      "\n",
      "Iteration: 260\n",
      "Train Loss: 10.931977272033691\n",
      "Average Return: 13.450948715209961\n",
      "\n",
      "Iteration: 270\n",
      "Train Loss: 5.649421691894531\n",
      "Average Return: 15.760852813720703\n",
      "\n",
      "Iteration: 280\n",
      "Train Loss: 19.48240852355957\n",
      "Average Return: 13.919361114501953\n",
      "\n",
      "Iteration: 290\n",
      "Train Loss: 30.130638122558594\n",
      "Average Return: 4.754091262817383\n",
      "\n",
      "Iteration: 300\n",
      "Train Loss: 6.693187713623047\n",
      "Average Return: 3.1666758060455322\n",
      "\n",
      "Iteration: 310\n",
      "Train Loss: 23.48661231994629\n",
      "Average Return: 15.952674865722656\n",
      "\n",
      "Iteration: 320\n",
      "Train Loss: 3.5685999393463135\n",
      "Average Return: 14.174643516540527\n",
      "\n",
      "Iteration: 330\n",
      "Train Loss: 5.535865783691406\n",
      "Average Return: 16.150808334350586\n",
      "\n",
      "Iteration: 340\n",
      "Train Loss: 4.466109275817871\n",
      "Average Return: 11.39156436920166\n",
      "\n",
      "Iteration: 350\n",
      "Train Loss: 50.89561080932617\n",
      "Average Return: 15.677608489990234\n",
      "\n",
      "Iteration: 360\n",
      "Train Loss: 9.021743774414062\n",
      "Average Return: 16.26715660095215\n",
      "\n",
      "Iteration: 370\n",
      "Train Loss: 46.68090057373047\n",
      "Average Return: 20.089998245239258\n",
      "\n",
      "Iteration: 380\n",
      "Train Loss: 49.81954574584961\n",
      "Average Return: 20.660791397094727\n",
      "\n",
      "Iteration: 390\n",
      "Train Loss: 8.928513526916504\n",
      "Average Return: 20.70993423461914\n",
      "\n",
      "Iteration: 400\n",
      "Train Loss: 11.118508338928223\n",
      "Average Return: 21.373302459716797\n",
      "\n",
      "Iteration: 410\n",
      "Train Loss: 9.310580253601074\n",
      "Average Return: 20.08750343322754\n",
      "\n",
      "Iteration: 420\n",
      "Train Loss: 18.57520866394043\n",
      "Average Return: 20.297964096069336\n",
      "\n",
      "Iteration: 430\n",
      "Train Loss: 10.786714553833008\n",
      "Average Return: 20.29979705810547\n",
      "\n",
      "Iteration: 440\n",
      "Train Loss: 12.780373573303223\n",
      "Average Return: 18.83087730407715\n",
      "\n",
      "Iteration: 450\n",
      "Train Loss: 2.8851094245910645\n",
      "Average Return: 19.947473526000977\n",
      "\n",
      "Iteration: 460\n",
      "Train Loss: 36.772945404052734\n",
      "Average Return: 14.118073463439941\n",
      "\n",
      "Iteration: 470\n",
      "Train Loss: 4.3315348625183105\n",
      "Average Return: 13.619087219238281\n",
      "\n",
      "Iteration: 480\n",
      "Train Loss: 41.179054260253906\n",
      "Average Return: 17.773025512695312\n",
      "\n",
      "Iteration: 490\n",
      "Train Loss: 10.486613273620605\n",
      "Average Return: 12.478010177612305\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 52.11640167236328\n",
      "Average Return: 13.920574188232422\n",
      "\n",
      "Iteration: 510\n",
      "Train Loss: 17.969274520874023\n",
      "Average Return: 12.220952033996582\n",
      "\n",
      "Iteration: 520\n",
      "Train Loss: 65.19528198242188\n",
      "Average Return: 13.012805938720703\n",
      "\n",
      "Iteration: 530\n",
      "Train Loss: 10.140687942504883\n",
      "Average Return: 12.85046672821045\n",
      "\n",
      "Iteration: 540\n",
      "Train Loss: 55.398311614990234\n",
      "Average Return: 13.045550346374512\n",
      "\n",
      "Iteration: 550\n",
      "Train Loss: 48.78448486328125\n",
      "Average Return: 13.51598072052002\n",
      "\n",
      "Iteration: 560\n",
      "Train Loss: 12.79448127746582\n",
      "Average Return: 13.73784351348877\n",
      "\n",
      "Iteration: 570\n",
      "Train Loss: 56.768463134765625\n",
      "Average Return: 12.605636596679688\n",
      "\n",
      "Iteration: 580\n",
      "Train Loss: 15.446490287780762\n",
      "Average Return: 8.916104316711426\n",
      "\n",
      "Iteration: 590\n",
      "Train Loss: 2.980489492416382\n",
      "Average Return: 12.141040802001953\n",
      "\n",
      "Iteration: 600\n",
      "Train Loss: 74.7153549194336\n",
      "Average Return: 13.346453666687012\n",
      "\n",
      "Iteration: 610\n",
      "Train Loss: 35.11101150512695\n",
      "Average Return: 13.068037033081055\n",
      "\n",
      "Iteration: 620\n",
      "Train Loss: 25.825441360473633\n",
      "Average Return: 13.59424877166748\n",
      "\n",
      "Iteration: 630\n",
      "Train Loss: 12.8534574508667\n",
      "Average Return: 13.178204536437988\n",
      "\n",
      "Iteration: 640\n",
      "Train Loss: 5.645459175109863\n",
      "Average Return: 13.33051586151123\n",
      "\n",
      "Iteration: 650\n",
      "Train Loss: 28.079082489013672\n",
      "Average Return: 13.551559448242188\n",
      "\n",
      "Iteration: 660\n",
      "Train Loss: 5.542553424835205\n",
      "Average Return: 12.955719947814941\n",
      "\n",
      "Iteration: 670\n",
      "Train Loss: 23.465728759765625\n",
      "Average Return: 13.4402437210083\n",
      "\n",
      "Iteration: 680\n",
      "Train Loss: 65.25836181640625\n",
      "Average Return: 13.170243263244629\n",
      "\n",
      "Iteration: 690\n",
      "Train Loss: 27.52640724182129\n",
      "Average Return: 13.292046546936035\n",
      "\n",
      "Iteration: 700\n",
      "Train Loss: 44.99168014526367\n",
      "Average Return: 13.322074890136719\n",
      "\n",
      "Iteration: 710\n",
      "Train Loss: 16.92856216430664\n",
      "Average Return: 13.272967338562012\n",
      "\n",
      "Iteration: 720\n",
      "Train Loss: 9.72978687286377\n",
      "Average Return: 13.300128936767578\n",
      "\n",
      "Iteration: 730\n",
      "Train Loss: 27.877553939819336\n",
      "Average Return: 13.339410781860352\n",
      "\n",
      "Iteration: 740\n",
      "Train Loss: 6.827239513397217\n",
      "Average Return: 12.741219520568848\n",
      "\n",
      "Iteration: 750\n",
      "Train Loss: 55.24945831298828\n",
      "Average Return: 13.118633270263672\n",
      "\n",
      "Iteration: 760\n",
      "Train Loss: 7.438310623168945\n",
      "Average Return: 13.080516815185547\n",
      "\n",
      "Iteration: 770\n",
      "Train Loss: 17.29999351501465\n",
      "Average Return: 13.232644081115723\n",
      "\n",
      "Iteration: 780\n",
      "Train Loss: 10.105113983154297\n",
      "Average Return: 12.979273796081543\n",
      "\n",
      "Iteration: 790\n",
      "Train Loss: 40.73450469970703\n",
      "Average Return: 13.474783897399902\n",
      "\n",
      "Iteration: 800\n",
      "Train Loss: 10.11422348022461\n",
      "Average Return: 13.071828842163086\n",
      "\n",
      "Iteration: 810\n",
      "Train Loss: 12.899821281433105\n",
      "Average Return: 15.868502616882324\n",
      "\n",
      "Iteration: 820\n",
      "Train Loss: 4.05698299407959\n",
      "Average Return: 17.774883270263672\n",
      "\n",
      "Iteration: 830\n",
      "Train Loss: 43.15232849121094\n",
      "Average Return: 13.214700698852539\n",
      "\n",
      "Iteration: 840\n",
      "Train Loss: 9.643631935119629\n",
      "Average Return: 12.817647933959961\n",
      "\n",
      "Iteration: 850\n",
      "Train Loss: 6.522194862365723\n",
      "Average Return: 13.505845069885254\n",
      "\n",
      "Iteration: 860\n",
      "Train Loss: 9.08486270904541\n",
      "Average Return: 13.044452667236328\n",
      "\n",
      "Iteration: 870\n",
      "Train Loss: 10.976066589355469\n",
      "Average Return: 12.520882606506348\n",
      "\n",
      "Iteration: 880\n",
      "Train Loss: 15.689227104187012\n",
      "Average Return: 12.811278343200684\n",
      "\n",
      "Iteration: 890\n",
      "Train Loss: 26.393142700195312\n",
      "Average Return: 12.789727210998535\n",
      "\n",
      "Iteration: 900\n",
      "Train Loss: 22.10457992553711\n",
      "Average Return: 12.72379207611084\n",
      "\n",
      "Iteration: 910\n",
      "Train Loss: 5.257726669311523\n",
      "Average Return: 13.320640563964844\n",
      "\n",
      "Iteration: 920\n",
      "Train Loss: 6.125690460205078\n",
      "Average Return: 13.092668533325195\n",
      "\n",
      "Iteration: 930\n",
      "Train Loss: 7.371875286102295\n",
      "Average Return: 12.759215354919434\n",
      "\n",
      "Iteration: 940\n",
      "Train Loss: 14.066940307617188\n",
      "Average Return: 12.99600601196289\n",
      "\n",
      "Iteration: 950\n",
      "Train Loss: 9.03464412689209\n",
      "Average Return: 12.829207420349121\n",
      "\n",
      "Iteration: 960\n",
      "Train Loss: 5.7928032875061035\n",
      "Average Return: 13.734415054321289\n",
      "\n",
      "Iteration: 970\n",
      "Train Loss: 3.581531524658203\n",
      "Average Return: 13.515545845031738\n",
      "\n",
      "Iteration: 980\n",
      "Train Loss: 3.1377081871032715\n",
      "Average Return: 13.00814437866211\n",
      "\n",
      "Iteration: 990\n",
      "Train Loss: 2.658965826034546\n",
      "Average Return: 12.9683837890625\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 7.799800872802734\n",
      "Average Return: 13.377433776855469\n",
      "Training has started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\metrics\\trading\\sharpe.py:20: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.exp(average_returns/std_returns)\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best average return found at -70.37957000732422! Saving checkpoint at iteration 0\n",
      "\n",
      "New best average return found at 22.008630752563477! Saving checkpoint at iteration 10\n",
      "\n",
      "Iteration: 10\n",
      "Train Loss: 8.902644157409668\n",
      "Average Return: 22.008630752563477\n",
      "\n",
      "Iteration: 20\n",
      "Train Loss: 103.37938690185547\n",
      "Average Return: 21.374168395996094\n",
      "\n",
      "New best average return found at 25.21816062927246! Saving checkpoint at iteration 30\n",
      "\n",
      "Iteration: 30\n",
      "Train Loss: 28.025283813476562\n",
      "Average Return: 25.21816062927246\n",
      "\n",
      "New best average return found at 28.651527404785156! Saving checkpoint at iteration 40\n",
      "\n",
      "Iteration: 40\n",
      "Train Loss: 490.78509521484375\n",
      "Average Return: 28.651527404785156\n",
      "\n",
      "New best average return found at 34.956356048583984! Saving checkpoint at iteration 50\n",
      "\n",
      "Iteration: 50\n",
      "Train Loss: 22.34660530090332\n",
      "Average Return: 34.956356048583984\n",
      "\n",
      "Iteration: 60\n",
      "Train Loss: 35.16065979003906\n",
      "Average Return: 28.90963363647461\n",
      "\n",
      "Iteration: 70\n",
      "Train Loss: 207.6363983154297\n",
      "Average Return: 33.41512680053711\n",
      "\n",
      "New best average return found at 37.607635498046875! Saving checkpoint at iteration 80\n",
      "\n",
      "Iteration: 80\n",
      "Train Loss: 24.84682846069336\n",
      "Average Return: 37.607635498046875\n",
      "\n",
      "Iteration: 90\n",
      "Train Loss: 1.9036866426467896\n",
      "Average Return: 21.968509674072266\n",
      "\n",
      "Iteration: 100\n",
      "Train Loss: 95.5042953491211\n",
      "Average Return: 34.51063537597656\n",
      "\n",
      "Iteration: 110\n",
      "Train Loss: 306.4447021484375\n",
      "Average Return: 31.73626136779785\n",
      "\n",
      "Iteration: 120\n",
      "Train Loss: 13.078022956848145\n",
      "Average Return: 30.6181640625\n",
      "\n",
      "Iteration: 130\n",
      "Train Loss: 55.13180923461914\n",
      "Average Return: 30.556928634643555\n",
      "\n",
      "Iteration: 140\n",
      "Train Loss: 34.152801513671875\n",
      "Average Return: 30.106555938720703\n",
      "\n",
      "Iteration: 150\n",
      "Train Loss: 18.528026580810547\n",
      "Average Return: 29.914350509643555\n",
      "\n",
      "Iteration: 160\n",
      "Train Loss: 84.08231353759766\n",
      "Average Return: 27.569538116455078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\metrics\\trading\\sortino.py:20: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.exp(average_returns/std_downfall_returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 170\n",
      "Train Loss: 20.440929412841797\n",
      "Average Return: 27.96502685546875\n",
      "\n",
      "Iteration: 180\n",
      "Train Loss: 118.69840240478516\n",
      "Average Return: 29.34032440185547\n",
      "\n",
      "Iteration: 190\n",
      "Train Loss: 9.380199432373047\n",
      "Average Return: 28.560409545898438\n",
      "\n",
      "Iteration: 200\n",
      "Train Loss: 48.52779769897461\n",
      "Average Return: 30.519847869873047\n",
      "\n",
      "Iteration: 210\n",
      "Train Loss: 261.7218933105469\n",
      "Average Return: 26.530473709106445\n",
      "\n",
      "Iteration: 220\n",
      "Train Loss: 3.2605135440826416\n",
      "Average Return: 21.467056274414062\n",
      "\n",
      "Iteration: 230\n",
      "Train Loss: 168.5249786376953\n",
      "Average Return: 25.69187355041504\n",
      "\n",
      "Iteration: 240\n",
      "Train Loss: 71.05810546875\n",
      "Average Return: 20.825489044189453\n",
      "\n",
      "Iteration: 250\n",
      "Train Loss: 9.380815505981445\n",
      "Average Return: 30.432716369628906\n",
      "\n",
      "Iteration: 260\n",
      "Train Loss: 46.635955810546875\n",
      "Average Return: 29.86541748046875\n",
      "\n",
      "Iteration: 270\n",
      "Train Loss: 30.219234466552734\n",
      "Average Return: 30.262235641479492\n",
      "\n",
      "Iteration: 280\n",
      "Train Loss: 27.41463851928711\n",
      "Average Return: 30.821786880493164\n",
      "\n",
      "Iteration: 290\n",
      "Train Loss: 48.271484375\n",
      "Average Return: 30.219783782958984\n",
      "\n",
      "Iteration: 300\n",
      "Train Loss: 14.364150047302246\n",
      "Average Return: 30.537858963012695\n",
      "\n",
      "Iteration: 310\n",
      "Train Loss: 292.3840026855469\n",
      "Average Return: 29.902151107788086\n",
      "\n",
      "Iteration: 320\n",
      "Train Loss: 26.374507904052734\n",
      "Average Return: 30.67232894897461\n",
      "\n",
      "Iteration: 330\n",
      "Train Loss: 37.23127746582031\n",
      "Average Return: 30.639623641967773\n",
      "\n",
      "Iteration: 340\n",
      "Train Loss: 51.477420806884766\n",
      "Average Return: 29.783720016479492\n",
      "\n",
      "Iteration: 350\n",
      "Train Loss: 95.26764678955078\n",
      "Average Return: 29.783720016479492\n",
      "\n",
      "Iteration: 360\n",
      "Train Loss: 38.3397331237793\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 370\n",
      "Train Loss: 34.18900680541992\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 380\n",
      "Train Loss: 25.16862678527832\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 390\n",
      "Train Loss: 157.70872497558594\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 400\n",
      "Train Loss: 22.674720764160156\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 410\n",
      "Train Loss: 27.053924560546875\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 420\n",
      "Train Loss: 61.70647430419922\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 430\n",
      "Train Loss: 44.219078063964844\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 440\n",
      "Train Loss: 45.79608917236328\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 450\n",
      "Train Loss: 9.148984909057617\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 460\n",
      "Train Loss: 78.54158020019531\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 470\n",
      "Train Loss: 10.827796936035156\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 480\n",
      "Train Loss: 72.17581939697266\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 490\n",
      "Train Loss: 22.701217651367188\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 59.570770263671875\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 510\n",
      "Train Loss: 35.35899353027344\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 520\n",
      "Train Loss: 50.27617263793945\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 530\n",
      "Train Loss: 5.6316046714782715\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 540\n",
      "Train Loss: 29.058273315429688\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 550\n",
      "Train Loss: 41.3600959777832\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 560\n",
      "Train Loss: 16.703533172607422\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 570\n",
      "Train Loss: 40.54213333129883\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 580\n",
      "Train Loss: 37.850425720214844\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 590\n",
      "Train Loss: 17.688669204711914\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 600\n",
      "Train Loss: 18.952104568481445\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 610\n",
      "Train Loss: 21.276811599731445\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 620\n",
      "Train Loss: 12.527653694152832\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 630\n",
      "Train Loss: 11.396919250488281\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 640\n",
      "Train Loss: 7.274628162384033\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 650\n",
      "Train Loss: 13.26972484588623\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 660\n",
      "Train Loss: 7.177597522735596\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 670\n",
      "Train Loss: 11.39826488494873\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 680\n",
      "Train Loss: 6.28428840637207\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 690\n",
      "Train Loss: 11.238432884216309\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 700\n",
      "Train Loss: 16.603717803955078\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 710\n",
      "Train Loss: 17.897924423217773\n",
      "Average Return: 29.795156478881836\n",
      "\n",
      "Iteration: 720\n",
      "Train Loss: 12.381959915161133\n",
      "Average Return: 29.324460983276367\n",
      "\n",
      "Iteration: 730\n",
      "Train Loss: 11.333327293395996\n",
      "Average Return: 29.346393585205078\n",
      "\n",
      "Iteration: 740\n",
      "Train Loss: 4.552072048187256\n",
      "Average Return: 28.606300354003906\n",
      "\n",
      "Iteration: 750\n",
      "Train Loss: 9.950947761535645\n",
      "Average Return: 23.237281799316406\n",
      "\n",
      "Iteration: 760\n",
      "Train Loss: 11.69604778289795\n",
      "Average Return: 10.45586109161377\n",
      "\n",
      "Iteration: 770\n",
      "Train Loss: 3.624530553817749\n",
      "Average Return: 18.952363967895508\n",
      "\n",
      "Iteration: 780\n",
      "Train Loss: 4.146727085113525\n",
      "Average Return: 25.440011978149414\n",
      "\n",
      "Iteration: 790\n",
      "Train Loss: 6.4997406005859375\n",
      "Average Return: 29.687076568603516\n",
      "\n",
      "Iteration: 800\n",
      "Train Loss: 7.244185447692871\n",
      "Average Return: 36.08198928833008\n",
      "\n",
      "Iteration: 810\n",
      "Train Loss: 5.11600399017334\n",
      "Average Return: 36.48427963256836\n",
      "\n",
      "Iteration: 820\n",
      "Train Loss: 3.8990936279296875\n",
      "Average Return: 35.42446517944336\n",
      "\n",
      "Iteration: 830\n",
      "Train Loss: 8.869812965393066\n",
      "Average Return: 35.877872467041016\n",
      "\n",
      "Iteration: 840\n",
      "Train Loss: 7.746964931488037\n",
      "Average Return: 34.81062698364258\n",
      "\n",
      "Iteration: 850\n",
      "Train Loss: 4.578929901123047\n",
      "Average Return: 34.946224212646484\n",
      "\n",
      "Iteration: 860\n",
      "Train Loss: 3.9077353477478027\n",
      "Average Return: 34.98997497558594\n",
      "\n",
      "Iteration: 870\n",
      "Train Loss: 3.3710978031158447\n",
      "Average Return: 35.06792068481445\n",
      "\n",
      "Iteration: 880\n",
      "Train Loss: 5.61407995223999\n",
      "Average Return: 35.202308654785156\n",
      "\n",
      "Iteration: 890\n",
      "Train Loss: 4.586523532867432\n",
      "Average Return: 36.60660934448242\n",
      "\n",
      "Iteration: 900\n",
      "Train Loss: 3.3482837677001953\n",
      "Average Return: 35.11757278442383\n",
      "\n",
      "Iteration: 910\n",
      "Train Loss: 3.5338475704193115\n",
      "Average Return: 36.075927734375\n",
      "\n",
      "Iteration: 920\n",
      "Train Loss: 2.7918717861175537\n",
      "Average Return: 35.019229888916016\n",
      "\n",
      "Iteration: 930\n",
      "Train Loss: 1.914982557296753\n",
      "Average Return: 35.25650405883789\n",
      "\n",
      "Iteration: 940\n",
      "Train Loss: 4.433526992797852\n",
      "Average Return: 35.98371505737305\n",
      "\n",
      "Iteration: 950\n",
      "Train Loss: 2.2878425121307373\n",
      "Average Return: 36.82791519165039\n",
      "\n",
      "Iteration: 960\n",
      "Train Loss: 3.4375336170196533\n",
      "Average Return: 36.52330017089844\n",
      "\n",
      "Iteration: 970\n",
      "Train Loss: 5.32725191116333\n",
      "Average Return: 36.463279724121094\n",
      "\n",
      "Iteration: 980\n",
      "Train Loss: 1.9969242811203003\n",
      "Average Return: 35.9007453918457\n",
      "\n",
      "Iteration: 990\n",
      "Train Loss: 1.109368085861206\n",
      "Average Return: 35.95665740966797\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 1.8598759174346924\n",
      "Average Return: 35.0665168762207\n",
      "Training has started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\metrics\\trading\\sharpe.py:20: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.exp(average_returns/std_returns)\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best average return found at -43.482452392578125! Saving checkpoint at iteration 0\n",
      "\n",
      "New best average return found at 6.350119590759277! Saving checkpoint at iteration 10\n",
      "\n",
      "Iteration: 10\n",
      "Train Loss: 5.717967510223389\n",
      "Average Return: 6.350119590759277\n",
      "\n",
      "Iteration: 20\n",
      "Train Loss: 17.89725685119629\n",
      "Average Return: -0.9094057083129883\n",
      "\n",
      "Iteration: 30\n",
      "Train Loss: 5.808187961578369\n",
      "Average Return: 2.1024227142333984\n",
      "\n",
      "Iteration: 40\n",
      "Train Loss: 12.159335136413574\n",
      "Average Return: 3.8655874729156494\n",
      "\n",
      "New best average return found at 8.949485778808594! Saving checkpoint at iteration 50\n",
      "\n",
      "Iteration: 50\n",
      "Train Loss: 31.96603012084961\n",
      "Average Return: 8.949485778808594\n",
      "\n",
      "New best average return found at 11.190781593322754! Saving checkpoint at iteration 60\n",
      "\n",
      "Iteration: 60\n",
      "Train Loss: 87.64916229248047\n",
      "Average Return: 11.190781593322754\n",
      "\n",
      "New best average return found at 12.906606674194336! Saving checkpoint at iteration 70\n",
      "\n",
      "Iteration: 70\n",
      "Train Loss: 8.28457260131836\n",
      "Average Return: 12.906606674194336\n",
      "\n",
      "New best average return found at 13.313718795776367! Saving checkpoint at iteration 80\n",
      "\n",
      "Iteration: 80\n",
      "Train Loss: 19.3743896484375\n",
      "Average Return: 13.313718795776367\n",
      "\n",
      "Iteration: 90\n",
      "Train Loss: 17.805198669433594\n",
      "Average Return: 11.031230926513672\n",
      "\n",
      "Iteration: 100\n",
      "Train Loss: 11.997817993164062\n",
      "Average Return: 10.895613670349121\n",
      "\n",
      "Iteration: 110\n",
      "Train Loss: 18.265968322753906\n",
      "Average Return: 9.542101860046387\n",
      "\n",
      "Iteration: 120\n",
      "Train Loss: 7.611515998840332\n",
      "Average Return: 4.830700397491455\n",
      "\n",
      "Iteration: 130\n",
      "Train Loss: 5.648504257202148\n",
      "Average Return: 5.008371353149414\n",
      "\n",
      "Iteration: 140\n",
      "Train Loss: 60.396568298339844\n",
      "Average Return: 9.606801986694336\n",
      "\n",
      "Iteration: 150\n",
      "Train Loss: 11.814657211303711\n",
      "Average Return: 10.392273902893066\n",
      "\n",
      "Iteration: 160\n",
      "Train Loss: 18.694366455078125\n",
      "Average Return: 9.663054466247559\n",
      "\n",
      "Iteration: 170\n",
      "Train Loss: 21.34576416015625\n",
      "Average Return: 4.69434118270874\n",
      "\n",
      "Iteration: 180\n",
      "Train Loss: 8.992116928100586\n",
      "Average Return: 5.913061141967773\n",
      "\n",
      "Iteration: 190\n",
      "Train Loss: 10.1295804977417\n",
      "Average Return: 11.803691864013672\n",
      "\n",
      "Iteration: 200\n",
      "Train Loss: 53.223793029785156\n",
      "Average Return: 10.65333080291748\n",
      "\n",
      "Iteration: 210\n",
      "Train Loss: 4.5884623527526855\n",
      "Average Return: 7.315345287322998\n",
      "\n",
      "Iteration: 220\n",
      "Train Loss: 24.07796287536621\n",
      "Average Return: 4.930490970611572\n",
      "\n",
      "Iteration: 230\n",
      "Train Loss: 29.39575958251953\n",
      "Average Return: 11.80036449432373\n",
      "\n",
      "Iteration: 240\n",
      "Train Loss: 20.08658218383789\n",
      "Average Return: 12.587717056274414\n",
      "\n",
      "Iteration: 250\n",
      "Train Loss: 45.848419189453125\n",
      "Average Return: 13.164127349853516\n",
      "\n",
      "New best average return found at 13.543082237243652! Saving checkpoint at iteration 260\n",
      "\n",
      "Iteration: 260\n",
      "Train Loss: 31.785789489746094\n",
      "Average Return: 13.543082237243652\n",
      "\n",
      "Iteration: 270\n",
      "Train Loss: 9.290351867675781\n",
      "Average Return: 13.31379508972168\n",
      "\n",
      "Iteration: 280\n",
      "Train Loss: 25.917360305786133\n",
      "Average Return: 12.680493354797363\n",
      "\n",
      "Iteration: 290\n",
      "Train Loss: 5.355820178985596\n",
      "Average Return: 12.929274559020996\n",
      "\n",
      "Iteration: 300\n",
      "Train Loss: 23.670942306518555\n",
      "Average Return: 13.346511840820312\n",
      "\n",
      "Iteration: 310\n",
      "Train Loss: 75.50590515136719\n",
      "Average Return: 12.257745742797852\n",
      "\n",
      "Iteration: 320\n",
      "Train Loss: 41.982276916503906\n",
      "Average Return: 10.65924072265625\n",
      "\n",
      "Iteration: 330\n",
      "Train Loss: 32.373878479003906\n",
      "Average Return: 12.283181190490723\n",
      "\n",
      "Iteration: 340\n",
      "Train Loss: 29.412883758544922\n",
      "Average Return: 13.026080131530762\n",
      "\n",
      "Iteration: 350\n",
      "Train Loss: 21.439739227294922\n",
      "Average Return: 13.188821792602539\n",
      "\n",
      "Iteration: 360\n",
      "Train Loss: 145.67254638671875\n",
      "Average Return: 13.399723052978516\n",
      "\n",
      "New best average return found at 14.79572582244873! Saving checkpoint at iteration 370\n",
      "\n",
      "Iteration: 370\n",
      "Train Loss: 6.630471706390381\n",
      "Average Return: 14.79572582244873\n",
      "\n",
      "Iteration: 380\n",
      "Train Loss: 55.40235900878906\n",
      "Average Return: 13.95549201965332\n",
      "\n",
      "Iteration: 390\n",
      "Train Loss: 51.97328567504883\n",
      "Average Return: 13.434450149536133\n",
      "\n",
      "Iteration: 400\n",
      "Train Loss: 86.96724700927734\n",
      "Average Return: 12.550981521606445\n",
      "\n",
      "Iteration: 410\n",
      "Train Loss: 13.420316696166992\n",
      "Average Return: 13.19485855102539\n",
      "\n",
      "Iteration: 420\n",
      "Train Loss: 33.23507308959961\n",
      "Average Return: 13.913655281066895\n",
      "\n",
      "Iteration: 430\n",
      "Train Loss: 11.162884712219238\n",
      "Average Return: 13.86148452758789\n",
      "\n",
      "Iteration: 440\n",
      "Train Loss: 17.250499725341797\n",
      "Average Return: 13.515096664428711\n",
      "\n",
      "Iteration: 450\n",
      "Train Loss: 8.529043197631836\n",
      "Average Return: 13.21894645690918\n",
      "\n",
      "Iteration: 460\n",
      "Train Loss: 24.273784637451172\n",
      "Average Return: 12.998827934265137\n",
      "\n",
      "Iteration: 470\n",
      "Train Loss: 15.031035423278809\n",
      "Average Return: 13.512619018554688\n",
      "\n",
      "Iteration: 480\n",
      "Train Loss: 41.71146774291992\n",
      "Average Return: 13.415714263916016\n",
      "\n",
      "Iteration: 490\n",
      "Train Loss: 10.93930435180664\n",
      "Average Return: 12.903731346130371\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 25.446388244628906\n",
      "Average Return: 13.636429786682129\n",
      "\n",
      "Iteration: 510\n",
      "Train Loss: 9.256244659423828\n",
      "Average Return: 13.657088279724121\n",
      "\n",
      "Iteration: 520\n",
      "Train Loss: 14.734920501708984\n",
      "Average Return: 13.950783729553223\n",
      "\n",
      "Iteration: 530\n",
      "Train Loss: 8.94505786895752\n",
      "Average Return: 13.507519721984863\n",
      "\n",
      "Iteration: 540\n",
      "Train Loss: 8.285704612731934\n",
      "Average Return: 12.117385864257812\n",
      "\n",
      "Iteration: 550\n",
      "Train Loss: 15.666844367980957\n",
      "Average Return: 13.911646842956543\n",
      "\n",
      "Iteration: 560\n",
      "Train Loss: 21.22838020324707\n",
      "Average Return: 14.378021240234375\n",
      "\n",
      "Iteration: 570\n",
      "Train Loss: 11.51523208618164\n",
      "Average Return: 14.310501098632812\n",
      "\n",
      "New best average return found at 14.908280372619629! Saving checkpoint at iteration 580\n",
      "\n",
      "Iteration: 580\n",
      "Train Loss: 10.492154121398926\n",
      "Average Return: 14.908280372619629\n",
      "\n",
      "Iteration: 590\n",
      "Train Loss: 3.1638925075531006\n",
      "Average Return: 14.276694297790527\n",
      "\n",
      "Iteration: 600\n",
      "Train Loss: 21.08310317993164\n",
      "Average Return: 13.340779304504395\n",
      "\n",
      "Iteration: 610\n",
      "Train Loss: 6.001909255981445\n",
      "Average Return: 13.437372207641602\n",
      "\n",
      "Iteration: 620\n",
      "Train Loss: 8.014055252075195\n",
      "Average Return: 13.523581504821777\n",
      "\n",
      "Iteration: 630\n",
      "Train Loss: 17.37592124938965\n",
      "Average Return: 13.427958488464355\n",
      "\n",
      "Iteration: 640\n",
      "Train Loss: 9.346596717834473\n",
      "Average Return: 13.673680305480957\n",
      "\n",
      "Iteration: 650\n",
      "Train Loss: 7.497898578643799\n",
      "Average Return: 13.425616264343262\n",
      "\n",
      "Iteration: 660\n",
      "Train Loss: 13.538159370422363\n",
      "Average Return: 8.592622756958008\n",
      "\n",
      "Iteration: 670\n",
      "Train Loss: 8.224180221557617\n",
      "Average Return: 11.685394287109375\n",
      "\n",
      "Iteration: 680\n",
      "Train Loss: 7.818062782287598\n",
      "Average Return: 13.566534996032715\n",
      "\n",
      "Iteration: 690\n",
      "Train Loss: 8.800600051879883\n",
      "Average Return: 14.191150665283203\n",
      "\n",
      "Iteration: 700\n",
      "Train Loss: 4.675321578979492\n",
      "Average Return: 14.065905570983887\n",
      "\n",
      "Iteration: 710\n",
      "Train Loss: 5.582211017608643\n",
      "Average Return: 14.08725357055664\n",
      "\n",
      "Iteration: 720\n",
      "Train Loss: 8.500458717346191\n",
      "Average Return: 14.231114387512207\n",
      "\n",
      "Iteration: 730\n",
      "Train Loss: 7.406980991363525\n",
      "Average Return: 14.788544654846191\n",
      "\n",
      "Iteration: 740\n",
      "Train Loss: 8.402616500854492\n",
      "Average Return: 14.577288627624512\n",
      "\n",
      "Iteration: 750\n",
      "Train Loss: 7.351477146148682\n",
      "Average Return: 14.277411460876465\n",
      "\n",
      "Iteration: 760\n",
      "Train Loss: 6.530326843261719\n",
      "Average Return: 14.454573631286621\n",
      "\n",
      "Iteration: 770\n",
      "Train Loss: 9.271016120910645\n",
      "Average Return: 14.121471405029297\n",
      "\n",
      "Iteration: 780\n",
      "Train Loss: 3.60937762260437\n",
      "Average Return: 14.635004997253418\n",
      "\n",
      "Iteration: 790\n",
      "Train Loss: 4.730104923248291\n",
      "Average Return: 14.610838890075684\n",
      "\n",
      "Iteration: 800\n",
      "Train Loss: 8.369335174560547\n",
      "Average Return: 14.49178695678711\n",
      "\n",
      "Iteration: 810\n",
      "Train Loss: 4.425408363342285\n",
      "Average Return: 14.414807319641113\n",
      "\n",
      "Iteration: 820\n",
      "Train Loss: 7.806736469268799\n",
      "Average Return: 14.554168701171875\n",
      "\n",
      "Iteration: 830\n",
      "Train Loss: 7.722527503967285\n",
      "Average Return: 14.537519454956055\n",
      "\n",
      "New best average return found at 15.032380104064941! Saving checkpoint at iteration 840\n",
      "\n",
      "Iteration: 840\n",
      "Train Loss: 13.338679313659668\n",
      "Average Return: 15.032380104064941\n",
      "\n",
      "Iteration: 850\n",
      "Train Loss: 6.583191871643066\n",
      "Average Return: 15.00649642944336\n",
      "\n",
      "New best average return found at 15.395424842834473! Saving checkpoint at iteration 860\n",
      "\n",
      "Iteration: 860\n",
      "Train Loss: 3.6206047534942627\n",
      "Average Return: 15.395424842834473\n",
      "\n",
      "New best average return found at 15.70590591430664! Saving checkpoint at iteration 870\n",
      "\n",
      "Iteration: 870\n",
      "Train Loss: 3.890368700027466\n",
      "Average Return: 15.70590591430664\n",
      "\n",
      "New best average return found at 15.848814964294434! Saving checkpoint at iteration 880\n",
      "\n",
      "Iteration: 880\n",
      "Train Loss: 5.0069451332092285\n",
      "Average Return: 15.848814964294434\n",
      "\n",
      "Iteration: 890\n",
      "Train Loss: 2.8142752647399902\n",
      "Average Return: 15.735565185546875\n",
      "\n",
      "New best average return found at 16.252450942993164! Saving checkpoint at iteration 900\n",
      "\n",
      "Iteration: 900\n",
      "Train Loss: 4.4447784423828125\n",
      "Average Return: 16.252450942993164\n",
      "\n",
      "Iteration: 910\n",
      "Train Loss: 4.710368633270264\n",
      "Average Return: 15.610177040100098\n",
      "\n",
      "Iteration: 920\n",
      "Train Loss: 7.349612712860107\n",
      "Average Return: 15.791852951049805\n",
      "\n",
      "New best average return found at 16.553607940673828! Saving checkpoint at iteration 930\n",
      "\n",
      "Iteration: 930\n",
      "Train Loss: 2.665226459503174\n",
      "Average Return: 16.553607940673828\n",
      "\n",
      "New best average return found at 16.610233306884766! Saving checkpoint at iteration 940\n",
      "\n",
      "Iteration: 940\n",
      "Train Loss: 1.0792862176895142\n",
      "Average Return: 16.610233306884766\n",
      "\n",
      "New best average return found at 16.97356414794922! Saving checkpoint at iteration 950\n",
      "\n",
      "Iteration: 950\n",
      "Train Loss: 3.308274030685425\n",
      "Average Return: 16.97356414794922\n",
      "\n",
      "Iteration: 960\n",
      "Train Loss: 3.211810350418091\n",
      "Average Return: 16.56825065612793\n",
      "\n",
      "Iteration: 970\n",
      "Train Loss: 5.185791969299316\n",
      "Average Return: 16.264169692993164\n",
      "\n",
      "Iteration: 980\n",
      "Train Loss: 3.9531619548797607\n",
      "Average Return: 16.330217361450195\n",
      "\n",
      "Iteration: 990\n",
      "Train Loss: 6.403876781463623\n",
      "Average Return: 16.493680953979492\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 5.454502582550049\n",
      "Average Return: 16.81334686279297\n",
      "Training has started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\metrics\\trading\\sharpe.py:20: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.exp(average_returns/std_returns)\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "C:\\Users\\kochlis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best average return found at -58.91674041748047! Saving checkpoint at iteration 0\n",
      "\n",
      "New best average return found at 26.361770629882812! Saving checkpoint at iteration 10\n",
      "\n",
      "Iteration: 10\n",
      "Train Loss: 6.428366184234619\n",
      "Average Return: 26.361770629882812\n",
      "\n",
      "Iteration: 20\n",
      "Train Loss: 40.21513748168945\n",
      "Average Return: 16.489662170410156\n",
      "\n",
      "Iteration: 30\n",
      "Train Loss: 6.854959487915039\n",
      "Average Return: 15.390291213989258\n",
      "\n",
      "Iteration: 40\n",
      "Train Loss: 28.67376136779785\n",
      "Average Return: 24.604101181030273\n",
      "\n",
      "Iteration: 50\n",
      "Train Loss: 27.152767181396484\n",
      "Average Return: 24.73705291748047\n",
      "\n",
      "Iteration: 60\n",
      "Train Loss: 1053.169921875\n",
      "Average Return: 26.084291458129883\n",
      "\n",
      "Iteration: 70\n",
      "Train Loss: 55.041316986083984\n",
      "Average Return: 20.81643295288086\n",
      "\n",
      "Iteration: 80\n",
      "Train Loss: 22.07404136657715\n",
      "Average Return: 20.322132110595703\n",
      "\n",
      "Iteration: 90\n",
      "Train Loss: 9.879684448242188\n",
      "Average Return: 20.790447235107422\n",
      "\n",
      "Iteration: 100\n",
      "Train Loss: 44.56101608276367\n",
      "Average Return: 19.316007614135742\n",
      "\n",
      "Iteration: 110\n",
      "Train Loss: 38.80495071411133\n",
      "Average Return: 12.022146224975586\n",
      "\n",
      "Iteration: 120\n",
      "Train Loss: 16.007911682128906\n",
      "Average Return: -5.431260108947754\n",
      "\n",
      "Iteration: 130\n",
      "Train Loss: 12.21621036529541\n",
      "Average Return: 15.544374465942383\n",
      "\n",
      "Iteration: 140\n",
      "Train Loss: 957.609375\n",
      "Average Return: 23.05544662475586\n",
      "\n",
      "New best average return found at 27.856672286987305! Saving checkpoint at iteration 150\n",
      "\n",
      "Iteration: 150\n",
      "Train Loss: 135.41432189941406\n",
      "Average Return: 27.856672286987305\n",
      "\n",
      "Iteration: 160\n",
      "Train Loss: 22.837251663208008\n",
      "Average Return: 26.99066734313965\n",
      "\n",
      "Iteration: 170\n",
      "Train Loss: 27.830507278442383\n",
      "Average Return: 27.699636459350586\n",
      "\n",
      "Iteration: 180\n",
      "Train Loss: 35.9219856262207\n",
      "Average Return: 27.079574584960938\n",
      "\n",
      "Iteration: 190\n",
      "Train Loss: 20.582101821899414\n",
      "Average Return: 27.69312858581543\n",
      "\n",
      "Iteration: 200\n",
      "Train Loss: 79.68191528320312\n",
      "Average Return: 27.730571746826172\n",
      "\n",
      "Iteration: 210\n",
      "Train Loss: 6.699339866638184\n",
      "Average Return: 26.93389320373535\n",
      "\n",
      "Iteration: 220\n",
      "Train Loss: 113.27386474609375\n",
      "Average Return: 26.165475845336914\n",
      "\n",
      "Iteration: 230\n",
      "Train Loss: 62.286441802978516\n",
      "Average Return: 26.787979125976562\n",
      "\n",
      "Iteration: 240\n",
      "Train Loss: 43.32990646362305\n",
      "Average Return: 26.17116928100586\n",
      "\n",
      "Iteration: 250\n",
      "Train Loss: 42.55189895629883\n",
      "Average Return: 25.375247955322266\n",
      "\n",
      "Iteration: 260\n",
      "Train Loss: 54.34849166870117\n",
      "Average Return: 26.153907775878906\n",
      "\n",
      "Iteration: 270\n",
      "Train Loss: 22.64219856262207\n",
      "Average Return: 26.437135696411133\n",
      "\n",
      "Iteration: 280\n",
      "Train Loss: 77.61539459228516\n",
      "Average Return: 26.41953468322754\n",
      "\n",
      "Iteration: 290\n",
      "Train Loss: 12.277826309204102\n",
      "Average Return: 26.5269718170166\n",
      "\n",
      "Iteration: 300\n",
      "Train Loss: 84.37376403808594\n",
      "Average Return: 26.939767837524414\n",
      "\n",
      "Iteration: 310\n",
      "Train Loss: 69.39826965332031\n",
      "Average Return: 27.579021453857422\n",
      "\n",
      "Iteration: 320\n",
      "Train Loss: 47.51915740966797\n",
      "Average Return: 27.673715591430664\n",
      "\n",
      "New best average return found at 27.939010620117188! Saving checkpoint at iteration 330\n",
      "\n",
      "Iteration: 330\n",
      "Train Loss: 46.51441955566406\n",
      "Average Return: 27.939010620117188\n",
      "\n",
      "Iteration: 340\n",
      "Train Loss: 101.99452209472656\n",
      "Average Return: 26.555160522460938\n",
      "\n",
      "Iteration: 350\n",
      "Train Loss: 28.43804168701172\n",
      "Average Return: 27.747966766357422\n",
      "\n",
      "Iteration: 360\n",
      "Train Loss: 75.82357025146484\n",
      "Average Return: 27.05731201171875\n",
      "\n",
      "Iteration: 370\n",
      "Train Loss: 8.860486030578613\n",
      "Average Return: 27.334917068481445\n",
      "\n",
      "Iteration: 380\n",
      "Train Loss: 72.94178771972656\n",
      "Average Return: 26.046558380126953\n",
      "\n",
      "Iteration: 390\n",
      "Train Loss: 83.26740264892578\n",
      "Average Return: 25.84210777282715\n",
      "\n",
      "Iteration: 400\n",
      "Train Loss: 41.15266799926758\n",
      "Average Return: 26.004724502563477\n",
      "\n",
      "Iteration: 410\n",
      "Train Loss: 18.796789169311523\n",
      "Average Return: 26.1152286529541\n",
      "\n",
      "Iteration: 420\n",
      "Train Loss: 74.9642333984375\n",
      "Average Return: 24.93499755859375\n",
      "\n",
      "Iteration: 430\n",
      "Train Loss: 20.84750747680664\n",
      "Average Return: 25.744800567626953\n",
      "\n",
      "Iteration: 440\n",
      "Train Loss: 101.236572265625\n",
      "Average Return: 24.386552810668945\n",
      "\n",
      "Iteration: 450\n",
      "Train Loss: 13.75354290008545\n",
      "Average Return: 24.611553192138672\n",
      "\n",
      "Iteration: 460\n",
      "Train Loss: 86.2830581665039\n",
      "Average Return: 18.037809371948242\n",
      "\n",
      "Iteration: 470\n",
      "Train Loss: 99.42405700683594\n",
      "Average Return: 3.38581919670105\n",
      "\n",
      "Iteration: 480\n",
      "Train Loss: 60.193241119384766\n",
      "Average Return: 19.38148307800293\n",
      "\n",
      "Iteration: 490\n",
      "Train Loss: 27.265453338623047\n",
      "Average Return: 21.879270553588867\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 48.17691421508789\n",
      "Average Return: 22.754207611083984\n",
      "\n",
      "Iteration: 510\n",
      "Train Loss: 49.3855094909668\n",
      "Average Return: 23.44530487060547\n",
      "\n",
      "Iteration: 520\n",
      "Train Loss: 33.31328201293945\n",
      "Average Return: 23.275842666625977\n",
      "\n",
      "Iteration: 530\n",
      "Train Loss: 123.68376922607422\n",
      "Average Return: 16.166486740112305\n",
      "\n",
      "Iteration: 540\n",
      "Train Loss: 43.29308319091797\n",
      "Average Return: 16.41792869567871\n",
      "\n",
      "Iteration: 550\n",
      "Train Loss: 144.6944580078125\n",
      "Average Return: 19.593488693237305\n",
      "\n",
      "Iteration: 560\n",
      "Train Loss: 17.502845764160156\n",
      "Average Return: 23.458406448364258\n",
      "\n",
      "Iteration: 570\n",
      "Train Loss: 25.668859481811523\n",
      "Average Return: 23.205610275268555\n",
      "\n",
      "Iteration: 580\n",
      "Train Loss: 36.6065673828125\n",
      "Average Return: 22.714311599731445\n",
      "\n",
      "Iteration: 590\n",
      "Train Loss: 68.79574584960938\n",
      "Average Return: 22.856685638427734\n",
      "\n",
      "Iteration: 600\n",
      "Train Loss: 36.69700241088867\n",
      "Average Return: 21.124277114868164\n",
      "\n",
      "Iteration: 610\n",
      "Train Loss: 13.262995719909668\n",
      "Average Return: 18.756683349609375\n",
      "\n",
      "Iteration: 620\n",
      "Train Loss: 18.141307830810547\n",
      "Average Return: 20.34483528137207\n",
      "\n",
      "Iteration: 630\n",
      "Train Loss: 30.81398582458496\n",
      "Average Return: 22.66313934326172\n",
      "\n",
      "Iteration: 640\n",
      "Train Loss: 35.15428924560547\n",
      "Average Return: 20.75568962097168\n",
      "\n",
      "Iteration: 650\n",
      "Train Loss: 27.061504364013672\n",
      "Average Return: 20.462255477905273\n",
      "\n",
      "Iteration: 660\n",
      "Train Loss: 30.451982498168945\n",
      "Average Return: 22.8002986907959\n",
      "\n",
      "Iteration: 670\n",
      "Train Loss: 40.640071868896484\n",
      "Average Return: 21.883380889892578\n",
      "\n",
      "Iteration: 680\n",
      "Train Loss: 16.501239776611328\n",
      "Average Return: 24.03807258605957\n",
      "\n",
      "Iteration: 690\n",
      "Train Loss: 45.179569244384766\n",
      "Average Return: 22.059297561645508\n",
      "\n",
      "Iteration: 700\n",
      "Train Loss: 13.315130233764648\n",
      "Average Return: 18.839317321777344\n",
      "\n",
      "Iteration: 710\n",
      "Train Loss: 13.983436584472656\n",
      "Average Return: 20.52726936340332\n",
      "\n",
      "Iteration: 720\n",
      "Train Loss: 46.4443359375\n",
      "Average Return: 16.557756423950195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kochlis\\Documents\\Research\\TraderNetv2\\metrics\\trading\\sortino.py:20: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.exp(average_returns/std_downfall_returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 730\n",
      "Train Loss: 30.38315773010254\n",
      "Average Return: 20.944732666015625\n",
      "\n",
      "Iteration: 740\n",
      "Train Loss: 26.6074275970459\n",
      "Average Return: 21.806758880615234\n",
      "\n",
      "Iteration: 750\n",
      "Train Loss: 45.98372268676758\n",
      "Average Return: 25.34726905822754\n",
      "\n",
      "Iteration: 760\n",
      "Train Loss: 12.464831352233887\n",
      "Average Return: 24.05818748474121\n",
      "\n",
      "Iteration: 770\n",
      "Train Loss: 23.500608444213867\n",
      "Average Return: 19.314208984375\n",
      "\n",
      "Iteration: 780\n",
      "Train Loss: 14.037674903869629\n",
      "Average Return: 20.93787956237793\n",
      "\n",
      "Iteration: 790\n",
      "Train Loss: 14.23483943939209\n",
      "Average Return: 24.04638671875\n",
      "\n",
      "Iteration: 800\n",
      "Train Loss: 54.20442199707031\n",
      "Average Return: 24.16851806640625\n",
      "\n",
      "Iteration: 810\n",
      "Train Loss: 16.710580825805664\n",
      "Average Return: 23.73822784423828\n",
      "\n",
      "Iteration: 820\n",
      "Train Loss: 39.46760940551758\n",
      "Average Return: 21.864519119262695\n",
      "\n",
      "Iteration: 830\n",
      "Train Loss: 57.25161361694336\n",
      "Average Return: 25.275304794311523\n",
      "\n",
      "Iteration: 840\n",
      "Train Loss: 8.103677749633789\n",
      "Average Return: 23.893707275390625\n",
      "\n",
      "Iteration: 850\n",
      "Train Loss: 28.092546463012695\n",
      "Average Return: 24.21570587158203\n",
      "\n",
      "Iteration: 860\n",
      "Train Loss: 13.447042465209961\n",
      "Average Return: 23.605039596557617\n",
      "\n",
      "Iteration: 870\n",
      "Train Loss: 16.947803497314453\n",
      "Average Return: 24.04161834716797\n",
      "\n",
      "Iteration: 880\n",
      "Train Loss: 6.029308319091797\n",
      "Average Return: 24.87282943725586\n",
      "\n",
      "Iteration: 890\n",
      "Train Loss: 13.185542106628418\n",
      "Average Return: 25.130878448486328\n",
      "\n",
      "Iteration: 900\n",
      "Train Loss: 36.14801025390625\n",
      "Average Return: 25.41843032836914\n",
      "\n",
      "Iteration: 910\n",
      "Train Loss: 12.516639709472656\n",
      "Average Return: 25.142484664916992\n",
      "\n",
      "Iteration: 920\n",
      "Train Loss: 7.550286769866943\n",
      "Average Return: 24.955053329467773\n",
      "\n",
      "Iteration: 930\n",
      "Train Loss: 18.49493980407715\n",
      "Average Return: 25.755231857299805\n",
      "\n",
      "Iteration: 940\n",
      "Train Loss: 5.301024913787842\n",
      "Average Return: 24.83254623413086\n",
      "\n",
      "Iteration: 950\n",
      "Train Loss: 8.783473014831543\n",
      "Average Return: 25.40896987915039\n",
      "\n",
      "Iteration: 960\n",
      "Train Loss: 6.627514839172363\n",
      "Average Return: 26.33321189880371\n",
      "\n",
      "Iteration: 970\n",
      "Train Loss: 12.423726081848145\n",
      "Average Return: 26.224735260009766\n",
      "\n",
      "Iteration: 980\n",
      "Train Loss: 9.128915786743164\n",
      "Average Return: 25.81789207458496\n",
      "\n",
      "Iteration: 990\n",
      "Train Loss: 8.22014045715332\n",
      "Average Return: 25.866750717163086\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 3.605536699295044\n",
      "Average Return: 24.643129348754883\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    'PPO': {dataset_name: {} for dataset_name in datasets_dict.keys()},\n",
    "    'DDQN': {dataset_name: {} for dataset_name in datasets_dict.keys()}\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'BTC': 'green',\n",
    "    'ETH': 'blue',\n",
    "    'XRP': 'red',\n",
    "    'ADA': 'black',\n",
    "    'LTC': 'orange'\n",
    "}\n",
    "linestyles = {\n",
    "    'Market-Orders': '--',\n",
    "    'Market-Limit-Orders': '-'\n",
    "}\n",
    "\n",
    "for agent_name, agent_config in agents_configs.items():\n",
    "    for dataset_name, dataset_filepath in datasets_dict.items():\n",
    "        for reward_fn_name, reward_fn_instance in rewards_dict.items():\n",
    "            tf.random.set_seed(seed=0)\n",
    "\n",
    "            train_params = {\n",
    "                'dataset_filepath': dataset_filepath,\n",
    "                'reward_fn_instance': reward_fn_instance,\n",
    "                'checkpoint_filepath': f'database/storage/checkpoints/experiments/tradernet/{agent_name}/{dataset_name}/{reward_fn_name}/',\n",
    "                **train_dict,\n",
    "                **agent_config\n",
    "            }\n",
    "            eval_avg_returns, eval_metrics = train(**train_params)\n",
    "\n",
    "            results[agent_name][dataset_name][reward_fn_name] = (eval_avg_returns, eval_metrics)\n",
    "\n",
    "        for reward_fn_name, reward_fn_results in results[agent_name][dataset_name].items():\n",
    "            eval_avg_returns, eval_metrics = reward_fn_results\n",
    "\n",
    "            metrics_dict = {\n",
    "                'steps': [10000*i for i in range(len(eval_avg_returns))],\n",
    "                'average_returns': eval_avg_returns,\n",
    "                **{metric.name: metric.episode_metrics for metric in eval_metrics}\n",
    "            }\n",
    "            metrics_df = pd.DataFrame(metrics_dict)\n",
    "            metrics_df.to_csv(f'experiments/tradernet/{agent_name}/{dataset_name}_{reward_fn_name}.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
