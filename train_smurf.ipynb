{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Importing Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import config\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from agents.tfagents.dqn import DQNAgent\n",
    "from agents.tfagents.ppo import PPOAgent\n",
    "from environments.environment import TradingEnvironment\n",
    "from environments.wrappers.tf.tfenv import TFTradingEnvironment\n",
    "from environments.rewards.smurf import SmurfRewardFunction\n",
    "from environments.rewards.marketorder import MarketOrderRF\n",
    "from environments.rewards.marketlimitorder import MarketLimitOrderRF\n",
    "from metrics.trading.pnl import CumulativeLogReturn\n",
    "from metrics.trading.risk import InvestmentRisk\n",
    "from metrics.trading.sharpe import SharpeRatio\n",
    "from metrics.trading.sortino import SortinoRatio\n",
    "from metrics.trading.drawdown import MaximumDrawdown"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reading Datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def read_dataset(\n",
    "        dataset_filepath,\n",
    "        timeframe_size,\n",
    "        target_horizon_len,\n",
    "        num_eval_samples,\n",
    "        fees,\n",
    "        reward_fn_instance\n",
    "):\n",
    "    # Reading dataset\n",
    "    crypto_dataset_df = pd.read_csv(config.dataset_save_filepath.format(dataset_filepath))\n",
    "    samples_df = crypto_dataset_df[config.regression_features]\n",
    "\n",
    "    # Scaling data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1.0))\n",
    "    samples = samples_df.to_numpy(dtype=np.float32)\n",
    "\n",
    "    num_train_scale_samples = samples.shape[0] - num_eval_samples - target_horizon_len - timeframe_size + 1\n",
    "    samples[: num_train_scale_samples] = scaler.fit_transform(samples[: num_train_scale_samples])\n",
    "    samples[num_train_scale_samples: ] = scaler.transform(samples[num_train_scale_samples: ])\n",
    "\n",
    "    # Constructing timeframes for train, test\n",
    "    inputs = np.float32([samples[i: i + timeframe_size] for i in range(samples.shape[0] - timeframe_size - target_horizon_len + 1)])\n",
    "\n",
    "    # Splitting inputs to train-test data\n",
    "    num_train_inputs = inputs.shape[0] - num_eval_samples\n",
    "    x_train = inputs[: num_train_inputs]\n",
    "    x_eval = inputs[num_train_inputs:]\n",
    "\n",
    "    # Computing reward functions for train, test data\n",
    "    closes = crypto_dataset_df['close'].to_numpy(dtype=np.float32)\n",
    "    highs = crypto_dataset_df['high'].to_numpy(dtype=np.float32)\n",
    "    lows = crypto_dataset_df['low'].to_numpy(dtype=np.float32)\n",
    "\n",
    "    train_reward_fn = SmurfRewardFunction(reward_function=reward_fn_instance(\n",
    "        timeframe_size=timeframe_size,\n",
    "        target_horizon_len=target_horizon_len,\n",
    "        highs=highs[: samples.shape[0] - num_eval_samples],\n",
    "        lows=lows[: samples.shape[0] - num_eval_samples],\n",
    "        closes=closes[: samples.shape[0] - num_eval_samples],\n",
    "        fees_percentage=fees\n",
    "    ))\n",
    "\n",
    "    eval_reward_fn = SmurfRewardFunction(reward_function=reward_fn_instance(\n",
    "        timeframe_size=timeframe_size,\n",
    "        target_horizon_len=target_horizon_len,\n",
    "        highs=highs[samples.shape[0] - num_eval_samples - timeframe_size - target_horizon_len + 1:],\n",
    "        lows=lows[samples.shape[0] - num_eval_samples - timeframe_size - target_horizon_len + 1:],\n",
    "        closes=closes[samples.shape[0] - num_eval_samples - timeframe_size - target_horizon_len + 1:],\n",
    "        fees_percentage=fees\n",
    "    ))\n",
    "\n",
    "    assert x_train.shape[0] == train_reward_fn.get_reward_fn_shape()[0], \\\n",
    "        f'AssertionError: DimensionMismatch: x_train: {x_train.shape}, train_reward_fn: {train_reward_fn.get_reward_fn_shape()}'\n",
    "    assert x_eval.shape[0] == eval_reward_fn.get_reward_fn_shape()[0], \\\n",
    "        f'AssertionError: DimensionMismatch: x_eval: {x_eval.shape}, eval_reward_fn: {eval_reward_fn.get_reward_fn_shape()}'\n",
    "\n",
    "    return x_train, train_reward_fn, x_eval, eval_reward_fn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building Agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def build_agent(\n",
    "        agent_instance,\n",
    "        observation_spec,\n",
    "        action_spec,\n",
    "        time_step_spec,\n",
    "        env_batch_size,\n",
    "        checkpoint_filepath,\n",
    "        fc_layers,\n",
    "        conv_layers\n",
    "):\n",
    "    return agent_instance(\n",
    "        input_tensor_spec=observation_spec,\n",
    "        action_spec=action_spec,\n",
    "        time_step_spec=time_step_spec,\n",
    "        env_batch_size=env_batch_size,\n",
    "        checkpoint_filepath=checkpoint_filepath,\n",
    "        fc_layers=fc_layers,\n",
    "        conv_layers=conv_layers\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building Trainer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def train(\n",
    "        dataset_filepath,\n",
    "        timeframe_size,\n",
    "        target_horizon_len,\n",
    "        num_eval_samples,\n",
    "        fees,\n",
    "        reward_fn_instance,\n",
    "        agent_instance,\n",
    "        checkpoint_filepath,\n",
    "        fc_layers,\n",
    "        conv_layers,\n",
    "        train_episode_steps,\n",
    "        train_iterations,\n",
    "        eval_episodes,\n",
    "        steps_per_eval,\n",
    "        steps_per_log,\n",
    "        steps_per_checkpoint,\n",
    "        save_best_only\n",
    "):\n",
    "    x_train, train_reward_fn, x_eval, eval_reward_fn = read_dataset(\n",
    "        dataset_filepath=dataset_filepath,\n",
    "        timeframe_size=timeframe_size,\n",
    "        target_horizon_len=target_horizon_len,\n",
    "        num_eval_samples=num_eval_samples,\n",
    "        fees=fees,\n",
    "        reward_fn_instance=reward_fn_instance\n",
    "    )\n",
    "    train_env = TFTradingEnvironment(\n",
    "        env=TradingEnvironment(env_config={\n",
    "            'states': x_train,\n",
    "            'reward_fn': train_reward_fn,\n",
    "            'episode_steps': train_episode_steps,\n",
    "            'metrics': [CumulativeLogReturn(), InvestmentRisk(), SharpeRatio(), SortinoRatio(), MaximumDrawdown()]\n",
    "        })\n",
    "    )\n",
    "    eval_env = TFTradingEnvironment(\n",
    "        env=TradingEnvironment(env_config={\n",
    "            'states': x_eval,\n",
    "            'reward_fn': eval_reward_fn,\n",
    "            'episode_steps': x_eval.shape[0] - 1,\n",
    "            'metrics': [CumulativeLogReturn(), InvestmentRisk(), SharpeRatio(), SortinoRatio(), MaximumDrawdown()]\n",
    "        })\n",
    "    )\n",
    "\n",
    "    tf_train_env = TFPyEnvironment(environment=train_env)\n",
    "    tf_eval_env = TFPyEnvironment(environment=eval_env)\n",
    "\n",
    "    agent = build_agent(\n",
    "        agent_instance=agent_instance,\n",
    "        observation_spec=tf_train_env.observation_spec(),\n",
    "        action_spec=tf_train_env.action_spec(),\n",
    "        time_step_spec=tf_train_env.time_step_spec(),\n",
    "        env_batch_size=tf_train_env.batch_size,\n",
    "        checkpoint_filepath=checkpoint_filepath,\n",
    "        fc_layers=fc_layers,\n",
    "        conv_layers=conv_layers,\n",
    "    )\n",
    "\n",
    "    agent.initialize()\n",
    "\n",
    "    eval_avg_returns = agent.train(\n",
    "        train_env=tf_train_env,\n",
    "        eval_env=tf_eval_env,\n",
    "        train_iterations=train_iterations,\n",
    "        eval_episodes=eval_episodes,\n",
    "        iterations_per_eval=steps_per_eval,\n",
    "        iterations_per_log=steps_per_log,\n",
    "        iterations_per_checkpoint=steps_per_checkpoint,\n",
    "        save_best_only=save_best_only\n",
    "    )\n",
    "    eval_metrics = eval_env.get_metrics()\n",
    "    return eval_avg_returns, eval_metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building Train Configs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "datasets_dict = {'BTC': 'BTC', 'ETH': 'ETH', 'ADA': 'ADA', 'XRP': 'XRP', 'LTC': 'LTC'}\n",
    "rewards_dict = {\n",
    "    'Market-Orders':  MarketOrderRF,\n",
    "    'Market-Limit Orders': MarketLimitOrderRF\n",
    "}\n",
    "agents_configs = {\n",
    "    'PPO': {\n",
    "        'agent_instance': PPOAgent,\n",
    "        'train_iterations': 1000,\n",
    "        'steps_per_eval': 10,\n",
    "        'steps_per_log': 10,\n",
    "        'steps_per_checkpoint': 10\n",
    "    },\n",
    "    'DDQN': {\n",
    "        'agent_instance': DQNAgent,\n",
    "        'train_iterations': 50000,\n",
    "        'steps_per_eval': 500,\n",
    "        'steps_per_log': 500,\n",
    "        'steps_per_checkpoint': 500\n",
    "    }\n",
    "}\n",
    "train_dict = {\n",
    "    'timeframe_size': 12,\n",
    "    'target_horizon_len': 20,\n",
    "    'num_eval_samples': 2250,\n",
    "    'fees': 0.01,\n",
    "    'fc_layers': [256, 256],\n",
    "    'conv_layers': [(32, 3, 1)],\n",
    "    'train_episode_steps': 100,\n",
    "    'eval_episodes': 1,\n",
    "    'save_best_only': True\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run TraderNet Experiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01853404 -0.00914936  0.0055    ]\n",
      "[-0.00677133 -0.00911128  0.0055    ]\n",
      "Collecting Initial Samples...\n",
      "Training has started...\n",
      "\n",
      "New best average return found at -16.57178497314453! Saving checkpoint at iteration 0\n",
      "WARNING:tensorflow:From C:\\Users\\Kohli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kohli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best average return found at -12.422480583190918! Saving checkpoint at iteration 500\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 0.002164715901017189\n",
      "Average Return: -12.422480583190918\n",
      "\n",
      "New best average return found at -4.974349498748779! Saving checkpoint at iteration 1000\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 0.002368217334151268\n",
      "Average Return: -4.974349498748779\n",
      "\n",
      "Iteration: 1500\n",
      "Train Loss: 0.0034210672602057457\n",
      "Average Return: -11.239537239074707\n",
      "\n",
      "Iteration: 2000\n",
      "Train Loss: 0.0015507049392908812\n",
      "Average Return: -8.947919845581055\n",
      "\n",
      "New best average return found at -2.382216691970825! Saving checkpoint at iteration 2500\n",
      "\n",
      "Iteration: 2500\n",
      "Train Loss: 0.0031417314894497395\n",
      "Average Return: -2.382216691970825\n",
      "\n",
      "New best average return found at 5.06573486328125! Saving checkpoint at iteration 3000\n",
      "\n",
      "Iteration: 3000\n",
      "Train Loss: 0.0056132301688194275\n",
      "Average Return: 5.06573486328125\n",
      "\n",
      "Iteration: 3500\n",
      "Train Loss: 0.009303438477218151\n",
      "Average Return: 2.7069029808044434\n",
      "\n",
      "Iteration: 4000\n",
      "Train Loss: 0.0033092633821070194\n",
      "Average Return: -12.278268814086914\n",
      "\n",
      "New best average return found at 5.760387420654297! Saving checkpoint at iteration 4500\n",
      "\n",
      "Iteration: 4500\n",
      "Train Loss: 0.0024295863695442677\n",
      "Average Return: 5.760387420654297\n",
      "\n",
      "Iteration: 5000\n",
      "Train Loss: 0.0027764951810240746\n",
      "Average Return: -4.662084102630615\n",
      "\n",
      "Iteration: 5500\n",
      "Train Loss: 0.003098664805293083\n",
      "Average Return: -12.274128913879395\n",
      "\n",
      "New best average return found at 11.539819717407227! Saving checkpoint at iteration 6000\n",
      "\n",
      "Iteration: 6000\n",
      "Train Loss: 0.002410882618278265\n",
      "Average Return: 11.539819717407227\n",
      "\n",
      "Iteration: 6500\n",
      "Train Loss: 0.0020708926022052765\n",
      "Average Return: 4.999699592590332\n",
      "\n",
      "Iteration: 7000\n",
      "Train Loss: 0.002373090712353587\n",
      "Average Return: -9.41542911529541\n",
      "\n",
      "Iteration: 7500\n",
      "Train Loss: 0.0016502251382917166\n",
      "Average Return: -0.7375508546829224\n",
      "\n",
      "Iteration: 8000\n",
      "Train Loss: 0.004786387085914612\n",
      "Average Return: -3.052398204803467\n",
      "\n",
      "Iteration: 8500\n",
      "Train Loss: 0.003971721976995468\n",
      "Average Return: 2.226060628890991\n",
      "\n",
      "Iteration: 9000\n",
      "Train Loss: 0.003403043607249856\n",
      "Average Return: 0.7624480724334717\n",
      "\n",
      "Iteration: 9500\n",
      "Train Loss: 0.006974234711378813\n",
      "Average Return: 2.8173317909240723\n",
      "\n",
      "Iteration: 10000\n",
      "Train Loss: 0.00297953630797565\n",
      "Average Return: 5.1815571784973145\n",
      "\n",
      "Iteration: 10500\n",
      "Train Loss: 0.003787008812651038\n",
      "Average Return: 10.342925071716309\n",
      "\n",
      "Iteration: 11000\n",
      "Train Loss: 0.004083265084773302\n",
      "Average Return: 1.430643916130066\n",
      "\n",
      "Iteration: 11500\n",
      "Train Loss: 0.0028911482077091932\n",
      "Average Return: -0.32036224007606506\n",
      "\n",
      "Iteration: 12000\n",
      "Train Loss: 0.004029873758554459\n",
      "Average Return: -3.6935513019561768\n",
      "\n",
      "New best average return found at 11.949156761169434! Saving checkpoint at iteration 12500\n",
      "\n",
      "Iteration: 12500\n",
      "Train Loss: 0.004942761734127998\n",
      "Average Return: 11.949156761169434\n",
      "\n",
      "Iteration: 13000\n",
      "Train Loss: 0.005790471564978361\n",
      "Average Return: -3.1711723804473877\n",
      "\n",
      "Iteration: 13500\n",
      "Train Loss: 0.00321864802390337\n",
      "Average Return: -0.44034838676452637\n",
      "\n",
      "Iteration: 14000\n",
      "Train Loss: 0.004941555671393871\n",
      "Average Return: -6.891396999359131\n",
      "\n",
      "Iteration: 14500\n",
      "Train Loss: 0.004726107232272625\n",
      "Average Return: -4.979492664337158\n",
      "\n",
      "Iteration: 15000\n",
      "Train Loss: 0.006596008315682411\n",
      "Average Return: 3.4390671253204346\n",
      "\n",
      "Iteration: 15500\n",
      "Train Loss: 0.0033213263377547264\n",
      "Average Return: -1.0558987855911255\n",
      "\n",
      "Iteration: 16000\n",
      "Train Loss: 0.003437688574194908\n",
      "Average Return: 6.152379035949707\n",
      "\n",
      "Iteration: 16500\n",
      "Train Loss: 0.004672907758504152\n",
      "Average Return: 5.143002033233643\n",
      "\n",
      "Iteration: 17000\n",
      "Train Loss: 0.0037238553632050753\n",
      "Average Return: 2.492591381072998\n",
      "\n",
      "Iteration: 17500\n",
      "Train Loss: 0.005180912092328072\n",
      "Average Return: 2.6381523609161377\n",
      "\n",
      "Iteration: 18000\n",
      "Train Loss: 0.004384171217679977\n",
      "Average Return: 2.1685006618499756\n",
      "\n",
      "Iteration: 18500\n",
      "Train Loss: 0.023530924692749977\n",
      "Average Return: 6.757209777832031\n",
      "\n",
      "Iteration: 19000\n",
      "Train Loss: 0.00694462051615119\n",
      "Average Return: 5.053895473480225\n",
      "\n",
      "Iteration: 19500\n",
      "Train Loss: 0.007051569409668446\n",
      "Average Return: 0.15808336436748505\n",
      "\n",
      "Iteration: 20000\n",
      "Train Loss: 0.006961312144994736\n",
      "Average Return: 4.23157262802124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kohli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\Kohli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "C:\\Users\\Kohli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 20500\n",
      "Train Loss: 0.006115785799920559\n",
      "Average Return: 0.6281086802482605\n",
      "\n",
      "Iteration: 21000\n",
      "Train Loss: 0.005628121551126242\n",
      "Average Return: -4.084413051605225\n",
      "\n",
      "Iteration: 21500\n",
      "Train Loss: 0.0035359784960746765\n",
      "Average Return: 7.508200168609619\n",
      "\n",
      "Iteration: 22000\n",
      "Train Loss: 0.004787422716617584\n",
      "Average Return: 3.374601364135742\n",
      "\n",
      "Iteration: 22500\n",
      "Train Loss: 0.016201427206397057\n",
      "Average Return: 2.8670308589935303\n",
      "\n",
      "Iteration: 23000\n",
      "Train Loss: 0.010247232392430305\n",
      "Average Return: -2.7473440170288086\n",
      "\n",
      "Iteration: 23500\n",
      "Train Loss: 0.013739490881562233\n",
      "Average Return: 8.578664779663086\n",
      "\n",
      "Iteration: 24000\n",
      "Train Loss: 0.012315332889556885\n",
      "Average Return: -0.26625263690948486\n",
      "\n",
      "Iteration: 24500\n",
      "Train Loss: 0.016522977501153946\n",
      "Average Return: -1.3087804317474365\n",
      "\n",
      "Iteration: 25000\n",
      "Train Loss: 0.0012990799732506275\n",
      "Average Return: 3.111482858657837\n",
      "\n",
      "Iteration: 25500\n",
      "Train Loss: 0.013929511420428753\n",
      "Average Return: 0.8290021419525146\n",
      "\n",
      "Iteration: 26000\n",
      "Train Loss: 0.007917647249996662\n",
      "Average Return: -3.5122430324554443\n",
      "\n",
      "Iteration: 26500\n",
      "Train Loss: 0.004368746653199196\n",
      "Average Return: -3.259702682495117\n",
      "\n",
      "Iteration: 27000\n",
      "Train Loss: 0.004065701737999916\n",
      "Average Return: -6.532232761383057\n",
      "\n",
      "Iteration: 27500\n",
      "Train Loss: 0.01486295461654663\n",
      "Average Return: -0.37913039326667786\n",
      "\n",
      "Iteration: 28000\n",
      "Train Loss: 0.003208391834050417\n",
      "Average Return: -10.557829856872559\n",
      "\n",
      "Iteration: 28500\n",
      "Train Loss: 0.007768794894218445\n",
      "Average Return: 1.5561491250991821\n",
      "\n",
      "Iteration: 29000\n",
      "Train Loss: 0.010547632351517677\n",
      "Average Return: -2.2193665504455566\n",
      "\n",
      "Iteration: 29500\n",
      "Train Loss: 0.010362663306295872\n",
      "Average Return: 5.795537948608398\n",
      "\n",
      "Iteration: 30000\n",
      "Train Loss: 0.003202661871910095\n",
      "Average Return: 4.87795877456665\n",
      "\n",
      "Iteration: 30500\n",
      "Train Loss: 0.005761319305747747\n",
      "Average Return: 7.364535808563232\n",
      "\n",
      "Iteration: 31000\n",
      "Train Loss: 0.00477150222286582\n",
      "Average Return: -10.88282299041748\n",
      "\n",
      "Iteration: 31500\n",
      "Train Loss: 0.0034152651205658913\n",
      "Average Return: -3.8864870071411133\n",
      "\n",
      "Iteration: 32000\n",
      "Train Loss: 0.008756299503147602\n",
      "Average Return: 3.793571710586548\n",
      "\n",
      "Iteration: 32500\n",
      "Train Loss: 0.003229961497709155\n",
      "Average Return: 0.5818710327148438\n",
      "\n",
      "Iteration: 33000\n",
      "Train Loss: 0.009854866191744804\n",
      "Average Return: -0.7472620010375977\n",
      "\n",
      "Iteration: 33500\n",
      "Train Loss: 0.007232993375509977\n",
      "Average Return: 2.6342339515686035\n",
      "\n",
      "Iteration: 34000\n",
      "Train Loss: 0.004407609812915325\n",
      "Average Return: 0.0262022502720356\n",
      "\n",
      "Iteration: 34500\n",
      "Train Loss: 0.005405030213296413\n",
      "Average Return: -6.492897987365723\n",
      "\n",
      "Iteration: 35000\n",
      "Train Loss: 0.010715429671108723\n",
      "Average Return: 1.255590796470642\n",
      "\n",
      "Iteration: 35500\n",
      "Train Loss: 0.0058011338114738464\n",
      "Average Return: -0.20525683462619781\n",
      "\n",
      "Iteration: 36000\n",
      "Train Loss: 0.00957681704312563\n",
      "Average Return: -7.558910846710205\n",
      "\n",
      "Iteration: 36500\n",
      "Train Loss: 0.0068952664732933044\n",
      "Average Return: -0.7380999326705933\n",
      "\n",
      "Iteration: 37000\n",
      "Train Loss: 0.007393331732600927\n",
      "Average Return: -0.646045446395874\n",
      "\n",
      "Iteration: 37500\n",
      "Train Loss: 0.004453663248568773\n",
      "Average Return: -10.332388877868652\n",
      "\n",
      "Iteration: 38000\n",
      "Train Loss: 0.007086480967700481\n",
      "Average Return: -0.27467894554138184\n",
      "\n",
      "Iteration: 38500\n",
      "Train Loss: 0.010243586264550686\n",
      "Average Return: -4.139746189117432\n",
      "\n",
      "Iteration: 39000\n",
      "Train Loss: 0.014536739327013493\n",
      "Average Return: -2.8191823959350586\n",
      "\n",
      "Iteration: 39500\n",
      "Train Loss: 0.013886336237192154\n",
      "Average Return: -1.7967926263809204\n",
      "\n",
      "Iteration: 40000\n",
      "Train Loss: 0.009449003264307976\n",
      "Average Return: 5.626032829284668\n",
      "\n",
      "Iteration: 40500\n",
      "Train Loss: 0.009974634274840355\n",
      "Average Return: 3.409313440322876\n",
      "\n",
      "Iteration: 41000\n",
      "Train Loss: 0.003953715320676565\n",
      "Average Return: 0.009019054472446442\n",
      "\n",
      "Iteration: 41500\n",
      "Train Loss: 0.008728137239813805\n",
      "Average Return: -1.6554069519042969\n",
      "\n",
      "Iteration: 42000\n",
      "Train Loss: 0.0037142056971788406\n",
      "Average Return: -4.711195468902588\n",
      "\n",
      "Iteration: 42500\n",
      "Train Loss: 0.01386098563671112\n",
      "Average Return: 8.002055168151855\n",
      "\n",
      "Iteration: 43000\n",
      "Train Loss: 0.007242920808494091\n",
      "Average Return: -2.6537630558013916\n",
      "\n",
      "Iteration: 43500\n",
      "Train Loss: 0.01006107497960329\n",
      "Average Return: 0.7477589845657349\n",
      "\n",
      "Iteration: 44000\n",
      "Train Loss: 0.006491910666227341\n",
      "Average Return: -3.4243438243865967\n",
      "\n",
      "Iteration: 44500\n",
      "Train Loss: 0.008208999410271645\n",
      "Average Return: -2.9506635665893555\n",
      "\n",
      "Iteration: 45000\n",
      "Train Loss: 0.013566250912845135\n",
      "Average Return: -1.7382171154022217\n",
      "\n",
      "Iteration: 45500\n",
      "Train Loss: 0.007833427749574184\n",
      "Average Return: -2.4994125366210938\n",
      "\n",
      "Iteration: 46000\n",
      "Train Loss: 0.013270098716020584\n",
      "Average Return: 6.984377861022949\n",
      "\n",
      "Iteration: 46500\n",
      "Train Loss: 0.011646009981632233\n",
      "Average Return: 2.870522975921631\n",
      "\n",
      "Iteration: 47000\n",
      "Train Loss: 0.02422204241156578\n",
      "Average Return: 0.83193039894104\n",
      "\n",
      "Iteration: 47500\n",
      "Train Loss: 0.019133271649479866\n",
      "Average Return: -2.9962451457977295\n",
      "\n",
      "Iteration: 48000\n",
      "Train Loss: 0.0060126809403300285\n",
      "Average Return: -5.956732273101807\n",
      "\n",
      "Iteration: 48500\n",
      "Train Loss: 0.009661473333835602\n",
      "Average Return: -8.856889724731445\n",
      "\n",
      "Iteration: 49000\n",
      "Train Loss: 0.008308571763336658\n",
      "Average Return: -5.23563289642334\n",
      "\n",
      "Iteration: 49500\n",
      "Train Loss: 0.017365533858537674\n",
      "Average Return: -5.019726753234863\n",
      "\n",
      "Iteration: 50000\n",
      "Train Loss: 0.007180715445429087\n",
      "Average Return: -4.962328910827637\n",
      "[-0.00968806  0.00177081  0.0055    ]\n",
      "[ 0.00140647 -0.00601929  0.0055    ]\n",
      "Collecting Initial Samples...\n",
      "Training has started...\n",
      "\n",
      "New best average return found at -5.9268693923950195! Saving checkpoint at iteration 0\n",
      "\n",
      "New best average return found at -5.436023712158203! Saving checkpoint at iteration 500\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 0.0020553329959511757\n",
      "Average Return: -5.436023712158203\n",
      "\n",
      "New best average return found at -1.1352100372314453! Saving checkpoint at iteration 1000\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 0.0038205296732485294\n",
      "Average Return: -1.1352100372314453\n",
      "\n",
      "Iteration: 1500\n",
      "Train Loss: 0.006010392680764198\n",
      "Average Return: -1.7079248428344727\n",
      "\n",
      "Iteration: 2000\n",
      "Train Loss: 0.0034790891222655773\n",
      "Average Return: -5.412572860717773\n",
      "\n",
      "Iteration: 2500\n",
      "Train Loss: 0.009995092637836933\n",
      "Average Return: -2.533175230026245\n",
      "\n",
      "New best average return found at 0.6796287894248962! Saving checkpoint at iteration 3000\n",
      "\n",
      "Iteration: 3000\n",
      "Train Loss: 0.01402287557721138\n",
      "Average Return: 0.6796287894248962\n",
      "\n",
      "New best average return found at 6.855253219604492! Saving checkpoint at iteration 3500\n",
      "\n",
      "Iteration: 3500\n",
      "Train Loss: 0.017647448927164078\n",
      "Average Return: 6.855253219604492\n",
      "\n",
      "Iteration: 4000\n",
      "Train Loss: 0.00911051407456398\n",
      "Average Return: -1.2091209888458252\n",
      "\n",
      "Iteration: 4500\n",
      "Train Loss: 0.007775294128805399\n",
      "Average Return: 2.0348870754241943\n",
      "\n",
      "Iteration: 5000\n",
      "Train Loss: 0.0077013541013002396\n",
      "Average Return: -4.767042636871338\n",
      "\n",
      "Iteration: 5500\n",
      "Train Loss: 0.007346814032644033\n",
      "Average Return: -2.4500036239624023\n",
      "\n",
      "Iteration: 6000\n",
      "Train Loss: 0.005798700265586376\n",
      "Average Return: 1.2739514112472534\n",
      "\n",
      "New best average return found at 10.084075927734375! Saving checkpoint at iteration 6500\n",
      "\n",
      "Iteration: 6500\n",
      "Train Loss: 0.010724336840212345\n",
      "Average Return: 10.084075927734375\n",
      "\n",
      "Iteration: 7000\n",
      "Train Loss: 0.005179033614695072\n",
      "Average Return: -1.1661349534988403\n",
      "\n",
      "Iteration: 7500\n",
      "Train Loss: 0.0039992667734622955\n",
      "Average Return: -0.9196488857269287\n",
      "\n",
      "Iteration: 8000\n",
      "Train Loss: 0.014901691116392612\n",
      "Average Return: 3.364877700805664\n",
      "\n",
      "Iteration: 8500\n",
      "Train Loss: 0.01628170907497406\n",
      "Average Return: 4.941080570220947\n",
      "\n",
      "Iteration: 9000\n",
      "Train Loss: 0.0059967744164168835\n",
      "Average Return: 3.5586321353912354\n",
      "\n",
      "Iteration: 9500\n",
      "Train Loss: 0.010703048668801785\n",
      "Average Return: 0.7702343463897705\n",
      "\n",
      "Iteration: 10000\n",
      "Train Loss: 0.009241319261491299\n",
      "Average Return: -4.0143561363220215\n",
      "\n",
      "New best average return found at 11.089746475219727! Saving checkpoint at iteration 10500\n",
      "\n",
      "Iteration: 10500\n",
      "Train Loss: 0.009976230561733246\n",
      "Average Return: 11.089746475219727\n",
      "\n",
      "Iteration: 11000\n",
      "Train Loss: 0.01669584959745407\n",
      "Average Return: -3.5330147743225098\n",
      "\n",
      "Iteration: 11500\n",
      "Train Loss: 0.008532716892659664\n",
      "Average Return: -2.6470394134521484\n",
      "\n",
      "Iteration: 12000\n",
      "Train Loss: 0.00944591872394085\n",
      "Average Return: 6.174392223358154\n",
      "\n",
      "Iteration: 12500\n",
      "Train Loss: 0.008416345342993736\n",
      "Average Return: 5.5731611251831055\n",
      "\n",
      "Iteration: 13000\n",
      "Train Loss: 0.016640320420265198\n",
      "Average Return: -2.434685230255127\n",
      "\n",
      "Iteration: 13500\n",
      "Train Loss: 0.011606577783823013\n",
      "Average Return: 3.178175449371338\n",
      "\n",
      "Iteration: 14000\n",
      "Train Loss: 0.018309911713004112\n",
      "Average Return: 7.282590389251709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kohli\\Desktop\\TraderNetv2\\metrics\\trading\\sortino.py:20: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.exp(average_returns/std_downfall_returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 14500\n",
      "Train Loss: 0.012537236325442791\n",
      "Average Return: -0.3190191984176636\n",
      "\n",
      "Iteration: 15000\n",
      "Train Loss: 0.023972511291503906\n",
      "Average Return: 5.57395601272583\n",
      "\n",
      "Iteration: 15500\n",
      "Train Loss: 0.011063171550631523\n",
      "Average Return: 9.120756149291992\n",
      "\n",
      "Iteration: 16000\n",
      "Train Loss: 0.012248865328729153\n",
      "Average Return: -4.242620468139648\n",
      "\n",
      "Iteration: 16500\n",
      "Train Loss: 0.01479065790772438\n",
      "Average Return: 2.4166364669799805\n",
      "\n",
      "Iteration: 17000\n",
      "Train Loss: 0.022776387631893158\n",
      "Average Return: 9.479940414428711\n",
      "\n",
      "Iteration: 17500\n",
      "Train Loss: 0.018192529678344727\n",
      "Average Return: 6.183734893798828\n",
      "\n",
      "Iteration: 18000\n",
      "Train Loss: 0.011010328307747841\n",
      "Average Return: 3.182417392730713\n",
      "\n",
      "Iteration: 18500\n",
      "Train Loss: 0.012737394310534\n",
      "Average Return: 8.657953262329102\n",
      "\n",
      "Iteration: 19000\n",
      "Train Loss: 0.0435476154088974\n",
      "Average Return: 4.152158260345459\n",
      "\n",
      "Iteration: 19500\n",
      "Train Loss: 0.02893104963004589\n",
      "Average Return: 5.033951759338379\n",
      "\n",
      "Iteration: 20000\n",
      "Train Loss: 0.025625063106417656\n",
      "Average Return: 5.325740337371826\n",
      "\n",
      "Iteration: 20500\n",
      "Train Loss: 0.012886586599051952\n",
      "Average Return: 4.3015031814575195\n",
      "\n",
      "New best average return found at 11.15096378326416! Saving checkpoint at iteration 21000\n",
      "\n",
      "Iteration: 21000\n",
      "Train Loss: 0.016114527359604836\n",
      "Average Return: 11.15096378326416\n",
      "\n",
      "Iteration: 21500\n",
      "Train Loss: 0.01807141676545143\n",
      "Average Return: 6.203341484069824\n",
      "\n",
      "Iteration: 22000\n",
      "Train Loss: 0.013196973130106926\n",
      "Average Return: 2.704026937484741\n",
      "\n",
      "Iteration: 22500\n",
      "Train Loss: 0.03391696885228157\n",
      "Average Return: 6.8334431648254395\n",
      "\n",
      "Iteration: 23000\n",
      "Train Loss: 0.02478557825088501\n",
      "Average Return: -2.1761388778686523\n",
      "\n",
      "Iteration: 23500\n",
      "Train Loss: 0.0351107232272625\n",
      "Average Return: 4.37005615234375\n",
      "\n",
      "Iteration: 24000\n",
      "Train Loss: 0.032876014709472656\n",
      "Average Return: -0.2962328791618347\n",
      "\n",
      "Iteration: 24500\n",
      "Train Loss: 0.03392776474356651\n",
      "Average Return: 6.153800010681152\n",
      "\n",
      "Iteration: 25000\n",
      "Train Loss: 0.0055491263046860695\n",
      "Average Return: -0.32113125920295715\n",
      "\n",
      "Iteration: 25500\n",
      "Train Loss: 0.011452730745077133\n",
      "Average Return: 0.7165119647979736\n",
      "\n",
      "Iteration: 26000\n",
      "Train Loss: 0.031356051564216614\n",
      "Average Return: 2.606886863708496\n",
      "\n",
      "Iteration: 26500\n",
      "Train Loss: 0.0158749520778656\n",
      "Average Return: 2.144186496734619\n",
      "\n",
      "Iteration: 27000\n",
      "Train Loss: 0.008590595796704292\n",
      "Average Return: -3.6226608753204346\n",
      "\n",
      "Iteration: 27500\n",
      "Train Loss: 0.04265506938099861\n",
      "Average Return: 1.201648235321045\n",
      "\n",
      "Iteration: 28000\n",
      "Train Loss: 0.009518655948340893\n",
      "Average Return: -2.2092342376708984\n",
      "\n",
      "Iteration: 28500\n",
      "Train Loss: 0.010392975993454456\n",
      "Average Return: 6.5969343185424805\n",
      "\n",
      "Iteration: 29000\n",
      "Train Loss: 0.02806476317346096\n",
      "Average Return: 0.086337611079216\n",
      "\n",
      "Iteration: 29500\n",
      "Train Loss: 0.015801606699824333\n",
      "Average Return: 2.3691036701202393\n",
      "\n",
      "Iteration: 30000\n",
      "Train Loss: 0.008984885178506374\n",
      "Average Return: 0.9773356318473816\n",
      "\n",
      "Iteration: 30500\n",
      "Train Loss: 0.031616609543561935\n",
      "Average Return: -0.2123214602470398\n",
      "\n",
      "Iteration: 31000\n",
      "Train Loss: 0.018689416348934174\n",
      "Average Return: 1.853790521621704\n",
      "\n",
      "Iteration: 31500\n",
      "Train Loss: 0.018324779346585274\n",
      "Average Return: 1.7832478284835815\n",
      "\n",
      "Iteration: 32000\n",
      "Train Loss: 0.01679924875497818\n",
      "Average Return: 5.882299900054932\n",
      "\n",
      "Iteration: 32500\n",
      "Train Loss: 0.006788438186049461\n",
      "Average Return: 3.4342825412750244\n",
      "\n",
      "Iteration: 33000\n",
      "Train Loss: 0.032223332673311234\n",
      "Average Return: 1.8264187574386597\n",
      "\n",
      "Iteration: 33500\n",
      "Train Loss: 0.015736697241663933\n",
      "Average Return: -0.31569913029670715\n",
      "\n",
      "Iteration: 34000\n",
      "Train Loss: 0.014585158787667751\n",
      "Average Return: 2.8643572330474854\n",
      "\n",
      "Iteration: 34500\n",
      "Train Loss: 0.020306937396526337\n",
      "Average Return: 1.2103893756866455\n",
      "\n",
      "Iteration: 35000\n",
      "Train Loss: 0.03529420495033264\n",
      "Average Return: 0.7160492539405823\n",
      "\n",
      "Iteration: 35500\n",
      "Train Loss: 0.036890603601932526\n",
      "Average Return: 3.316439390182495\n",
      "\n",
      "Iteration: 36000\n",
      "Train Loss: 0.03191016986966133\n",
      "Average Return: 2.8246548175811768\n",
      "\n",
      "Iteration: 36500\n",
      "Train Loss: 0.012770310044288635\n",
      "Average Return: 3.062868118286133\n",
      "\n",
      "Iteration: 37000\n",
      "Train Loss: 0.04248304292559624\n",
      "Average Return: 0.8371360301971436\n",
      "\n",
      "Iteration: 37500\n",
      "Train Loss: 0.015936164185404778\n",
      "Average Return: -0.48102006316185\n",
      "\n",
      "Iteration: 38000\n",
      "Train Loss: 0.023629002273082733\n",
      "Average Return: 0.4548693299293518\n",
      "\n",
      "Iteration: 38500\n",
      "Train Loss: 0.02435385249555111\n",
      "Average Return: 3.3657498359680176\n",
      "\n",
      "Iteration: 39000\n",
      "Train Loss: 0.058170922100543976\n",
      "Average Return: 4.834799289703369\n",
      "\n",
      "Iteration: 39500\n",
      "Train Loss: 0.04511892795562744\n",
      "Average Return: 3.036614418029785\n",
      "\n",
      "Iteration: 40000\n",
      "Train Loss: 0.02712244912981987\n",
      "Average Return: 4.845786094665527\n",
      "\n",
      "Iteration: 40500\n",
      "Train Loss: 0.031348034739494324\n",
      "Average Return: 0.7574614882469177\n",
      "\n",
      "Iteration: 41000\n",
      "Train Loss: 0.022932136431336403\n",
      "Average Return: 2.109562635421753\n",
      "\n",
      "Iteration: 41500\n",
      "Train Loss: 0.02641591802239418\n",
      "Average Return: 2.741323471069336\n",
      "\n",
      "Iteration: 42000\n",
      "Train Loss: 0.009781111031770706\n",
      "Average Return: 3.487088441848755\n",
      "\n",
      "Iteration: 42500\n",
      "Train Loss: 0.03175143525004387\n",
      "Average Return: 4.150259494781494\n",
      "\n",
      "Iteration: 43000\n",
      "Train Loss: 0.023486215621232986\n",
      "Average Return: 0.5345873832702637\n",
      "\n",
      "Iteration: 43500\n",
      "Train Loss: 0.04770593345165253\n",
      "Average Return: 1.7698101997375488\n",
      "\n",
      "Iteration: 44000\n",
      "Train Loss: 0.02072863280773163\n",
      "Average Return: 2.759277105331421\n",
      "\n",
      "Iteration: 44500\n",
      "Train Loss: 0.02858809381723404\n",
      "Average Return: 3.8290042877197266\n",
      "\n",
      "Iteration: 45000\n",
      "Train Loss: 0.036087099462747574\n",
      "Average Return: 2.222339391708374\n",
      "\n",
      "Iteration: 45500\n",
      "Train Loss: 0.0264056995511055\n",
      "Average Return: 4.55873441696167\n",
      "\n",
      "Iteration: 46000\n",
      "Train Loss: 0.03793987259268761\n",
      "Average Return: 2.2758524417877197\n",
      "\n",
      "Iteration: 46500\n",
      "Train Loss: 0.03000415302813053\n",
      "Average Return: 3.4877889156341553\n",
      "\n",
      "Iteration: 47000\n",
      "Train Loss: 0.017613861709833145\n",
      "Average Return: 2.1294608116149902\n",
      "\n",
      "Iteration: 47500\n",
      "Train Loss: 0.016205010935664177\n",
      "Average Return: 2.667348861694336\n",
      "\n",
      "Iteration: 48000\n",
      "Train Loss: 0.012568604201078415\n",
      "Average Return: 1.3087201118469238\n",
      "\n",
      "Iteration: 48500\n",
      "Train Loss: 0.024864470586180687\n",
      "Average Return: -0.9251583218574524\n",
      "\n",
      "Iteration: 49000\n",
      "Train Loss: 0.019656822085380554\n",
      "Average Return: 2.6435534954071045\n",
      "\n",
      "Iteration: 49500\n",
      "Train Loss: 0.027987362816929817\n",
      "Average Return: 0.5318158268928528\n",
      "\n",
      "Iteration: 50000\n",
      "Train Loss: 0.030062055215239525\n",
      "Average Return: 1.0375735759735107\n",
      "[ 0.08750807 -0.01160233  0.0055    ]\n",
      "[-0.01997623  0.01921783  0.0055    ]\n",
      "Collecting Initial Samples...\n",
      "Training has started...\n",
      "\n",
      "New best average return found at -0.9504973888397217! Saving checkpoint at iteration 0\n",
      "\n",
      "New best average return found at 4.570018291473389! Saving checkpoint at iteration 500\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 0.0028148535639047623\n",
      "Average Return: 4.570018291473389\n",
      "\n",
      "New best average return found at 5.421781539916992! Saving checkpoint at iteration 1000\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 0.003019496565684676\n",
      "Average Return: 5.421781539916992\n",
      "\n",
      "Iteration: 1500\n",
      "Train Loss: 0.0029176727402955294\n",
      "Average Return: 2.983503580093384\n",
      "\n",
      "Iteration: 2000\n",
      "Train Loss: 0.002395953517407179\n",
      "Average Return: 3.499786138534546\n",
      "\n",
      "Iteration: 2500\n",
      "Train Loss: 0.003185564186424017\n",
      "Average Return: 2.0775277614593506\n",
      "\n",
      "Iteration: 3000\n",
      "Train Loss: 0.006221956107765436\n",
      "Average Return: 2.120728015899658\n",
      "\n",
      "New best average return found at 6.458415508270264! Saving checkpoint at iteration 3500\n",
      "\n",
      "Iteration: 3500\n",
      "Train Loss: 0.010128811933100224\n",
      "Average Return: 6.458415508270264\n",
      "\n",
      "Iteration: 4000\n",
      "Train Loss: 0.008070899173617363\n",
      "Average Return: 3.7011308670043945\n",
      "\n",
      "New best average return found at 7.949519157409668! Saving checkpoint at iteration 4500\n",
      "\n",
      "Iteration: 4500\n",
      "Train Loss: 0.008184975013136864\n",
      "Average Return: 7.949519157409668\n",
      "\n",
      "Iteration: 5000\n",
      "Train Loss: 0.008588350377976894\n",
      "Average Return: 4.364499092102051\n",
      "\n",
      "Iteration: 5500\n",
      "Train Loss: 0.009943120181560516\n",
      "Average Return: 2.380652904510498\n",
      "\n",
      "Iteration: 6000\n",
      "Train Loss: 0.005753056611865759\n",
      "Average Return: 4.0820770263671875\n",
      "\n",
      "Iteration: 6500\n",
      "Train Loss: 0.015285473316907883\n",
      "Average Return: 3.485297918319702\n",
      "\n",
      "Iteration: 7000\n",
      "Train Loss: 0.004991193301975727\n",
      "Average Return: 5.72343635559082\n",
      "\n",
      "Iteration: 7500\n",
      "Train Loss: 0.0041401926428079605\n",
      "Average Return: 7.44674015045166\n",
      "\n",
      "Iteration: 8000\n",
      "Train Loss: 0.007664819713681936\n",
      "Average Return: 6.715669631958008\n",
      "\n",
      "Iteration: 8500\n",
      "Train Loss: 0.011235936544835567\n",
      "Average Return: 5.474985599517822\n",
      "\n",
      "Iteration: 9000\n",
      "Train Loss: 0.0065916432067751884\n",
      "Average Return: 6.527674198150635\n",
      "\n",
      "Iteration: 9500\n",
      "Train Loss: 0.015144947916269302\n",
      "Average Return: 4.391909599304199\n",
      "\n",
      "Iteration: 10000\n",
      "Train Loss: 0.007338142953813076\n",
      "Average Return: 3.3686583042144775\n",
      "\n",
      "Iteration: 10500\n",
      "Train Loss: 0.006249490659683943\n",
      "Average Return: 4.034685134887695\n",
      "\n",
      "Iteration: 11000\n",
      "Train Loss: 0.010268236510455608\n",
      "Average Return: 5.9450507164001465\n",
      "\n",
      "Iteration: 11500\n",
      "Train Loss: 0.007256801705807447\n",
      "Average Return: 2.4723868370056152\n",
      "\n",
      "Iteration: 12000\n",
      "Train Loss: 0.008690997958183289\n",
      "Average Return: 4.402858734130859\n",
      "\n",
      "Iteration: 12500\n",
      "Train Loss: 0.005852785427123308\n",
      "Average Return: 3.8081345558166504\n",
      "\n",
      "Iteration: 13000\n",
      "Train Loss: 0.005059707909822464\n",
      "Average Return: 6.124772548675537\n",
      "\n",
      "Iteration: 13500\n",
      "Train Loss: 0.006821316201239824\n",
      "Average Return: 2.5626728534698486\n",
      "\n",
      "Iteration: 14000\n",
      "Train Loss: 0.011918393895030022\n",
      "Average Return: 0.5070582032203674\n",
      "\n",
      "Iteration: 14500\n",
      "Train Loss: 0.007753715384751558\n",
      "Average Return: 3.0303220748901367\n",
      "\n",
      "Iteration: 15000\n",
      "Train Loss: 0.009620064869523048\n",
      "Average Return: 7.133653163909912\n",
      "\n",
      "Iteration: 15500\n",
      "Train Loss: 0.01414664275944233\n",
      "Average Return: 6.503829002380371\n",
      "\n",
      "Iteration: 16000\n",
      "Train Loss: 0.010534863919019699\n",
      "Average Return: 5.072902202606201\n",
      "\n",
      "Iteration: 16500\n",
      "Train Loss: 0.012086039409041405\n",
      "Average Return: 3.542565107345581\n",
      "\n",
      "Iteration: 17000\n",
      "Train Loss: 0.015993574634194374\n",
      "Average Return: 1.848357081413269\n",
      "\n",
      "Iteration: 17500\n",
      "Train Loss: 0.009522766806185246\n",
      "Average Return: 0.4015689790248871\n",
      "\n",
      "Iteration: 18000\n",
      "Train Loss: 0.008195070549845695\n",
      "Average Return: 2.5151443481445312\n",
      "\n",
      "Iteration: 18500\n",
      "Train Loss: 0.025938572362065315\n",
      "Average Return: 2.1407182216644287\n",
      "\n",
      "Iteration: 19000\n",
      "Train Loss: 0.01255276519805193\n",
      "Average Return: 3.320664405822754\n",
      "\n",
      "Iteration: 19500\n",
      "Train Loss: 0.048549894243478775\n",
      "Average Return: 2.569389581680298\n",
      "\n",
      "Iteration: 20000\n",
      "Train Loss: 0.016100430861115456\n",
      "Average Return: 1.9640401601791382\n",
      "\n",
      "Iteration: 20500\n",
      "Train Loss: 0.008514361456036568\n",
      "Average Return: 2.4316329956054688\n",
      "\n",
      "Iteration: 21000\n",
      "Train Loss: 0.011230381205677986\n",
      "Average Return: 2.055717945098877\n",
      "\n",
      "Iteration: 21500\n",
      "Train Loss: 0.012477030046284199\n",
      "Average Return: 3.1448042392730713\n",
      "\n",
      "Iteration: 22000\n",
      "Train Loss: 0.01066042110323906\n",
      "Average Return: 0.6615906953811646\n",
      "\n",
      "Iteration: 22500\n",
      "Train Loss: 0.011275631375610828\n",
      "Average Return: 4.306146621704102\n",
      "\n",
      "Iteration: 23000\n",
      "Train Loss: 0.00982233602553606\n",
      "Average Return: 2.1957454681396484\n",
      "\n",
      "Iteration: 23500\n",
      "Train Loss: 0.017256082966923714\n",
      "Average Return: 3.4608547687530518\n",
      "\n",
      "Iteration: 24000\n",
      "Train Loss: 0.020297633484005928\n",
      "Average Return: 3.3291754722595215\n",
      "\n",
      "Iteration: 24500\n",
      "Train Loss: 0.027099644765257835\n",
      "Average Return: 5.8309326171875\n",
      "\n",
      "Iteration: 25000\n",
      "Train Loss: 0.013920784927904606\n",
      "Average Return: 6.971850395202637\n",
      "\n",
      "Iteration: 25500\n",
      "Train Loss: 0.019587615504860878\n",
      "Average Return: 3.6799190044403076\n",
      "\n",
      "Iteration: 26000\n",
      "Train Loss: 0.011430520564317703\n",
      "Average Return: 4.654152870178223\n",
      "\n",
      "Iteration: 26500\n",
      "Train Loss: 0.0182991623878479\n",
      "Average Return: 4.311516761779785\n",
      "\n",
      "Iteration: 27000\n",
      "Train Loss: 0.016770176589488983\n",
      "Average Return: 5.393486022949219\n",
      "\n",
      "Iteration: 27500\n",
      "Train Loss: 0.021140005439519882\n",
      "Average Return: 3.936338424682617\n",
      "\n",
      "Iteration: 28000\n",
      "Train Loss: 0.01569332741200924\n",
      "Average Return: 7.5884270668029785\n",
      "\n",
      "Iteration: 28500\n",
      "Train Loss: 0.012925089336931705\n",
      "Average Return: 2.9806954860687256\n",
      "\n",
      "Iteration: 29000\n",
      "Train Loss: 0.01698329485952854\n",
      "Average Return: 3.1081724166870117\n",
      "\n",
      "Iteration: 29500\n",
      "Train Loss: 0.01295885257422924\n",
      "Average Return: 5.747872829437256\n",
      "\n",
      "Iteration: 30000\n",
      "Train Loss: 0.01581253670156002\n",
      "Average Return: 7.566363334655762\n",
      "\n",
      "Iteration: 30500\n",
      "Train Loss: 0.012269288301467896\n",
      "Average Return: 5.6371989250183105\n",
      "\n",
      "Iteration: 31000\n",
      "Train Loss: 0.039182040840387344\n",
      "Average Return: 4.370619773864746\n",
      "\n",
      "Iteration: 31500\n",
      "Train Loss: 0.03193745017051697\n",
      "Average Return: 6.0562944412231445\n",
      "\n",
      "Iteration: 32000\n",
      "Train Loss: 0.009259292855858803\n",
      "Average Return: 5.324036121368408\n",
      "\n",
      "Iteration: 32500\n",
      "Train Loss: 0.019197404384613037\n",
      "Average Return: 6.2896013259887695\n",
      "\n",
      "Iteration: 33000\n",
      "Train Loss: 0.017812417820096016\n",
      "Average Return: 6.025668621063232\n",
      "\n",
      "Iteration: 33500\n",
      "Train Loss: 0.022294797003269196\n",
      "Average Return: 4.277050971984863\n",
      "\n",
      "Iteration: 34000\n",
      "Train Loss: 0.014467899687588215\n",
      "Average Return: 5.276973247528076\n",
      "\n",
      "Iteration: 34500\n",
      "Train Loss: 0.013627307489514351\n",
      "Average Return: 5.310553073883057\n",
      "\n",
      "Iteration: 35000\n",
      "Train Loss: 0.0220923013985157\n",
      "Average Return: 5.942663669586182\n",
      "\n",
      "Iteration: 35500\n",
      "Train Loss: 0.012416422367095947\n",
      "Average Return: 6.917262554168701\n",
      "\n",
      "Iteration: 36000\n",
      "Train Loss: 0.01278492622077465\n",
      "Average Return: 3.0971412658691406\n",
      "\n",
      "Iteration: 36500\n",
      "Train Loss: 0.02036440372467041\n",
      "Average Return: 5.603600025177002\n",
      "\n",
      "Iteration: 37000\n",
      "Train Loss: 0.02063087187707424\n",
      "Average Return: 4.610559940338135\n",
      "\n",
      "Iteration: 37500\n",
      "Train Loss: 0.02211175486445427\n",
      "Average Return: 4.94388484954834\n",
      "\n",
      "Iteration: 38000\n",
      "Train Loss: 0.0314655601978302\n",
      "Average Return: 3.7117555141448975\n",
      "\n",
      "Iteration: 38500\n",
      "Train Loss: 0.019459759816527367\n",
      "Average Return: 5.388540744781494\n",
      "\n",
      "Iteration: 39000\n",
      "Train Loss: 0.017997603863477707\n",
      "Average Return: 4.716496467590332\n",
      "\n",
      "Iteration: 39500\n",
      "Train Loss: 0.019574714824557304\n",
      "Average Return: 5.472865104675293\n",
      "\n",
      "Iteration: 40000\n",
      "Train Loss: 0.045952048152685165\n",
      "Average Return: 6.3096113204956055\n",
      "\n",
      "Iteration: 40500\n",
      "Train Loss: 0.03661910444498062\n",
      "Average Return: 4.914664268493652\n",
      "\n",
      "Iteration: 41000\n",
      "Train Loss: 0.019425945356488228\n",
      "Average Return: 6.04649019241333\n",
      "\n",
      "Iteration: 41500\n",
      "Train Loss: 0.026038993149995804\n",
      "Average Return: 5.843971252441406\n",
      "\n",
      "Iteration: 42000\n",
      "Train Loss: 0.05185095965862274\n",
      "Average Return: 4.1673102378845215\n",
      "\n",
      "Iteration: 42500\n",
      "Train Loss: 0.022703323513269424\n",
      "Average Return: 4.311107158660889\n",
      "\n",
      "Iteration: 43000\n",
      "Train Loss: 0.016538910567760468\n",
      "Average Return: 4.937343120574951\n",
      "\n",
      "Iteration: 43500\n",
      "Train Loss: 0.036245912313461304\n",
      "Average Return: 5.228307723999023\n",
      "\n",
      "Iteration: 44000\n",
      "Train Loss: 0.038378190249204636\n",
      "Average Return: 5.994081974029541\n",
      "\n",
      "Iteration: 44500\n",
      "Train Loss: 0.01672963984310627\n",
      "Average Return: 5.685704708099365\n",
      "\n",
      "Iteration: 45000\n",
      "Train Loss: 0.024119850248098373\n",
      "Average Return: 5.2124552726745605\n",
      "\n",
      "Iteration: 45500\n",
      "Train Loss: 0.0334806926548481\n",
      "Average Return: 6.156665802001953\n",
      "\n",
      "Iteration: 46000\n",
      "Train Loss: 0.011709161102771759\n",
      "Average Return: 4.901533603668213\n",
      "\n",
      "Iteration: 46500\n",
      "Train Loss: 0.0385434553027153\n",
      "Average Return: 5.075347423553467\n",
      "\n",
      "Iteration: 47000\n",
      "Train Loss: 0.03095344826579094\n",
      "Average Return: 7.686628818511963\n",
      "\n",
      "Iteration: 47500\n",
      "Train Loss: 0.025934195145964622\n",
      "Average Return: 7.227249622344971\n",
      "\n",
      "Iteration: 48000\n",
      "Train Loss: 0.0279375147074461\n",
      "Average Return: 6.500317573547363\n",
      "\n",
      "Iteration: 48500\n",
      "Train Loss: 0.024116208776831627\n",
      "Average Return: 6.075719356536865\n",
      "\n",
      "Iteration: 49000\n",
      "Train Loss: 0.016743995249271393\n",
      "Average Return: 5.3441901206970215\n",
      "\n",
      "Iteration: 49500\n",
      "Train Loss: 0.03257271274924278\n",
      "Average Return: 7.03463888168335\n",
      "\n",
      "Iteration: 50000\n",
      "Train Loss: 0.04669810086488724\n",
      "Average Return: 6.2928032875061035\n",
      "[ 0.10294513 -0.00434707  0.0055    ]\n",
      "[-0.00475098  0.02731244  0.0055    ]\n",
      "Collecting Initial Samples...\n",
      "Training has started...\n",
      "\n",
      "New best average return found at 13.840436935424805! Saving checkpoint at iteration 0\n",
      "\n",
      "New best average return found at 19.946348190307617! Saving checkpoint at iteration 500\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 0.006114213727414608\n",
      "Average Return: 19.946348190307617\n",
      "\n",
      "New best average return found at 21.316450119018555! Saving checkpoint at iteration 1000\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 0.00589442765340209\n",
      "Average Return: 21.316450119018555\n",
      "\n",
      "Iteration: 1500\n",
      "Train Loss: 0.00702268723398447\n",
      "Average Return: 21.023984909057617\n",
      "\n",
      "Iteration: 2000\n",
      "Train Loss: 0.007647892460227013\n",
      "Average Return: 19.102901458740234\n",
      "\n",
      "Iteration: 2500\n",
      "Train Loss: 0.012732228264212608\n",
      "Average Return: 15.276375770568848\n",
      "\n",
      "New best average return found at 21.365827560424805! Saving checkpoint at iteration 3000\n",
      "\n",
      "Iteration: 3000\n",
      "Train Loss: 0.019050804898142815\n",
      "Average Return: 21.365827560424805\n",
      "\n",
      "Iteration: 3500\n",
      "Train Loss: 0.02683158591389656\n",
      "Average Return: 21.36334991455078\n",
      "\n",
      "Iteration: 4000\n",
      "Train Loss: 0.024647826328873634\n",
      "Average Return: 21.121532440185547\n",
      "\n",
      "Iteration: 4500\n",
      "Train Loss: 0.03155744820833206\n",
      "Average Return: 18.79696273803711\n",
      "\n",
      "Iteration: 5000\n",
      "Train Loss: 0.01626419834792614\n",
      "Average Return: 15.573246955871582\n",
      "\n",
      "Iteration: 5500\n",
      "Train Loss: 0.029007401317358017\n",
      "Average Return: 16.511436462402344\n",
      "\n",
      "Iteration: 6000\n",
      "Train Loss: 0.007321045733988285\n",
      "Average Return: 17.828466415405273\n",
      "\n",
      "Iteration: 6500\n",
      "Train Loss: 0.033613771200180054\n",
      "Average Return: 16.994298934936523\n",
      "\n",
      "Iteration: 7000\n",
      "Train Loss: 0.012614846229553223\n",
      "Average Return: 16.189666748046875\n",
      "\n",
      "Iteration: 7500\n",
      "Train Loss: 0.00866527296602726\n",
      "Average Return: 19.497751235961914\n",
      "\n",
      "Iteration: 8000\n",
      "Train Loss: 0.0255389716476202\n",
      "Average Return: 17.122495651245117\n",
      "\n",
      "Iteration: 8500\n",
      "Train Loss: 0.026208089664578438\n",
      "Average Return: 19.34674644470215\n",
      "\n",
      "Iteration: 9000\n",
      "Train Loss: 0.011955119669437408\n",
      "Average Return: 16.27536392211914\n",
      "\n",
      "Iteration: 9500\n",
      "Train Loss: 0.03928687423467636\n",
      "Average Return: 17.517730712890625\n",
      "\n",
      "Iteration: 10000\n",
      "Train Loss: 0.02270110696554184\n",
      "Average Return: 15.490431785583496\n",
      "\n",
      "Iteration: 10500\n",
      "Train Loss: 0.019675513729453087\n",
      "Average Return: 15.729460716247559\n",
      "\n",
      "Iteration: 11000\n",
      "Train Loss: 0.037081822752952576\n",
      "Average Return: 18.418359756469727\n",
      "\n",
      "New best average return found at 21.529449462890625! Saving checkpoint at iteration 11500\n",
      "\n",
      "Iteration: 11500\n",
      "Train Loss: 0.02527356892824173\n",
      "Average Return: 21.529449462890625\n",
      "\n",
      "Iteration: 12000\n",
      "Train Loss: 0.10852238535881042\n",
      "Average Return: 18.72149658203125\n",
      "\n",
      "Iteration: 12500\n",
      "Train Loss: 0.024041015654802322\n",
      "Average Return: 19.923742294311523\n",
      "\n",
      "Iteration: 13000\n",
      "Train Loss: 0.017357390373945236\n",
      "Average Return: 18.777074813842773\n",
      "\n",
      "Iteration: 13500\n",
      "Train Loss: 0.024108730256557465\n",
      "Average Return: 18.949981689453125\n",
      "\n",
      "Iteration: 14000\n",
      "Train Loss: 0.03325943648815155\n",
      "Average Return: 18.091886520385742\n",
      "\n",
      "Iteration: 14500\n",
      "Train Loss: 0.023703549057245255\n",
      "Average Return: 20.33707618713379\n",
      "\n",
      "Iteration: 15000\n",
      "Train Loss: 0.0291123166680336\n",
      "Average Return: 19.419742584228516\n",
      "\n",
      "Iteration: 15500\n",
      "Train Loss: 0.04370000213384628\n",
      "Average Return: 18.71815299987793\n",
      "\n",
      "Iteration: 16000\n",
      "Train Loss: 0.03338368237018585\n",
      "Average Return: 17.853609085083008\n",
      "\n",
      "Iteration: 16500\n",
      "Train Loss: 0.020105136558413506\n",
      "Average Return: 14.107475280761719\n",
      "\n",
      "Iteration: 17000\n",
      "Train Loss: 0.06711456924676895\n",
      "Average Return: 19.826799392700195\n",
      "\n",
      "Iteration: 17500\n",
      "Train Loss: 0.03998900204896927\n",
      "Average Return: 17.053865432739258\n",
      "\n",
      "Iteration: 18000\n",
      "Train Loss: 0.017391003668308258\n",
      "Average Return: 18.855506896972656\n",
      "\n",
      "Iteration: 18500\n",
      "Train Loss: 0.07452026754617691\n",
      "Average Return: 17.103343963623047\n",
      "\n",
      "Iteration: 19000\n",
      "Train Loss: 0.04958229139447212\n",
      "Average Return: 18.760723114013672\n",
      "\n",
      "Iteration: 19500\n",
      "Train Loss: 0.08879014849662781\n",
      "Average Return: 17.300537109375\n",
      "\n",
      "Iteration: 20000\n",
      "Train Loss: 0.034543607383966446\n",
      "Average Return: 17.369844436645508\n",
      "\n",
      "Iteration: 20500\n",
      "Train Loss: 0.03829532861709595\n",
      "Average Return: 18.76987648010254\n",
      "\n",
      "Iteration: 21000\n",
      "Train Loss: 0.03653031215071678\n",
      "Average Return: 15.615498542785645\n",
      "\n",
      "Iteration: 21500\n",
      "Train Loss: 0.013261095620691776\n",
      "Average Return: 17.470481872558594\n",
      "\n",
      "Iteration: 22000\n",
      "Train Loss: 0.04160945117473602\n",
      "Average Return: 19.86798095703125\n",
      "\n",
      "Iteration: 22500\n",
      "Train Loss: 0.03654737025499344\n",
      "Average Return: 18.42576026916504\n",
      "\n",
      "Iteration: 23000\n",
      "Train Loss: 0.03602395951747894\n",
      "Average Return: 18.145910263061523\n",
      "\n",
      "Iteration: 23500\n",
      "Train Loss: 0.052747566252946854\n",
      "Average Return: 16.7108154296875\n",
      "\n",
      "Iteration: 24000\n",
      "Train Loss: 0.03450784832239151\n",
      "Average Return: 16.66572380065918\n",
      "\n",
      "Iteration: 24500\n",
      "Train Loss: 0.058210425078868866\n",
      "Average Return: 17.583126068115234\n",
      "\n",
      "Iteration: 25000\n",
      "Train Loss: 0.04078636318445206\n",
      "Average Return: 17.107622146606445\n",
      "\n",
      "Iteration: 25500\n",
      "Train Loss: 0.04458443820476532\n",
      "Average Return: 17.625513076782227\n",
      "\n",
      "Iteration: 26000\n",
      "Train Loss: 0.021735109388828278\n",
      "Average Return: 15.686413764953613\n",
      "\n",
      "Iteration: 26500\n",
      "Train Loss: 0.06301416456699371\n",
      "Average Return: 14.750591278076172\n",
      "\n",
      "Iteration: 27000\n",
      "Train Loss: 0.05429154634475708\n",
      "Average Return: 13.906765937805176\n",
      "\n",
      "Iteration: 27500\n",
      "Train Loss: 0.04108323156833649\n",
      "Average Return: 14.604355812072754\n",
      "\n",
      "Iteration: 28000\n",
      "Train Loss: 0.04022756963968277\n",
      "Average Return: 14.91170883178711\n",
      "\n",
      "Iteration: 28500\n",
      "Train Loss: 0.0301813967525959\n",
      "Average Return: 16.602506637573242\n",
      "\n",
      "Iteration: 29000\n",
      "Train Loss: 0.05451229214668274\n",
      "Average Return: 16.41139030456543\n",
      "\n",
      "Iteration: 29500\n",
      "Train Loss: 0.03654639795422554\n",
      "Average Return: 17.111501693725586\n",
      "\n",
      "Iteration: 30000\n",
      "Train Loss: 0.03077990561723709\n",
      "Average Return: 16.908855438232422\n",
      "\n",
      "Iteration: 30500\n",
      "Train Loss: 0.02051299251616001\n",
      "Average Return: 16.481752395629883\n",
      "\n",
      "Iteration: 31000\n",
      "Train Loss: 0.08236920833587646\n",
      "Average Return: 15.889859199523926\n",
      "\n",
      "Iteration: 31500\n",
      "Train Loss: 0.06471411883831024\n",
      "Average Return: 16.750106811523438\n",
      "\n",
      "Iteration: 32000\n",
      "Train Loss: 0.032408565282821655\n",
      "Average Return: 18.249691009521484\n",
      "\n",
      "Iteration: 32500\n",
      "Train Loss: 0.053541772067546844\n",
      "Average Return: 18.827566146850586\n",
      "\n",
      "Iteration: 33000\n",
      "Train Loss: 0.03168687969446182\n",
      "Average Return: 18.97916030883789\n",
      "\n",
      "Iteration: 33500\n",
      "Train Loss: 0.06806018948554993\n",
      "Average Return: 16.56060791015625\n",
      "\n",
      "Iteration: 34000\n",
      "Train Loss: 0.023869652301073074\n",
      "Average Return: 18.578950881958008\n",
      "\n",
      "Iteration: 34500\n",
      "Train Loss: 0.024721426889300346\n",
      "Average Return: 18.454256057739258\n",
      "\n",
      "Iteration: 35000\n",
      "Train Loss: 0.0533745214343071\n",
      "Average Return: 18.854475021362305\n",
      "\n",
      "Iteration: 35500\n",
      "Train Loss: 0.017641134560108185\n",
      "Average Return: 17.520788192749023\n",
      "\n",
      "Iteration: 36000\n",
      "Train Loss: 0.040930524468421936\n",
      "Average Return: 17.184762954711914\n",
      "\n",
      "Iteration: 36500\n",
      "Train Loss: 0.07635422050952911\n",
      "Average Return: 17.990568161010742\n",
      "\n",
      "Iteration: 37000\n",
      "Train Loss: 0.03852621838450432\n",
      "Average Return: 17.51554298400879\n",
      "\n",
      "Iteration: 37500\n",
      "Train Loss: 0.054786257445812225\n",
      "Average Return: 15.725855827331543\n",
      "\n",
      "Iteration: 38000\n",
      "Train Loss: 0.06625214964151382\n",
      "Average Return: 16.322406768798828\n",
      "\n",
      "Iteration: 38500\n",
      "Train Loss: 0.0415988489985466\n",
      "Average Return: 15.61486530303955\n",
      "\n",
      "Iteration: 39000\n",
      "Train Loss: 0.05256813019514084\n",
      "Average Return: 16.647085189819336\n",
      "\n",
      "Iteration: 39500\n",
      "Train Loss: 0.04397501423954964\n",
      "Average Return: 18.36735725402832\n",
      "\n",
      "Iteration: 40000\n",
      "Train Loss: 0.09139300882816315\n",
      "Average Return: 17.474658966064453\n",
      "\n",
      "Iteration: 40500\n",
      "Train Loss: 0.08616483956575394\n",
      "Average Return: 15.782842636108398\n",
      "\n",
      "Iteration: 41000\n",
      "Train Loss: 0.035664819180965424\n",
      "Average Return: 17.64005470275879\n",
      "\n",
      "Iteration: 41500\n",
      "Train Loss: 0.06110486388206482\n",
      "Average Return: 17.122806549072266\n",
      "\n",
      "Iteration: 42000\n",
      "Train Loss: 0.12293875962495804\n",
      "Average Return: 15.885910034179688\n",
      "\n",
      "Iteration: 42500\n",
      "Train Loss: 0.03752528503537178\n",
      "Average Return: 14.299742698669434\n",
      "\n",
      "Iteration: 43000\n",
      "Train Loss: 0.03639521449804306\n",
      "Average Return: 15.35865306854248\n",
      "\n",
      "Iteration: 43500\n",
      "Train Loss: 0.10259287059307098\n",
      "Average Return: 16.847782135009766\n",
      "\n",
      "Iteration: 44000\n",
      "Train Loss: 0.07273298501968384\n",
      "Average Return: 17.00514793395996\n",
      "\n",
      "Iteration: 44500\n",
      "Train Loss: 0.05854785442352295\n",
      "Average Return: 17.918672561645508\n",
      "\n",
      "Iteration: 45000\n",
      "Train Loss: 0.06741993874311447\n",
      "Average Return: 14.87214183807373\n",
      "\n",
      "Iteration: 45500\n",
      "Train Loss: 0.08678200840950012\n",
      "Average Return: 15.648958206176758\n",
      "\n",
      "Iteration: 46000\n",
      "Train Loss: 0.03979190066456795\n",
      "Average Return: 14.328060150146484\n",
      "\n",
      "Iteration: 46500\n",
      "Train Loss: 0.06826020777225494\n",
      "Average Return: 15.025760650634766\n",
      "\n",
      "Iteration: 47000\n",
      "Train Loss: 0.05330107733607292\n",
      "Average Return: 15.767072677612305\n",
      "\n",
      "Iteration: 47500\n",
      "Train Loss: 0.06964316219091415\n",
      "Average Return: 17.194900512695312\n",
      "\n",
      "Iteration: 48000\n",
      "Train Loss: 0.05504268780350685\n",
      "Average Return: 16.221960067749023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kohli\\Desktop\\TraderNetv2\\metrics\\trading\\sortino.py:20: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.exp(average_returns/std_downfall_returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 48500\n",
      "Train Loss: 0.06042177602648735\n",
      "Average Return: 16.117435455322266\n",
      "\n",
      "Iteration: 49000\n",
      "Train Loss: 0.04850027710199356\n",
      "Average Return: 16.54693031311035\n",
      "\n",
      "Iteration: 49500\n",
      "Train Loss: 0.07344761490821838\n",
      "Average Return: 16.248594284057617\n",
      "\n",
      "Iteration: 50000\n",
      "Train Loss: 0.09420417249202728\n",
      "Average Return: 14.750248908996582\n",
      "[-0.00268924  0.00833656  0.0055    ]\n",
      "[0.00048856 0.00315544 0.0055    ]\n",
      "Collecting Initial Samples...\n",
      "Training has started...\n",
      "\n",
      "New best average return found at 12.291582107543945! Saving checkpoint at iteration 0\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 0.00220282468944788\n",
      "Average Return: -3.736907482147217\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 0.002757929265499115\n",
      "Average Return: 0.6621885299682617\n",
      "\n",
      "Iteration: 1500\n",
      "Train Loss: 0.004209998995065689\n",
      "Average Return: 8.344561576843262\n",
      "\n",
      "Iteration: 2000\n",
      "Train Loss: 0.0019672829657793045\n",
      "Average Return: -1.7098358869552612\n",
      "\n",
      "Iteration: 2500\n",
      "Train Loss: 0.0024183657951653004\n",
      "Average Return: 5.365845203399658\n",
      "\n",
      "Iteration: 3000\n",
      "Train Loss: 0.003456167643889785\n",
      "Average Return: 5.354405879974365\n",
      "\n",
      "Iteration: 3500\n",
      "Train Loss: 0.005137308035045862\n",
      "Average Return: 5.173117160797119\n",
      "\n",
      "Iteration: 4000\n",
      "Train Loss: 0.006498896516859531\n",
      "Average Return: 4.571653366088867\n",
      "\n",
      "Iteration: 4500\n",
      "Train Loss: 0.0053704772144556046\n",
      "Average Return: 3.794158458709717\n",
      "\n",
      "Iteration: 5000\n",
      "Train Loss: 0.007258159574121237\n",
      "Average Return: 1.555629849433899\n",
      "\n",
      "Iteration: 5500\n",
      "Train Loss: 0.008414960466325283\n",
      "Average Return: 0.592251718044281\n",
      "\n",
      "Iteration: 6000\n",
      "Train Loss: 0.0037598898634314537\n",
      "Average Return: 3.5498881340026855\n",
      "\n",
      "Iteration: 6500\n",
      "Train Loss: 0.005747739225625992\n",
      "Average Return: 10.22899341583252\n",
      "\n",
      "Iteration: 7000\n",
      "Train Loss: 0.010475311428308487\n",
      "Average Return: 3.33944034576416\n",
      "\n",
      "Iteration: 7500\n",
      "Train Loss: 0.004527225159108639\n",
      "Average Return: 4.7377824783325195\n",
      "\n",
      "Iteration: 8000\n",
      "Train Loss: 0.00903356447815895\n",
      "Average Return: 4.403441429138184\n",
      "\n",
      "Iteration: 8500\n",
      "Train Loss: 0.009584276005625725\n",
      "Average Return: 5.596371650695801\n",
      "\n",
      "Iteration: 9000\n",
      "Train Loss: 0.009606634266674519\n",
      "Average Return: 9.460647583007812\n",
      "\n",
      "Iteration: 9500\n",
      "Train Loss: 0.012774324044585228\n",
      "Average Return: 8.614592552185059\n",
      "\n",
      "Iteration: 10000\n",
      "Train Loss: 0.014031664468348026\n",
      "Average Return: -0.17713703215122223\n",
      "\n",
      "Iteration: 10500\n",
      "Train Loss: 0.01542507205158472\n",
      "Average Return: 3.013742208480835\n",
      "\n",
      "Iteration: 11000\n",
      "Train Loss: 0.012541821226477623\n",
      "Average Return: 0.013328379020094872\n",
      "\n",
      "Iteration: 11500\n",
      "Train Loss: 0.00795566476881504\n",
      "Average Return: 4.104037761688232\n",
      "\n",
      "Iteration: 12000\n",
      "Train Loss: 0.0040426105260849\n",
      "Average Return: 4.857593536376953\n",
      "\n",
      "Iteration: 12500\n",
      "Train Loss: 0.01589229330420494\n",
      "Average Return: 1.159116506576538\n",
      "\n",
      "Iteration: 13000\n",
      "Train Loss: 0.011390462517738342\n",
      "Average Return: 2.6815037727355957\n",
      "\n",
      "Iteration: 13500\n",
      "Train Loss: 0.023827845230698586\n",
      "Average Return: -1.7521305084228516\n",
      "\n",
      "Iteration: 14000\n",
      "Train Loss: 0.01302323117852211\n",
      "Average Return: 1.6775188446044922\n",
      "\n",
      "Iteration: 14500\n",
      "Train Loss: 0.009754525497555733\n",
      "Average Return: 2.8178768157958984\n",
      "\n",
      "Iteration: 15000\n",
      "Train Loss: 0.01576061360538006\n",
      "Average Return: 1.1382863521575928\n",
      "\n",
      "Iteration: 15500\n",
      "Train Loss: 0.012734156101942062\n",
      "Average Return: 2.0137860774993896\n",
      "\n",
      "Iteration: 16000\n",
      "Train Loss: 0.01611020788550377\n",
      "Average Return: 4.361443996429443\n",
      "\n",
      "Iteration: 16500\n",
      "Train Loss: 0.009902918711304665\n",
      "Average Return: 4.462963581085205\n",
      "\n",
      "Iteration: 17000\n",
      "Train Loss: 0.029570482671260834\n",
      "Average Return: 4.3891520500183105\n",
      "\n",
      "Iteration: 17500\n",
      "Train Loss: 0.04208149015903473\n",
      "Average Return: 2.189785957336426\n",
      "\n",
      "Iteration: 18000\n",
      "Train Loss: 0.046694401651620865\n",
      "Average Return: -0.4799096882343292\n",
      "\n",
      "Iteration: 18500\n",
      "Train Loss: 0.025475354865193367\n",
      "Average Return: -1.3056402206420898\n",
      "\n",
      "Iteration: 19000\n",
      "Train Loss: 0.017717797309160233\n",
      "Average Return: 3.373734474182129\n",
      "\n",
      "Iteration: 19500\n",
      "Train Loss: 0.013781886547803879\n",
      "Average Return: 2.355468273162842\n",
      "\n",
      "Iteration: 20000\n",
      "Train Loss: 0.062377989292144775\n",
      "Average Return: 1.0710943937301636\n",
      "\n",
      "Iteration: 20500\n",
      "Train Loss: 0.02623252384364605\n",
      "Average Return: 1.3206433057785034\n",
      "\n",
      "Iteration: 21000\n",
      "Train Loss: 0.02587888017296791\n",
      "Average Return: 2.2186787128448486\n",
      "\n",
      "Iteration: 21500\n",
      "Train Loss: 0.00867857038974762\n",
      "Average Return: 0.1788317859172821\n",
      "\n",
      "Iteration: 22000\n",
      "Train Loss: 0.007024798076599836\n",
      "Average Return: 1.4699351787567139\n",
      "\n",
      "Iteration: 22500\n",
      "Train Loss: 0.030614515766501427\n",
      "Average Return: 5.814586639404297\n",
      "\n",
      "Iteration: 23000\n",
      "Train Loss: 0.006922203116118908\n",
      "Average Return: 3.500530481338501\n",
      "\n",
      "Iteration: 23500\n",
      "Train Loss: 0.0128196831792593\n",
      "Average Return: 2.5864078998565674\n",
      "\n",
      "Iteration: 24000\n",
      "Train Loss: 0.01481835450977087\n",
      "Average Return: 6.594135761260986\n",
      "\n",
      "Iteration: 24500\n",
      "Train Loss: 0.027646463364362717\n",
      "Average Return: 0.08675379306077957\n",
      "\n",
      "Iteration: 25000\n",
      "Train Loss: 0.01701148971915245\n",
      "Average Return: 4.8479838371276855\n",
      "\n",
      "Iteration: 25500\n",
      "Train Loss: 0.013255228288471699\n",
      "Average Return: -1.0325267314910889\n",
      "\n",
      "Iteration: 26000\n",
      "Train Loss: 0.01691790297627449\n",
      "Average Return: 5.409987449645996\n",
      "\n",
      "Iteration: 26500\n",
      "Train Loss: 0.020963171496987343\n",
      "Average Return: 5.146527290344238\n",
      "\n",
      "Iteration: 27000\n",
      "Train Loss: 0.018110809847712517\n",
      "Average Return: 6.727366924285889\n",
      "\n",
      "Iteration: 27500\n",
      "Train Loss: 0.01808355748653412\n",
      "Average Return: 4.873386383056641\n",
      "\n",
      "Iteration: 28000\n",
      "Train Loss: 0.012797493487596512\n",
      "Average Return: 5.2516584396362305\n",
      "\n",
      "Iteration: 28500\n",
      "Train Loss: 0.027030007913708687\n",
      "Average Return: 3.6934635639190674\n",
      "\n",
      "Iteration: 29000\n",
      "Train Loss: 0.021491874009370804\n",
      "Average Return: 3.5130269527435303\n",
      "\n",
      "Iteration: 29500\n",
      "Train Loss: 0.026851670816540718\n",
      "Average Return: 2.1807162761688232\n",
      "\n",
      "Iteration: 30000\n",
      "Train Loss: 0.015056231990456581\n",
      "Average Return: 5.573556900024414\n",
      "\n",
      "Iteration: 30500\n",
      "Train Loss: 0.014652767218649387\n",
      "Average Return: 3.10962176322937\n",
      "\n",
      "Iteration: 31000\n",
      "Train Loss: 0.020224161446094513\n",
      "Average Return: 4.304610252380371\n",
      "\n",
      "Iteration: 31500\n",
      "Train Loss: 0.01431529875844717\n",
      "Average Return: 1.6044052839279175\n",
      "\n",
      "Iteration: 32000\n",
      "Train Loss: 0.022509615868330002\n",
      "Average Return: 3.0506443977355957\n",
      "\n",
      "Iteration: 32500\n",
      "Train Loss: 0.02419668808579445\n",
      "Average Return: 4.293529987335205\n",
      "\n",
      "Iteration: 33000\n",
      "Train Loss: 0.016633301973342896\n",
      "Average Return: 6.64499044418335\n",
      "\n",
      "Iteration: 33500\n",
      "Train Loss: 0.01810573972761631\n",
      "Average Return: 3.5582876205444336\n",
      "\n",
      "Iteration: 34000\n",
      "Train Loss: 0.018196651712059975\n",
      "Average Return: 4.075159549713135\n",
      "\n",
      "Iteration: 34500\n",
      "Train Loss: 0.017311574891209602\n",
      "Average Return: 4.339103698730469\n",
      "\n",
      "Iteration: 35000\n",
      "Train Loss: 0.02059953473508358\n",
      "Average Return: 4.336085319519043\n",
      "\n",
      "Iteration: 35500\n",
      "Train Loss: 0.019583510234951973\n",
      "Average Return: 3.408933401107788\n",
      "\n",
      "Iteration: 36000\n",
      "Train Loss: 0.021321725100278854\n",
      "Average Return: 2.46677303314209\n",
      "\n",
      "Iteration: 36500\n",
      "Train Loss: 0.018456127494573593\n",
      "Average Return: 2.8202247619628906\n",
      "\n",
      "Iteration: 37000\n",
      "Train Loss: 0.03207540884613991\n",
      "Average Return: 3.7560739517211914\n",
      "\n",
      "Iteration: 37500\n",
      "Train Loss: 0.015589100308716297\n",
      "Average Return: 4.148589611053467\n",
      "\n",
      "Iteration: 38000\n",
      "Train Loss: 0.023772895336151123\n",
      "Average Return: 4.984317779541016\n",
      "\n",
      "Iteration: 38500\n",
      "Train Loss: 0.024967385455965996\n",
      "Average Return: 6.237004280090332\n",
      "\n",
      "Iteration: 39000\n",
      "Train Loss: 0.03512478619813919\n",
      "Average Return: 5.741809844970703\n",
      "\n",
      "Iteration: 39500\n",
      "Train Loss: 0.019344963133335114\n",
      "Average Return: 4.4566731452941895\n",
      "\n",
      "Iteration: 40000\n",
      "Train Loss: 0.023866569623351097\n",
      "Average Return: 3.9053027629852295\n",
      "\n",
      "Iteration: 40500\n",
      "Train Loss: 0.03538745641708374\n",
      "Average Return: 5.548161506652832\n",
      "\n",
      "Iteration: 41000\n",
      "Train Loss: 0.019822098314762115\n",
      "Average Return: 1.7916042804718018\n",
      "\n",
      "Iteration: 41500\n",
      "Train Loss: 0.0293559692800045\n",
      "Average Return: 2.4006288051605225\n",
      "\n",
      "Iteration: 42000\n",
      "Train Loss: 0.02225513383746147\n",
      "Average Return: 5.243511199951172\n",
      "\n",
      "Iteration: 42500\n",
      "Train Loss: 0.030776653438806534\n",
      "Average Return: 3.509725570678711\n",
      "\n",
      "Iteration: 43000\n",
      "Train Loss: 0.021671047434210777\n",
      "Average Return: 4.186438083648682\n",
      "\n",
      "Iteration: 43500\n",
      "Train Loss: 0.027362048625946045\n",
      "Average Return: 5.076998233795166\n",
      "\n",
      "Iteration: 44000\n",
      "Train Loss: 0.01933646947145462\n",
      "Average Return: 3.0475807189941406\n",
      "\n",
      "Iteration: 44500\n",
      "Train Loss: 0.01808852143585682\n",
      "Average Return: 4.126412391662598\n",
      "\n",
      "Iteration: 45000\n",
      "Train Loss: 0.017619606107473373\n",
      "Average Return: 4.527165412902832\n",
      "\n",
      "Iteration: 45500\n",
      "Train Loss: 0.017136892303824425\n",
      "Average Return: 5.081984519958496\n",
      "\n",
      "Iteration: 46000\n",
      "Train Loss: 0.014610249549150467\n",
      "Average Return: 4.638633728027344\n",
      "\n",
      "Iteration: 46500\n",
      "Train Loss: 0.018155375495553017\n",
      "Average Return: 3.6816039085388184\n",
      "\n",
      "Iteration: 47000\n",
      "Train Loss: 0.0231182798743248\n",
      "Average Return: 4.51503324508667\n",
      "\n",
      "Iteration: 47500\n",
      "Train Loss: 0.0143346032127738\n",
      "Average Return: 5.762074947357178\n",
      "\n",
      "Iteration: 48000\n",
      "Train Loss: 0.018773261457681656\n",
      "Average Return: 5.1272172927856445\n",
      "\n",
      "Iteration: 48500\n",
      "Train Loss: 0.021046442911028862\n",
      "Average Return: 4.598287582397461\n",
      "\n",
      "Iteration: 49000\n",
      "Train Loss: 0.018827538937330246\n",
      "Average Return: 4.087630271911621\n",
      "\n",
      "Iteration: 49500\n",
      "Train Loss: 0.021465010941028595\n",
      "Average Return: 6.064727783203125\n",
      "\n",
      "Iteration: 50000\n",
      "Train Loss: 0.03437327593564987\n",
      "Average Return: 5.836003303527832\n",
      "[0.04696136 0.01193588 0.0055    ]\n",
      "[0.00632184 0.0072379  0.0055    ]\n",
      "Collecting Initial Samples...\n",
      "Training has started...\n",
      "\n",
      "New best average return found at 12.375312805175781! Saving checkpoint at iteration 0\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 0.004623102024197578\n",
      "Average Return: 10.397420883178711\n",
      "\n",
      "New best average return found at 14.540393829345703! Saving checkpoint at iteration 1000\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 0.008606797084212303\n",
      "Average Return: 14.540393829345703\n",
      "\n",
      "Iteration: 1500\n",
      "Train Loss: 0.007002356927841902\n",
      "Average Return: 10.191876411437988\n",
      "\n",
      "Iteration: 2000\n",
      "Train Loss: 0.0033335527405142784\n",
      "Average Return: 11.522625923156738\n",
      "\n",
      "Iteration: 2500\n",
      "Train Loss: 0.004204547498375177\n",
      "Average Return: 12.218823432922363\n",
      "\n",
      "Iteration: 3000\n",
      "Train Loss: 0.006328930612653494\n",
      "Average Return: 14.127537727355957\n",
      "\n",
      "New best average return found at 16.615007400512695! Saving checkpoint at iteration 3500\n",
      "\n",
      "Iteration: 3500\n",
      "Train Loss: 0.007085511926561594\n",
      "Average Return: 16.615007400512695\n",
      "\n",
      "New best average return found at 16.98896026611328! Saving checkpoint at iteration 4000\n",
      "\n",
      "Iteration: 4000\n",
      "Train Loss: 0.015258452855050564\n",
      "Average Return: 16.98896026611328\n",
      "\n",
      "Iteration: 4500\n",
      "Train Loss: 0.014844737015664577\n",
      "Average Return: 16.686670303344727\n",
      "\n",
      "New best average return found at 17.786239624023438! Saving checkpoint at iteration 5000\n",
      "\n",
      "Iteration: 5000\n",
      "Train Loss: 0.016291160136461258\n",
      "Average Return: 17.786239624023438\n",
      "\n",
      "Iteration: 5500\n",
      "Train Loss: 0.019651751965284348\n",
      "Average Return: 14.702278137207031\n",
      "\n",
      "Iteration: 6000\n",
      "Train Loss: 0.01697520911693573\n",
      "Average Return: 15.064809799194336\n",
      "\n",
      "Iteration: 6500\n",
      "Train Loss: 0.0194692462682724\n",
      "Average Return: 14.160837173461914\n",
      "\n",
      "Iteration: 7000\n",
      "Train Loss: 0.023250266909599304\n",
      "Average Return: 13.767335891723633\n",
      "\n",
      "Iteration: 7500\n",
      "Train Loss: 0.012810620479285717\n",
      "Average Return: 14.240281105041504\n",
      "\n",
      "Iteration: 8000\n",
      "Train Loss: 0.02595215104520321\n",
      "Average Return: 14.427071571350098\n",
      "\n",
      "Iteration: 8500\n",
      "Train Loss: 0.029327403753995895\n",
      "Average Return: 15.557071685791016\n",
      "\n",
      "Iteration: 9000\n",
      "Train Loss: 0.023224644362926483\n",
      "Average Return: 14.232175827026367\n",
      "\n",
      "Iteration: 9500\n",
      "Train Loss: 0.04513297975063324\n",
      "Average Return: 14.184442520141602\n",
      "\n",
      "Iteration: 10000\n",
      "Train Loss: 0.04017210751771927\n",
      "Average Return: 16.102205276489258\n",
      "\n",
      "Iteration: 10500\n",
      "Train Loss: 0.03118841163814068\n",
      "Average Return: 16.280729293823242\n",
      "\n",
      "Iteration: 11000\n",
      "Train Loss: 0.03973960131406784\n",
      "Average Return: 15.748517990112305\n",
      "\n",
      "Iteration: 11500\n",
      "Train Loss: 0.025709345936775208\n",
      "Average Return: 15.748866081237793\n",
      "\n",
      "Iteration: 12000\n",
      "Train Loss: 0.011253328993916512\n",
      "Average Return: 15.002519607543945\n",
      "\n",
      "Iteration: 12500\n",
      "Train Loss: 0.0359322614967823\n",
      "Average Return: 15.594134330749512\n",
      "\n",
      "Iteration: 13000\n",
      "Train Loss: 0.0460154265165329\n",
      "Average Return: 17.033063888549805\n",
      "\n",
      "New best average return found at 18.40960693359375! Saving checkpoint at iteration 13500\n",
      "\n",
      "Iteration: 13500\n",
      "Train Loss: 0.05628792196512222\n",
      "Average Return: 18.40960693359375\n",
      "\n",
      "Iteration: 14000\n",
      "Train Loss: 0.03772102668881416\n",
      "Average Return: 14.37324333190918\n",
      "\n",
      "Iteration: 14500\n",
      "Train Loss: 0.03634554147720337\n",
      "Average Return: 14.844695091247559\n",
      "\n",
      "Iteration: 15000\n",
      "Train Loss: 0.04396911710500717\n",
      "Average Return: 14.665220260620117\n",
      "\n",
      "Iteration: 15500\n",
      "Train Loss: 0.0316639244556427\n",
      "Average Return: 16.8168888092041\n",
      "\n",
      "Iteration: 16000\n",
      "Train Loss: 0.04868985712528229\n",
      "Average Return: 16.73137092590332\n",
      "\n",
      "Iteration: 16500\n",
      "Train Loss: 0.025157814845442772\n",
      "Average Return: 16.984630584716797\n",
      "\n",
      "Iteration: 17000\n",
      "Train Loss: 0.08526621758937836\n",
      "Average Return: 17.347515106201172\n",
      "\n",
      "Iteration: 17500\n",
      "Train Loss: 0.0984310656785965\n",
      "Average Return: 15.412256240844727\n",
      "\n",
      "Iteration: 18000\n",
      "Train Loss: 0.037992507219314575\n",
      "Average Return: 14.813507080078125\n",
      "\n",
      "Iteration: 18500\n",
      "Train Loss: 0.041446492075920105\n",
      "Average Return: 15.802957534790039\n",
      "\n",
      "Iteration: 19000\n",
      "Train Loss: 0.03466680273413658\n",
      "Average Return: 16.832124710083008\n",
      "\n",
      "Iteration: 19500\n",
      "Train Loss: 0.026700368151068687\n",
      "Average Return: 16.53758430480957\n",
      "\n",
      "Iteration: 20000\n",
      "Train Loss: 0.14819005131721497\n",
      "Average Return: 15.444005012512207\n",
      "\n",
      "Iteration: 20500\n",
      "Train Loss: 0.047652050852775574\n",
      "Average Return: 14.580605506896973\n",
      "\n",
      "Iteration: 21000\n",
      "Train Loss: 0.050615452229976654\n",
      "Average Return: 17.314889907836914\n",
      "\n",
      "Iteration: 21500\n",
      "Train Loss: 0.03142309933900833\n",
      "Average Return: 17.52644157409668\n",
      "\n",
      "Iteration: 22000\n",
      "Train Loss: 0.016611184924840927\n",
      "Average Return: 14.776533126831055\n",
      "\n",
      "Iteration: 22500\n",
      "Train Loss: 0.05928048864006996\n",
      "Average Return: 15.851628303527832\n",
      "\n",
      "Iteration: 23000\n",
      "Train Loss: 0.026850106194615364\n",
      "Average Return: 13.299501419067383\n",
      "\n",
      "Iteration: 23500\n",
      "Train Loss: 0.04151073843240738\n",
      "Average Return: 13.947516441345215\n",
      "\n",
      "Iteration: 24000\n",
      "Train Loss: 0.03451596945524216\n",
      "Average Return: 15.408905982971191\n",
      "\n",
      "Iteration: 24500\n",
      "Train Loss: 0.03564229607582092\n",
      "Average Return: 17.75934600830078\n",
      "\n",
      "Iteration: 25000\n",
      "Train Loss: 0.03559596836566925\n",
      "Average Return: 13.615422248840332\n",
      "\n",
      "Iteration: 25500\n",
      "Train Loss: 0.04927600175142288\n",
      "Average Return: 12.122808456420898\n",
      "\n",
      "Iteration: 26000\n",
      "Train Loss: 0.03475891053676605\n",
      "Average Return: 12.956917762756348\n",
      "\n",
      "Iteration: 26500\n",
      "Train Loss: 0.06515325605869293\n",
      "Average Return: 15.841440200805664\n",
      "\n",
      "Iteration: 27000\n",
      "Train Loss: 0.045130565762519836\n",
      "Average Return: 12.95622730255127\n",
      "\n",
      "Iteration: 27500\n",
      "Train Loss: 0.051944516599178314\n",
      "Average Return: 14.100566864013672\n",
      "\n",
      "Iteration: 28000\n",
      "Train Loss: 0.043869636952877045\n",
      "Average Return: 12.010252952575684\n",
      "\n",
      "Iteration: 28500\n",
      "Train Loss: 0.0722968578338623\n",
      "Average Return: 13.795888900756836\n",
      "\n",
      "Iteration: 29000\n",
      "Train Loss: 0.04956142604351044\n",
      "Average Return: 14.72072696685791\n",
      "\n",
      "Iteration: 29500\n",
      "Train Loss: 0.05363049358129501\n",
      "Average Return: 15.874043464660645\n",
      "\n",
      "Iteration: 30000\n",
      "Train Loss: 0.042272187769412994\n",
      "Average Return: 15.097321510314941\n",
      "\n",
      "Iteration: 30500\n",
      "Train Loss: 0.03436031565070152\n",
      "Average Return: 15.28752326965332\n",
      "\n",
      "Iteration: 31000\n",
      "Train Loss: 0.051357753574848175\n",
      "Average Return: 14.033299446105957\n",
      "\n",
      "Iteration: 31500\n",
      "Train Loss: 0.04809840768575668\n",
      "Average Return: 13.889668464660645\n",
      "\n",
      "Iteration: 32000\n",
      "Train Loss: 0.05367518961429596\n",
      "Average Return: 13.940828323364258\n",
      "\n",
      "Iteration: 32500\n",
      "Train Loss: 0.041738010942935944\n",
      "Average Return: 16.068603515625\n",
      "\n",
      "Iteration: 33000\n",
      "Train Loss: 0.047516465187072754\n",
      "Average Return: 14.5391263961792\n",
      "\n",
      "Iteration: 33500\n",
      "Train Loss: 0.05332295596599579\n",
      "Average Return: 13.99186897277832\n",
      "\n",
      "Iteration: 34000\n",
      "Train Loss: 0.04493582248687744\n",
      "Average Return: 14.997801780700684\n",
      "\n",
      "Iteration: 34500\n",
      "Train Loss: 0.02890853025019169\n",
      "Average Return: 14.882060050964355\n",
      "\n",
      "Iteration: 35000\n",
      "Train Loss: 0.04495865851640701\n",
      "Average Return: 14.086055755615234\n",
      "\n",
      "Iteration: 35500\n",
      "Train Loss: 0.06982748955488205\n",
      "Average Return: 15.1318359375\n",
      "\n",
      "Iteration: 36000\n",
      "Train Loss: 0.0706152692437172\n",
      "Average Return: 16.47022819519043\n",
      "\n",
      "Iteration: 36500\n",
      "Train Loss: 0.049045074731111526\n",
      "Average Return: 14.876964569091797\n",
      "\n",
      "Iteration: 37000\n",
      "Train Loss: 0.05829707160592079\n",
      "Average Return: 15.298691749572754\n",
      "\n",
      "Iteration: 37500\n",
      "Train Loss: 0.02974492311477661\n",
      "Average Return: 15.617188453674316\n",
      "\n",
      "Iteration: 38000\n",
      "Train Loss: 0.06488969922065735\n",
      "Average Return: 15.50561809539795\n",
      "\n",
      "Iteration: 38500\n",
      "Train Loss: 0.06780469417572021\n",
      "Average Return: 14.822375297546387\n",
      "\n",
      "Iteration: 39000\n",
      "Train Loss: 0.043886758387088776\n",
      "Average Return: 15.526101112365723\n",
      "\n",
      "Iteration: 39500\n",
      "Train Loss: 0.03705291450023651\n",
      "Average Return: 15.370845794677734\n",
      "\n",
      "Iteration: 40000\n",
      "Train Loss: 0.05361093953251839\n",
      "Average Return: 15.984271049499512\n",
      "\n",
      "Iteration: 40500\n",
      "Train Loss: 0.056476786732673645\n",
      "Average Return: 14.290549278259277\n",
      "\n",
      "Iteration: 41000\n",
      "Train Loss: 0.040804002434015274\n",
      "Average Return: 13.716079711914062\n",
      "\n",
      "Iteration: 41500\n",
      "Train Loss: 0.07310861349105835\n",
      "Average Return: 15.195987701416016\n",
      "\n",
      "Iteration: 42000\n",
      "Train Loss: 0.03777451813220978\n",
      "Average Return: 15.305458068847656\n",
      "\n",
      "Iteration: 42500\n",
      "Train Loss: 0.08996817469596863\n",
      "Average Return: 14.478373527526855\n",
      "\n",
      "Iteration: 43000\n",
      "Train Loss: 0.05237879976630211\n",
      "Average Return: 13.944493293762207\n",
      "\n",
      "Iteration: 43500\n",
      "Train Loss: 0.04940754920244217\n",
      "Average Return: 13.587197303771973\n",
      "\n",
      "Iteration: 44000\n",
      "Train Loss: 0.0670471265912056\n",
      "Average Return: 12.913115501403809\n",
      "\n",
      "Iteration: 44500\n",
      "Train Loss: 0.04627140983939171\n",
      "Average Return: 14.171948432922363\n",
      "\n",
      "Iteration: 45000\n",
      "Train Loss: 0.045330166816711426\n",
      "Average Return: 14.278231620788574\n",
      "\n",
      "Iteration: 45500\n",
      "Train Loss: 0.05723433196544647\n",
      "Average Return: 12.834579467773438\n",
      "\n",
      "Iteration: 46000\n",
      "Train Loss: 0.036512166261672974\n",
      "Average Return: 14.889885902404785\n",
      "\n",
      "Iteration: 46500\n",
      "Train Loss: 0.04565738886594772\n",
      "Average Return: 14.703871726989746\n",
      "\n",
      "Iteration: 47000\n",
      "Train Loss: 0.03888464346528053\n",
      "Average Return: 13.4169282913208\n",
      "\n",
      "Iteration: 47500\n",
      "Train Loss: 0.035978760570287704\n",
      "Average Return: 13.474946975708008\n",
      "\n",
      "Iteration: 48000\n",
      "Train Loss: 0.03529483079910278\n",
      "Average Return: 14.042680740356445\n",
      "\n",
      "Iteration: 48500\n",
      "Train Loss: 0.03543480485677719\n",
      "Average Return: 14.418628692626953\n",
      "\n",
      "Iteration: 49000\n",
      "Train Loss: 0.06321197003126144\n",
      "Average Return: 12.994242668151855\n",
      "\n",
      "Iteration: 49500\n",
      "Train Loss: 0.047409459948539734\n",
      "Average Return: 14.530744552612305\n",
      "\n",
      "Iteration: 50000\n",
      "Train Loss: 0.03362186625599861\n",
      "Average Return: 14.271580696105957\n",
      "[-0.01534128  0.03474983  0.0055    ]\n",
      "[-0.00219025 -0.00437301  0.0055    ]\n",
      "Collecting Initial Samples...\n",
      "Training has started...\n",
      "\n",
      "New best average return found at 6.747776508331299! Saving checkpoint at iteration 0\n",
      "\n",
      "New best average return found at 10.516430854797363! Saving checkpoint at iteration 500\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 0.0014215190894901752\n",
      "Average Return: 10.516430854797363\n",
      "\n",
      "New best average return found at 10.832916259765625! Saving checkpoint at iteration 1000\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 0.0030967537313699722\n",
      "Average Return: 10.832916259765625\n",
      "\n",
      "Iteration: 1500\n",
      "Train Loss: 0.0021706270053982735\n",
      "Average Return: 8.589308738708496\n",
      "\n",
      "Iteration: 2000\n",
      "Train Loss: 0.0011761983623728156\n",
      "Average Return: 7.768527030944824\n",
      "\n",
      "Iteration: 2500\n",
      "Train Loss: 0.009427033364772797\n",
      "Average Return: 1.0896241664886475\n",
      "\n",
      "Iteration: 3000\n",
      "Train Loss: 0.004330676048994064\n",
      "Average Return: -0.9039548635482788\n",
      "\n",
      "Iteration: 3500\n",
      "Train Loss: 0.0018173169810324907\n",
      "Average Return: 5.346364974975586\n",
      "\n",
      "Iteration: 4000\n",
      "Train Loss: 0.010004562325775623\n",
      "Average Return: 2.9724810123443604\n",
      "\n",
      "New best average return found at 11.866592407226562! Saving checkpoint at iteration 4500\n",
      "\n",
      "Iteration: 4500\n",
      "Train Loss: 0.016900360584259033\n",
      "Average Return: 11.866592407226562\n",
      "\n",
      "Iteration: 5000\n",
      "Train Loss: 0.007075247820466757\n",
      "Average Return: 6.231865882873535\n",
      "\n",
      "Iteration: 5500\n",
      "Train Loss: 0.007222142070531845\n",
      "Average Return: 6.957529067993164\n",
      "\n",
      "Iteration: 6000\n",
      "Train Loss: 0.00860010739415884\n",
      "Average Return: 7.866326332092285\n",
      "\n",
      "Iteration: 6500\n",
      "Train Loss: 0.01255481131374836\n",
      "Average Return: 2.4403350353240967\n",
      "\n",
      "Iteration: 7000\n",
      "Train Loss: 0.016410112380981445\n",
      "Average Return: 10.712008476257324\n",
      "\n",
      "Iteration: 7500\n",
      "Train Loss: 0.016490589827299118\n",
      "Average Return: 6.880502700805664\n",
      "\n",
      "New best average return found at 12.87362289428711! Saving checkpoint at iteration 8000\n",
      "\n",
      "Iteration: 8000\n",
      "Train Loss: 0.009428787045180798\n",
      "Average Return: 12.87362289428711\n",
      "\n",
      "Iteration: 8500\n",
      "Train Loss: 0.009663804434239864\n",
      "Average Return: 9.073282241821289\n",
      "\n",
      "Iteration: 9000\n",
      "Train Loss: 0.028391558676958084\n",
      "Average Return: 7.8156561851501465\n",
      "\n",
      "Iteration: 9500\n",
      "Train Loss: 0.003854516427963972\n",
      "Average Return: 7.451493740081787\n",
      "\n",
      "Iteration: 10000\n",
      "Train Loss: 0.011780958622694016\n",
      "Average Return: 12.288601875305176\n",
      "\n",
      "Iteration: 10500\n",
      "Train Loss: 0.010344975627958775\n",
      "Average Return: 3.8093791007995605\n",
      "\n",
      "Iteration: 11000\n",
      "Train Loss: 0.00870606116950512\n",
      "Average Return: 12.660933494567871\n",
      "\n",
      "Iteration: 11500\n",
      "Train Loss: 0.005642133764922619\n",
      "Average Return: 8.258001327514648\n",
      "\n",
      "Iteration: 12000\n",
      "Train Loss: 0.030651628971099854\n",
      "Average Return: 6.053271770477295\n",
      "\n",
      "Iteration: 12500\n",
      "Train Loss: 0.01681225560605526\n",
      "Average Return: 7.585173606872559\n",
      "\n",
      "Iteration: 13000\n",
      "Train Loss: 0.016596423462033272\n",
      "Average Return: 11.56358528137207\n",
      "\n",
      "Iteration: 13500\n",
      "Train Loss: 0.022604838013648987\n",
      "Average Return: 10.723557472229004\n",
      "\n",
      "New best average return found at 13.240567207336426! Saving checkpoint at iteration 14000\n",
      "\n",
      "Iteration: 14000\n",
      "Train Loss: 0.03665421903133392\n",
      "Average Return: 13.240567207336426\n",
      "\n",
      "Iteration: 14500\n",
      "Train Loss: 0.04843880981206894\n",
      "Average Return: 6.728538990020752\n",
      "\n",
      "Iteration: 15000\n",
      "Train Loss: 0.024166535586118698\n",
      "Average Return: 9.144700050354004\n",
      "\n",
      "Iteration: 15500\n",
      "Train Loss: 0.016606895253062248\n",
      "Average Return: 8.01418399810791\n",
      "\n",
      "Iteration: 16000\n",
      "Train Loss: 0.02367958053946495\n",
      "Average Return: 10.11029052734375\n",
      "\n",
      "Iteration: 16500\n",
      "Train Loss: 0.022755172103643417\n",
      "Average Return: 11.73267936706543\n",
      "\n",
      "Iteration: 17000\n",
      "Train Loss: 0.012719132006168365\n",
      "Average Return: 6.646961688995361\n",
      "\n",
      "Iteration: 17500\n",
      "Train Loss: 0.015154983848333359\n",
      "Average Return: 5.479188919067383\n",
      "\n",
      "Iteration: 18000\n",
      "Train Loss: 0.01293736882507801\n",
      "Average Return: 10.83181095123291\n",
      "\n",
      "Iteration: 18500\n",
      "Train Loss: 0.01729046180844307\n",
      "Average Return: 8.297070503234863\n",
      "\n",
      "Iteration: 19000\n",
      "Train Loss: 0.08624355494976044\n",
      "Average Return: 10.496776580810547\n",
      "\n",
      "Iteration: 19500\n",
      "Train Loss: 0.007554136216640472\n",
      "Average Return: 9.486637115478516\n",
      "\n",
      "Iteration: 20000\n",
      "Train Loss: 0.03743204474449158\n",
      "Average Return: 12.635290145874023\n",
      "\n",
      "Iteration: 20500\n",
      "Train Loss: 0.021181652322411537\n",
      "Average Return: 10.738664627075195\n",
      "\n",
      "Iteration: 21000\n",
      "Train Loss: 0.0123904999345541\n",
      "Average Return: 8.658957481384277\n",
      "\n",
      "Iteration: 21500\n",
      "Train Loss: 0.016650358214974403\n",
      "Average Return: 8.776991844177246\n",
      "\n",
      "Iteration: 22000\n",
      "Train Loss: 0.009698600508272648\n",
      "Average Return: 9.146258354187012\n",
      "\n",
      "Iteration: 22500\n",
      "Train Loss: 0.006606702692806721\n",
      "Average Return: 13.011098861694336\n",
      "\n",
      "Iteration: 23000\n",
      "Train Loss: 0.007468326948583126\n",
      "Average Return: 7.742692947387695\n",
      "\n",
      "Iteration: 23500\n",
      "Train Loss: 0.008643114939332008\n",
      "Average Return: 6.334720134735107\n",
      "\n",
      "Iteration: 24000\n",
      "Train Loss: 0.011593014933168888\n",
      "Average Return: 9.105306625366211\n",
      "\n",
      "Iteration: 24500\n",
      "Train Loss: 0.013707268983125687\n",
      "Average Return: 7.524693489074707\n",
      "\n",
      "Iteration: 25000\n",
      "Train Loss: 0.027044672518968582\n",
      "Average Return: 9.540754318237305\n",
      "\n",
      "Iteration: 25500\n",
      "Train Loss: 0.025205334648489952\n",
      "Average Return: 9.731691360473633\n",
      "\n",
      "Iteration: 26000\n",
      "Train Loss: 0.028788402676582336\n",
      "Average Return: 6.5959086418151855\n",
      "\n",
      "Iteration: 26500\n",
      "Train Loss: 0.023402607068419456\n",
      "Average Return: 7.277412414550781\n",
      "\n",
      "Iteration: 27000\n",
      "Train Loss: 0.02589266374707222\n",
      "Average Return: 6.423729419708252\n",
      "\n",
      "Iteration: 27500\n",
      "Train Loss: 0.03175252676010132\n",
      "Average Return: 7.430108547210693\n",
      "\n",
      "Iteration: 28000\n",
      "Train Loss: 0.03162090852856636\n",
      "Average Return: 7.792376518249512\n",
      "\n",
      "Iteration: 28500\n",
      "Train Loss: 0.021627672016620636\n",
      "Average Return: 6.30924654006958\n",
      "\n",
      "Iteration: 29000\n",
      "Train Loss: 0.01080744806677103\n",
      "Average Return: 7.651496887207031\n",
      "\n",
      "Iteration: 29500\n",
      "Train Loss: 0.037861887365579605\n",
      "Average Return: 7.002650260925293\n",
      "\n",
      "Iteration: 30000\n",
      "Train Loss: 0.023065999150276184\n",
      "Average Return: 7.578276634216309\n",
      "\n",
      "Iteration: 30500\n",
      "Train Loss: 0.02186586707830429\n",
      "Average Return: 9.21262264251709\n",
      "\n",
      "Iteration: 31000\n",
      "Train Loss: 0.018200283870100975\n",
      "Average Return: 9.127423286437988\n",
      "\n",
      "Iteration: 31500\n",
      "Train Loss: 0.026253260672092438\n",
      "Average Return: 9.917839050292969\n",
      "\n",
      "Iteration: 32000\n",
      "Train Loss: 0.011586291715502739\n",
      "Average Return: 5.883276462554932\n",
      "\n",
      "Iteration: 32500\n",
      "Train Loss: 0.019841954112052917\n",
      "Average Return: 9.977078437805176\n",
      "\n",
      "Iteration: 33000\n",
      "Train Loss: 0.015913426876068115\n",
      "Average Return: 7.966765403747559\n",
      "\n",
      "Iteration: 33500\n",
      "Train Loss: 0.019412361085414886\n",
      "Average Return: 10.695587158203125\n",
      "\n",
      "Iteration: 34000\n",
      "Train Loss: 0.05580420419573784\n",
      "Average Return: 8.592058181762695\n",
      "\n",
      "Iteration: 34500\n",
      "Train Loss: 0.014356989413499832\n",
      "Average Return: 8.89930534362793\n",
      "\n",
      "Iteration: 35000\n",
      "Train Loss: 0.031860291957855225\n",
      "Average Return: 9.485207557678223\n",
      "\n",
      "Iteration: 35500\n",
      "Train Loss: 0.017215799540281296\n",
      "Average Return: 8.361896514892578\n",
      "\n",
      "Iteration: 36000\n",
      "Train Loss: 0.02194499410688877\n",
      "Average Return: 6.796107769012451\n",
      "\n",
      "Iteration: 36500\n",
      "Train Loss: 0.013463580049574375\n",
      "Average Return: 9.156170845031738\n",
      "\n",
      "Iteration: 37000\n",
      "Train Loss: 0.023344215005636215\n",
      "Average Return: 12.588356018066406\n",
      "\n",
      "Iteration: 37500\n",
      "Train Loss: 0.017561186105012894\n",
      "Average Return: 8.402509689331055\n",
      "\n",
      "Iteration: 38000\n",
      "Train Loss: 0.021400906145572662\n",
      "Average Return: 7.615683078765869\n",
      "\n",
      "Iteration: 38500\n",
      "Train Loss: 0.02227623015642166\n",
      "Average Return: 7.141514778137207\n",
      "\n",
      "Iteration: 39000\n",
      "Train Loss: 0.01728484034538269\n",
      "Average Return: 7.4286699295043945\n",
      "\n",
      "Iteration: 39500\n",
      "Train Loss: 0.012322461232542992\n",
      "Average Return: 8.073492050170898\n",
      "\n",
      "Iteration: 40000\n",
      "Train Loss: 0.015225880779325962\n",
      "Average Return: 5.269008159637451\n",
      "\n",
      "Iteration: 40500\n",
      "Train Loss: 0.019353169947862625\n",
      "Average Return: 6.365853309631348\n",
      "\n",
      "Iteration: 41000\n",
      "Train Loss: 0.017303962260484695\n",
      "Average Return: 7.775454998016357\n",
      "\n",
      "Iteration: 41500\n",
      "Train Loss: 0.015309866517782211\n",
      "Average Return: 6.835840225219727\n",
      "\n",
      "Iteration: 42000\n",
      "Train Loss: 0.014655710197985172\n",
      "Average Return: 8.72482681274414\n",
      "\n",
      "Iteration: 42500\n",
      "Train Loss: 0.0191037617623806\n",
      "Average Return: 9.082887649536133\n",
      "\n",
      "Iteration: 43000\n",
      "Train Loss: 0.03432248532772064\n",
      "Average Return: 7.729053020477295\n",
      "\n",
      "Iteration: 43500\n",
      "Train Loss: 0.01135143730789423\n",
      "Average Return: 8.835014343261719\n",
      "\n",
      "Iteration: 44000\n",
      "Train Loss: 0.007595680188387632\n",
      "Average Return: 8.107705116271973\n",
      "\n",
      "Iteration: 44500\n",
      "Train Loss: 0.012585503980517387\n",
      "Average Return: 6.021041393280029\n",
      "\n",
      "Iteration: 45000\n",
      "Train Loss: 0.018704332411289215\n",
      "Average Return: 7.941607475280762\n",
      "\n",
      "Iteration: 45500\n",
      "Train Loss: 0.014579781331121922\n",
      "Average Return: 8.763083457946777\n",
      "\n",
      "Iteration: 46000\n",
      "Train Loss: 0.06882024556398392\n",
      "Average Return: 7.080569744110107\n",
      "\n",
      "Iteration: 46500\n",
      "Train Loss: 0.01798582822084427\n",
      "Average Return: 7.743836402893066\n",
      "\n",
      "Iteration: 47000\n",
      "Train Loss: 0.01279834657907486\n",
      "Average Return: 8.146239280700684\n",
      "\n",
      "Iteration: 47500\n",
      "Train Loss: 0.01778249442577362\n",
      "Average Return: 10.211819648742676\n",
      "\n",
      "Iteration: 48000\n",
      "Train Loss: 0.018651800230145454\n",
      "Average Return: 7.7254719734191895\n",
      "\n",
      "Iteration: 48500\n",
      "Train Loss: 0.021676242351531982\n",
      "Average Return: 8.593409538269043\n",
      "\n",
      "Iteration: 49000\n",
      "Train Loss: 0.017993230372667313\n",
      "Average Return: 8.524045944213867\n",
      "\n",
      "Iteration: 49500\n",
      "Train Loss: 0.016862086951732635\n",
      "Average Return: 10.327404975891113\n",
      "\n",
      "Iteration: 50000\n",
      "Train Loss: 0.022435050457715988\n",
      "Average Return: 7.411862850189209\n",
      "[-0.00696099  0.03875525  0.0055    ]\n",
      "[ 0.03491753 -0.00145815  0.0055    ]\n",
      "Collecting Initial Samples...\n",
      "Training has started...\n",
      "\n",
      "New best average return found at 22.36600112915039! Saving checkpoint at iteration 0\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 0.0023241136223077774\n",
      "Average Return: 19.2887020111084\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 0.010655400343239307\n",
      "Average Return: 16.760774612426758\n",
      "\n",
      "Iteration: 1500\n",
      "Train Loss: 0.003920241724699736\n",
      "Average Return: 17.662067413330078\n",
      "\n",
      "Iteration: 2000\n",
      "Train Loss: 0.006619035266339779\n",
      "Average Return: 12.988914489746094\n",
      "\n",
      "Iteration: 2500\n",
      "Train Loss: 0.028548751026391983\n",
      "Average Return: 18.14165687561035\n",
      "\n",
      "Iteration: 3000\n",
      "Train Loss: 0.012802058830857277\n",
      "Average Return: 16.203182220458984\n",
      "\n",
      "Iteration: 3500\n",
      "Train Loss: 0.01099378801882267\n",
      "Average Return: 17.99489974975586\n",
      "\n",
      "Iteration: 4000\n",
      "Train Loss: 0.06847095489501953\n",
      "Average Return: 16.980688095092773\n",
      "\n",
      "Iteration: 4500\n",
      "Train Loss: 0.019991524517536163\n",
      "Average Return: 19.79362678527832\n",
      "\n",
      "Iteration: 5000\n",
      "Train Loss: 0.013364473357796669\n",
      "Average Return: 18.713468551635742\n",
      "\n",
      "Iteration: 5500\n",
      "Train Loss: 0.03519001975655556\n",
      "Average Return: 19.277456283569336\n",
      "\n",
      "Iteration: 6000\n",
      "Train Loss: 0.02618907392024994\n",
      "Average Return: 19.55780029296875\n",
      "\n",
      "Iteration: 6500\n",
      "Train Loss: 0.03524760901927948\n",
      "Average Return: 17.475120544433594\n",
      "\n",
      "Iteration: 7000\n",
      "Train Loss: 0.027735430747270584\n",
      "Average Return: 15.739130973815918\n",
      "\n",
      "Iteration: 7500\n",
      "Train Loss: 0.04856100678443909\n",
      "Average Return: 15.213380813598633\n",
      "\n",
      "Iteration: 8000\n",
      "Train Loss: 0.038933441042900085\n",
      "Average Return: 17.047367095947266\n",
      "\n",
      "Iteration: 8500\n",
      "Train Loss: 0.03884875401854515\n",
      "Average Return: 19.90680694580078\n",
      "\n",
      "Iteration: 9000\n",
      "Train Loss: 0.0564546100795269\n",
      "Average Return: 15.829584121704102\n",
      "\n",
      "Iteration: 9500\n",
      "Train Loss: 0.01571667194366455\n",
      "Average Return: 17.058629989624023\n",
      "\n",
      "Iteration: 10000\n",
      "Train Loss: 0.025617003440856934\n",
      "Average Return: 20.675628662109375\n",
      "\n",
      "Iteration: 10500\n",
      "Train Loss: 0.023716621100902557\n",
      "Average Return: 16.154504776000977\n",
      "\n",
      "Iteration: 11000\n",
      "Train Loss: 0.020871799439191818\n",
      "Average Return: 16.742610931396484\n",
      "\n",
      "Iteration: 11500\n",
      "Train Loss: 0.032765891402959824\n",
      "Average Return: 20.118091583251953\n",
      "\n",
      "Iteration: 12000\n",
      "Train Loss: 0.13097509741783142\n",
      "Average Return: 16.203937530517578\n",
      "\n",
      "Iteration: 12500\n",
      "Train Loss: 0.09468680620193481\n",
      "Average Return: 18.99445343017578\n",
      "\n",
      "Iteration: 13000\n",
      "Train Loss: 0.052360765635967255\n",
      "Average Return: 19.8094539642334\n",
      "\n",
      "Iteration: 13500\n",
      "Train Loss: 0.058759063482284546\n",
      "Average Return: 21.87427520751953\n",
      "\n",
      "Iteration: 14000\n",
      "Train Loss: 0.058224424719810486\n",
      "Average Return: 22.135732650756836\n",
      "\n",
      "Iteration: 14500\n",
      "Train Loss: 0.04845454916357994\n",
      "Average Return: 20.079858779907227\n",
      "\n",
      "Iteration: 15000\n",
      "Train Loss: 0.0857621282339096\n",
      "Average Return: 18.897075653076172\n",
      "\n",
      "Iteration: 15500\n",
      "Train Loss: 0.05340609326958656\n",
      "Average Return: 20.464557647705078\n",
      "\n",
      "Iteration: 16000\n",
      "Train Loss: 0.07889268547296524\n",
      "Average Return: 19.636096954345703\n",
      "\n",
      "Iteration: 16500\n",
      "Train Loss: 0.03969145566225052\n",
      "Average Return: 19.453880310058594\n",
      "\n",
      "Iteration: 17000\n",
      "Train Loss: 0.03575648367404938\n",
      "Average Return: 20.61193084716797\n",
      "\n",
      "Iteration: 17500\n",
      "Train Loss: 0.025219522416591644\n",
      "Average Return: 18.445524215698242\n",
      "\n",
      "Iteration: 18000\n",
      "Train Loss: 0.052331142127513885\n",
      "Average Return: 19.44684600830078\n",
      "\n",
      "Iteration: 18500\n",
      "Train Loss: 0.053864702582359314\n",
      "Average Return: 19.23542594909668\n",
      "\n",
      "Iteration: 19000\n",
      "Train Loss: 0.06744739413261414\n",
      "Average Return: 16.611831665039062\n",
      "\n",
      "Iteration: 19500\n",
      "Train Loss: 0.030143985524773598\n",
      "Average Return: 17.904682159423828\n",
      "\n",
      "Iteration: 20000\n",
      "Train Loss: 0.11131320893764496\n",
      "Average Return: 17.015539169311523\n",
      "\n",
      "Iteration: 20500\n",
      "Train Loss: 0.07907824218273163\n",
      "Average Return: 18.533475875854492\n",
      "\n",
      "Iteration: 21000\n",
      "Train Loss: 0.06837072968482971\n",
      "Average Return: 16.07004737854004\n",
      "\n",
      "Iteration: 21500\n",
      "Train Loss: 0.06467822194099426\n",
      "Average Return: 17.51568603515625\n",
      "\n",
      "Iteration: 22000\n",
      "Train Loss: 0.02796311117708683\n",
      "Average Return: 16.3829345703125\n",
      "\n",
      "Iteration: 22500\n",
      "Train Loss: 0.016156304627656937\n",
      "Average Return: 14.472763061523438\n",
      "\n",
      "Iteration: 23000\n",
      "Train Loss: 0.022449497133493423\n",
      "Average Return: 16.418983459472656\n",
      "\n",
      "Iteration: 23500\n",
      "Train Loss: 0.018457673490047455\n",
      "Average Return: 16.68231201171875\n",
      "\n",
      "Iteration: 24000\n",
      "Train Loss: 0.04010704904794693\n",
      "Average Return: 16.13222885131836\n",
      "\n",
      "Iteration: 24500\n",
      "Train Loss: 0.05094200000166893\n",
      "Average Return: 14.794672966003418\n",
      "\n",
      "Iteration: 25000\n",
      "Train Loss: 0.06402188539505005\n",
      "Average Return: 17.252904891967773\n",
      "\n",
      "Iteration: 25500\n",
      "Train Loss: 0.052339181303977966\n",
      "Average Return: 16.592758178710938\n",
      "\n",
      "Iteration: 26000\n",
      "Train Loss: 0.05093863978981972\n",
      "Average Return: 15.689143180847168\n",
      "\n",
      "Iteration: 26500\n",
      "Train Loss: 0.06692485511302948\n",
      "Average Return: 16.397043228149414\n",
      "\n",
      "Iteration: 27000\n",
      "Train Loss: 0.06714577972888947\n",
      "Average Return: 16.771257400512695\n",
      "\n",
      "Iteration: 27500\n",
      "Train Loss: 0.08218240737915039\n",
      "Average Return: 17.69671058654785\n",
      "\n",
      "Iteration: 28000\n",
      "Train Loss: 0.05106520652770996\n",
      "Average Return: 16.68661880493164\n",
      "\n",
      "Iteration: 28500\n",
      "Train Loss: 0.041852544993162155\n",
      "Average Return: 15.250639915466309\n",
      "\n",
      "Iteration: 29000\n",
      "Train Loss: 0.04997452348470688\n",
      "Average Return: 17.045032501220703\n",
      "\n",
      "Iteration: 29500\n",
      "Train Loss: 0.05277874693274498\n",
      "Average Return: 18.670103073120117\n",
      "\n",
      "Iteration: 30000\n",
      "Train Loss: 0.032114237546920776\n",
      "Average Return: 19.06519889831543\n",
      "\n",
      "Iteration: 30500\n",
      "Train Loss: 0.06460288166999817\n",
      "Average Return: 20.45799446105957\n",
      "\n",
      "Iteration: 31000\n",
      "Train Loss: 0.06227840110659599\n",
      "Average Return: 19.33248519897461\n",
      "\n",
      "Iteration: 31500\n",
      "Train Loss: 0.041655804961919785\n",
      "Average Return: 20.12526512145996\n",
      "\n",
      "Iteration: 32000\n",
      "Train Loss: 0.05381426960229874\n",
      "Average Return: 18.198013305664062\n",
      "\n",
      "Iteration: 32500\n",
      "Train Loss: 0.029798591509461403\n",
      "Average Return: 18.937763214111328\n",
      "\n",
      "Iteration: 33000\n",
      "Train Loss: 0.04042675346136093\n",
      "Average Return: 18.59891700744629\n",
      "\n",
      "Iteration: 33500\n",
      "Train Loss: 0.04661071300506592\n",
      "Average Return: 17.55858039855957\n",
      "\n",
      "Iteration: 34000\n",
      "Train Loss: 0.11327040195465088\n",
      "Average Return: 20.118310928344727\n",
      "\n",
      "Iteration: 34500\n",
      "Train Loss: 0.0473458357155323\n",
      "Average Return: 18.4605655670166\n",
      "\n",
      "Iteration: 35000\n",
      "Train Loss: 0.09104878455400467\n",
      "Average Return: 17.407405853271484\n",
      "\n",
      "Iteration: 35500\n",
      "Train Loss: 0.05004747211933136\n",
      "Average Return: 20.74947166442871\n",
      "\n",
      "Iteration: 36000\n",
      "Train Loss: 0.051272228360176086\n",
      "Average Return: 20.38264274597168\n",
      "\n",
      "Iteration: 36500\n",
      "Train Loss: 0.04239245876669884\n",
      "Average Return: 18.75084114074707\n",
      "\n",
      "Iteration: 37000\n",
      "Train Loss: 0.04179760813713074\n",
      "Average Return: 18.22861099243164\n",
      "\n",
      "Iteration: 37500\n",
      "Train Loss: 0.05802509933710098\n",
      "Average Return: 20.955957412719727\n",
      "\n",
      "Iteration: 38000\n",
      "Train Loss: 0.05274929106235504\n",
      "Average Return: 19.074960708618164\n",
      "\n",
      "Iteration: 38500\n",
      "Train Loss: 0.0676247775554657\n",
      "Average Return: 18.752647399902344\n",
      "\n",
      "Iteration: 39000\n",
      "Train Loss: 0.0347045361995697\n",
      "Average Return: 21.913354873657227\n",
      "\n",
      "Iteration: 39500\n",
      "Train Loss: 0.03257244825363159\n",
      "Average Return: 19.593795776367188\n",
      "\n",
      "Iteration: 40000\n",
      "Train Loss: 0.02600260265171528\n",
      "Average Return: 19.11810874938965\n",
      "\n",
      "Iteration: 40500\n",
      "Train Loss: 0.04535418748855591\n",
      "Average Return: 19.163206100463867\n",
      "\n",
      "Iteration: 41000\n",
      "Train Loss: 0.056466177105903625\n",
      "Average Return: 18.431922912597656\n",
      "\n",
      "Iteration: 41500\n",
      "Train Loss: 0.03208392858505249\n",
      "Average Return: 18.860519409179688\n",
      "\n",
      "Iteration: 42000\n",
      "Train Loss: 0.03868001326918602\n",
      "Average Return: 18.56111717224121\n",
      "\n",
      "Iteration: 42500\n",
      "Train Loss: 0.02823144569993019\n",
      "Average Return: 18.68071746826172\n",
      "\n",
      "Iteration: 43000\n",
      "Train Loss: 0.04566056281328201\n",
      "Average Return: 18.64871597290039\n",
      "\n",
      "Iteration: 43500\n",
      "Train Loss: 0.03907673805952072\n",
      "Average Return: 18.344493865966797\n",
      "\n",
      "Iteration: 44000\n",
      "Train Loss: 0.025758372619748116\n",
      "Average Return: 21.223220825195312\n",
      "\n",
      "Iteration: 44500\n",
      "Train Loss: 0.032426219433546066\n",
      "Average Return: 19.121421813964844\n",
      "\n",
      "Iteration: 45000\n",
      "Train Loss: 0.027822259813547134\n",
      "Average Return: 20.249149322509766\n",
      "\n",
      "Iteration: 45500\n",
      "Train Loss: 0.029813962057232857\n",
      "Average Return: 20.64830207824707\n",
      "\n",
      "Iteration: 46000\n",
      "Train Loss: 0.1064271330833435\n",
      "Average Return: 18.66703224182129\n",
      "\n",
      "Iteration: 46500\n",
      "Train Loss: 0.031302571296691895\n",
      "Average Return: 19.41950035095215\n",
      "\n",
      "Iteration: 47000\n",
      "Train Loss: 0.022997383028268814\n",
      "Average Return: 17.84726333618164\n",
      "\n",
      "Iteration: 47500\n",
      "Train Loss: 0.044488392770290375\n",
      "Average Return: 19.978958129882812\n",
      "\n",
      "Iteration: 48000\n",
      "Train Loss: 0.048754990100860596\n",
      "Average Return: 19.24829864501953\n",
      "\n",
      "Iteration: 48500\n",
      "Train Loss: 0.03197433799505234\n",
      "Average Return: 20.759897232055664\n",
      "\n",
      "Iteration: 49000\n",
      "Train Loss: 0.031184911727905273\n",
      "Average Return: 20.284921646118164\n",
      "\n",
      "Iteration: 49500\n",
      "Train Loss: 0.05293848365545273\n",
      "Average Return: 19.882389068603516\n",
      "\n",
      "Iteration: 50000\n",
      "Train Loss: 0.03264562413096428\n",
      "Average Return: 20.26991844177246\n",
      "[-0.00702677  0.01057561  0.0055    ]\n",
      "[-0.00560453  0.00925274  0.0055    ]\n",
      "Collecting Initial Samples...\n",
      "Training has started...\n",
      "\n",
      "New best average return found at -0.9471167325973511! Saving checkpoint at iteration 0\n",
      "\n",
      "New best average return found at 1.6434963941574097! Saving checkpoint at iteration 500\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 0.003219506936147809\n",
      "Average Return: 1.6434963941574097\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 0.0031786304898560047\n",
      "Average Return: -0.5969918370246887\n",
      "\n",
      "Iteration: 1500\n",
      "Train Loss: 0.0019472367130219936\n",
      "Average Return: -0.07595354318618774\n",
      "\n",
      "Iteration: 2000\n",
      "Train Loss: 0.0013705348828807473\n",
      "Average Return: 1.2539260387420654\n",
      "\n",
      "New best average return found at 7.452581405639648! Saving checkpoint at iteration 2500\n",
      "\n",
      "Iteration: 2500\n",
      "Train Loss: 0.004540863446891308\n",
      "Average Return: 7.452581405639648\n",
      "\n",
      "New best average return found at 10.990928649902344! Saving checkpoint at iteration 3000\n",
      "\n",
      "Iteration: 3000\n",
      "Train Loss: 0.008610133081674576\n",
      "Average Return: 10.990928649902344\n",
      "\n",
      "New best average return found at 12.013711929321289! Saving checkpoint at iteration 3500\n",
      "\n",
      "Iteration: 3500\n",
      "Train Loss: 0.0017907795263454318\n",
      "Average Return: 12.013711929321289\n",
      "\n",
      "Iteration: 4000\n",
      "Train Loss: 0.003121432149782777\n",
      "Average Return: -5.837356090545654\n",
      "\n",
      "Iteration: 4500\n",
      "Train Loss: 0.0031094211153686047\n",
      "Average Return: 8.20291805267334\n",
      "\n",
      "Iteration: 5000\n",
      "Train Loss: 0.003443135181441903\n",
      "Average Return: 0.46434590220451355\n",
      "\n",
      "Iteration: 5500\n",
      "Train Loss: 0.0031894694548100233\n",
      "Average Return: -2.326495409011841\n",
      "\n",
      "Iteration: 6000\n",
      "Train Loss: 0.01850125379860401\n",
      "Average Return: 1.7189058065414429\n",
      "\n",
      "Iteration: 6500\n",
      "Train Loss: 0.021759888157248497\n",
      "Average Return: 0.40361398458480835\n",
      "\n",
      "Iteration: 7000\n",
      "Train Loss: 0.006510820705443621\n",
      "Average Return: 2.3592536449432373\n",
      "\n",
      "Iteration: 7500\n",
      "Train Loss: 0.006359705701470375\n",
      "Average Return: 6.238558292388916\n",
      "\n",
      "Iteration: 8000\n",
      "Train Loss: 0.009970523416996002\n",
      "Average Return: 1.7728976011276245\n",
      "\n",
      "Iteration: 8500\n",
      "Train Loss: 0.006191767752170563\n",
      "Average Return: 3.3232438564300537\n",
      "\n",
      "Iteration: 9000\n",
      "Train Loss: 0.01784364879131317\n",
      "Average Return: 11.053607940673828\n",
      "\n",
      "Iteration: 9500\n",
      "Train Loss: 0.007589605636894703\n",
      "Average Return: 2.7738656997680664\n",
      "\n",
      "Iteration: 10000\n",
      "Train Loss: 0.005840562749654055\n",
      "Average Return: -0.08278101682662964\n",
      "\n",
      "Iteration: 10500\n",
      "Train Loss: 0.016326311975717545\n",
      "Average Return: 10.908937454223633\n",
      "\n",
      "Iteration: 11000\n",
      "Train Loss: 0.0087056839838624\n",
      "Average Return: 2.051992177963257\n",
      "\n",
      "Iteration: 11500\n",
      "Train Loss: 0.005051480606198311\n",
      "Average Return: 3.9150044918060303\n",
      "\n",
      "Iteration: 12000\n",
      "Train Loss: 0.01005600392818451\n",
      "Average Return: 5.668182849884033\n",
      "\n",
      "Iteration: 12500\n",
      "Train Loss: 0.007280214224010706\n",
      "Average Return: 3.413785457611084\n",
      "\n",
      "Iteration: 13000\n",
      "Train Loss: 0.0050965845584869385\n",
      "Average Return: 9.215442657470703\n",
      "\n",
      "Iteration: 13500\n",
      "Train Loss: 0.016534611582756042\n",
      "Average Return: 6.696802616119385\n",
      "\n",
      "Iteration: 14000\n",
      "Train Loss: 0.01019954401999712\n",
      "Average Return: -0.03243117034435272\n",
      "\n",
      "Iteration: 14500\n",
      "Train Loss: 0.00667787017300725\n",
      "Average Return: 1.1040973663330078\n",
      "\n",
      "Iteration: 15000\n",
      "Train Loss: 0.008010543882846832\n",
      "Average Return: 10.174077033996582\n",
      "\n",
      "Iteration: 15500\n",
      "Train Loss: 0.01899411529302597\n",
      "Average Return: 2.2063419818878174\n",
      "\n",
      "Iteration: 16000\n",
      "Train Loss: 0.010818996466696262\n",
      "Average Return: 1.8656535148620605\n",
      "\n",
      "Iteration: 16500\n",
      "Train Loss: 0.00662436755374074\n",
      "Average Return: -1.2532575130462646\n",
      "\n",
      "Iteration: 17000\n",
      "Train Loss: 0.017753880470991135\n",
      "Average Return: 1.9758238792419434\n",
      "\n",
      "Iteration: 17500\n",
      "Train Loss: 0.013497771695256233\n",
      "Average Return: -1.8979560136795044\n",
      "\n",
      "Iteration: 18000\n",
      "Train Loss: 0.018066925927996635\n",
      "Average Return: 3.800783395767212\n",
      "\n",
      "Iteration: 18500\n",
      "Train Loss: 0.018864095211029053\n",
      "Average Return: 2.199921131134033\n",
      "\n",
      "Iteration: 19000\n",
      "Train Loss: 0.013497172854840755\n",
      "Average Return: 2.104809045791626\n",
      "\n",
      "Iteration: 19500\n",
      "Train Loss: 0.013110987842082977\n",
      "Average Return: 0.6630869507789612\n",
      "\n",
      "Iteration: 20000\n",
      "Train Loss: 0.012167800217866898\n",
      "Average Return: 1.508213758468628\n",
      "\n",
      "Iteration: 20500\n",
      "Train Loss: 0.016210565343499184\n",
      "Average Return: 3.81240177154541\n",
      "\n",
      "Iteration: 21000\n",
      "Train Loss: 0.007037795148789883\n",
      "Average Return: -0.04329865425825119\n",
      "\n",
      "Iteration: 21500\n",
      "Train Loss: 0.034036412835121155\n",
      "Average Return: 6.569337368011475\n",
      "\n",
      "Iteration: 22000\n",
      "Train Loss: 0.042432621121406555\n",
      "Average Return: 1.6008756160736084\n",
      "\n",
      "Iteration: 22500\n",
      "Train Loss: 0.009804296307265759\n",
      "Average Return: 4.2097578048706055\n",
      "\n",
      "Iteration: 23000\n",
      "Train Loss: 0.017269346863031387\n",
      "Average Return: 4.791322231292725\n",
      "\n",
      "Iteration: 23500\n",
      "Train Loss: 0.01596967875957489\n",
      "Average Return: 1.1018437147140503\n",
      "\n",
      "Iteration: 24000\n",
      "Train Loss: 0.04256119951605797\n",
      "Average Return: -2.7837119102478027\n",
      "\n",
      "Iteration: 24500\n",
      "Train Loss: 0.016363663598895073\n",
      "Average Return: 0.9914593696594238\n",
      "\n",
      "Iteration: 25000\n",
      "Train Loss: 0.010907821357250214\n",
      "Average Return: 0.1472967118024826\n",
      "\n",
      "Iteration: 25500\n",
      "Train Loss: 0.02079925686120987\n",
      "Average Return: 0.5804553627967834\n",
      "\n",
      "Iteration: 26000\n",
      "Train Loss: 0.008674107491970062\n",
      "Average Return: 5.063954830169678\n",
      "\n",
      "Iteration: 26500\n",
      "Train Loss: 0.023364126682281494\n",
      "Average Return: 1.9226921796798706\n",
      "\n",
      "Iteration: 27000\n",
      "Train Loss: 0.01274428516626358\n",
      "Average Return: 6.919699668884277\n",
      "\n",
      "Iteration: 27500\n",
      "Train Loss: 0.017015226185321808\n",
      "Average Return: 4.5882039070129395\n",
      "\n",
      "Iteration: 28000\n",
      "Train Loss: 0.05100155249238014\n",
      "Average Return: 2.0221219062805176\n",
      "\n",
      "Iteration: 28500\n",
      "Train Loss: 0.014928546734154224\n",
      "Average Return: 0.2789748013019562\n",
      "\n",
      "Iteration: 29000\n",
      "Train Loss: 0.009967846795916557\n",
      "Average Return: 5.198424816131592\n",
      "\n",
      "Iteration: 29500\n",
      "Train Loss: 0.010862911120057106\n",
      "Average Return: -2.22284197807312\n",
      "\n",
      "Iteration: 30000\n",
      "Train Loss: 0.026612456887960434\n",
      "Average Return: 7.287075042724609\n",
      "\n",
      "Iteration: 30500\n",
      "Train Loss: 0.020827077329158783\n",
      "Average Return: 4.913759231567383\n",
      "\n",
      "Iteration: 31000\n",
      "Train Loss: 0.008828403428196907\n",
      "Average Return: 6.530497074127197\n",
      "\n",
      "Iteration: 31500\n",
      "Train Loss: 0.04539540037512779\n",
      "Average Return: 1.747388482093811\n",
      "\n",
      "Iteration: 32000\n",
      "Train Loss: 0.021054163575172424\n",
      "Average Return: 4.197016716003418\n",
      "\n",
      "Iteration: 32500\n",
      "Train Loss: 0.02395506575703621\n",
      "Average Return: 4.30658483505249\n",
      "\n",
      "Iteration: 33000\n",
      "Train Loss: 0.01145027857273817\n",
      "Average Return: 5.728306770324707\n",
      "\n",
      "Iteration: 33500\n",
      "Train Loss: 0.02696787565946579\n",
      "Average Return: 9.771988868713379\n",
      "\n",
      "Iteration: 34000\n",
      "Train Loss: 0.023197859525680542\n",
      "Average Return: 6.442987442016602\n",
      "\n",
      "Iteration: 34500\n",
      "Train Loss: 0.015166716650128365\n",
      "Average Return: 4.138990879058838\n",
      "\n",
      "Iteration: 35000\n",
      "Train Loss: 0.016969965770840645\n",
      "Average Return: 4.320252895355225\n",
      "\n",
      "Iteration: 35500\n",
      "Train Loss: 0.03605356812477112\n",
      "Average Return: 3.4966609477996826\n",
      "\n",
      "Iteration: 36000\n",
      "Train Loss: 0.01916082203388214\n",
      "Average Return: 2.6902236938476562\n",
      "\n",
      "Iteration: 36500\n",
      "Train Loss: 0.040422871708869934\n",
      "Average Return: 3.673194408416748\n",
      "\n",
      "Iteration: 37000\n",
      "Train Loss: 0.035983167588710785\n",
      "Average Return: 1.2154133319854736\n",
      "\n",
      "Iteration: 37500\n",
      "Train Loss: 0.022987477481365204\n",
      "Average Return: 3.870565414428711\n",
      "\n",
      "Iteration: 38000\n",
      "Train Loss: 0.024873722344636917\n",
      "Average Return: 3.6348049640655518\n",
      "\n",
      "Iteration: 38500\n",
      "Train Loss: 0.02676505781710148\n",
      "Average Return: 1.3549884557724\n",
      "\n",
      "Iteration: 39000\n",
      "Train Loss: 0.020916547626256943\n",
      "Average Return: 1.6371690034866333\n",
      "\n",
      "Iteration: 39500\n",
      "Train Loss: 0.016332296654582024\n",
      "Average Return: 4.229336738586426\n",
      "\n",
      "Iteration: 40000\n",
      "Train Loss: 0.015354126691818237\n",
      "Average Return: 2.762901782989502\n",
      "\n",
      "Iteration: 40500\n",
      "Train Loss: 0.01603475585579872\n",
      "Average Return: 5.428163528442383\n",
      "\n",
      "Iteration: 41000\n",
      "Train Loss: 0.012109700590372086\n",
      "Average Return: 2.891171932220459\n",
      "\n",
      "Iteration: 41500\n",
      "Train Loss: 0.010907929390668869\n",
      "Average Return: 2.9703261852264404\n",
      "\n",
      "Iteration: 42000\n",
      "Train Loss: 0.013162821531295776\n",
      "Average Return: 2.587985038757324\n",
      "\n",
      "Iteration: 42500\n",
      "Train Loss: 0.021621098741889\n",
      "Average Return: 4.673980236053467\n",
      "\n",
      "Iteration: 43000\n",
      "Train Loss: 0.02406708151102066\n",
      "Average Return: 2.989905595779419\n",
      "\n",
      "Iteration: 43500\n",
      "Train Loss: 0.020323950797319412\n",
      "Average Return: 4.901907920837402\n",
      "\n",
      "Iteration: 44000\n",
      "Train Loss: 0.011701852083206177\n",
      "Average Return: 2.146012783050537\n",
      "\n",
      "Iteration: 44500\n",
      "Train Loss: 0.013346865773200989\n",
      "Average Return: 1.3285727500915527\n",
      "\n",
      "Iteration: 45000\n",
      "Train Loss: 0.012188433669507504\n",
      "Average Return: 2.8518848419189453\n",
      "\n",
      "Iteration: 45500\n",
      "Train Loss: 0.011758720502257347\n",
      "Average Return: 1.2436771392822266\n",
      "\n",
      "Iteration: 46000\n",
      "Train Loss: 0.03479244187474251\n",
      "Average Return: 2.636528730392456\n",
      "\n",
      "Iteration: 46500\n",
      "Train Loss: 0.017461635172367096\n",
      "Average Return: 4.757403373718262\n",
      "\n",
      "Iteration: 47000\n",
      "Train Loss: 0.008582358248531818\n",
      "Average Return: 2.023815870285034\n",
      "\n",
      "Iteration: 47500\n",
      "Train Loss: 0.012800371274352074\n",
      "Average Return: 4.09764289855957\n",
      "\n",
      "Iteration: 48000\n",
      "Train Loss: 0.024038834497332573\n",
      "Average Return: 5.863907814025879\n",
      "\n",
      "Iteration: 48500\n",
      "Train Loss: 0.025160297751426697\n",
      "Average Return: 3.87622332572937\n",
      "\n",
      "Iteration: 49000\n",
      "Train Loss: 0.027158811688423157\n",
      "Average Return: 3.515838623046875\n",
      "\n",
      "Iteration: 49500\n",
      "Train Loss: 0.02940801903605461\n",
      "Average Return: 3.720283269882202\n",
      "\n",
      "Iteration: 50000\n",
      "Train Loss: 0.016503730788826942\n",
      "Average Return: 4.751126766204834\n",
      "[0.01415272 0.02214792 0.0055    ]\n",
      "[0.00109896 0.01435984 0.0055    ]\n",
      "Collecting Initial Samples...\n",
      "Training has started...\n",
      "\n",
      "New best average return found at 12.827679634094238! Saving checkpoint at iteration 0\n",
      "\n",
      "New best average return found at 14.783758163452148! Saving checkpoint at iteration 500\n",
      "\n",
      "Iteration: 500\n",
      "Train Loss: 0.004321601241827011\n",
      "Average Return: 14.783758163452148\n",
      "\n",
      "Iteration: 1000\n",
      "Train Loss: 0.004018357023596764\n",
      "Average Return: 12.592541694641113\n",
      "\n",
      "Iteration: 1500\n",
      "Train Loss: 0.0022404242772608995\n",
      "Average Return: 12.62445068359375\n",
      "\n",
      "Iteration: 2000\n",
      "Train Loss: 0.004481286276131868\n",
      "Average Return: 12.74549388885498\n",
      "\n",
      "Iteration: 2500\n",
      "Train Loss: 0.01058683916926384\n",
      "Average Return: 12.505008697509766\n",
      "\n",
      "Iteration: 3000\n",
      "Train Loss: 0.01735054701566696\n",
      "Average Return: 12.017404556274414\n",
      "\n",
      "New best average return found at 15.203051567077637! Saving checkpoint at iteration 3500\n",
      "\n",
      "Iteration: 3500\n",
      "Train Loss: 0.00631334213539958\n",
      "Average Return: 15.203051567077637\n",
      "\n",
      "Iteration: 4000\n",
      "Train Loss: 0.011137370020151138\n",
      "Average Return: 9.727149963378906\n",
      "\n",
      "Iteration: 4500\n",
      "Train Loss: 0.009837275370955467\n",
      "Average Return: 13.517986297607422\n",
      "\n",
      "Iteration: 5000\n",
      "Train Loss: 0.006725236773490906\n",
      "Average Return: 12.408409118652344\n",
      "\n",
      "Iteration: 5500\n",
      "Train Loss: 0.008289461955428123\n",
      "Average Return: 13.227662086486816\n",
      "\n",
      "Iteration: 6000\n",
      "Train Loss: 0.038602352142333984\n",
      "Average Return: 13.514925003051758\n",
      "\n",
      "Iteration: 6500\n",
      "Train Loss: 0.03484727442264557\n",
      "Average Return: 10.342228889465332\n",
      "\n",
      "Iteration: 7000\n",
      "Train Loss: 0.019851045683026314\n",
      "Average Return: 11.91082763671875\n",
      "\n",
      "Iteration: 7500\n",
      "Train Loss: 0.01034765224903822\n",
      "Average Return: 12.088983535766602\n",
      "\n",
      "Iteration: 8000\n",
      "Train Loss: 0.04454634338617325\n",
      "Average Return: 12.014832496643066\n",
      "\n",
      "Iteration: 8500\n",
      "Train Loss: 0.02203931286931038\n",
      "Average Return: 12.698625564575195\n",
      "\n",
      "Iteration: 9000\n",
      "Train Loss: 0.03939010202884674\n",
      "Average Return: 10.696942329406738\n",
      "\n",
      "Iteration: 9500\n",
      "Train Loss: 0.033097926527261734\n",
      "Average Return: 10.567728996276855\n",
      "\n",
      "Iteration: 10000\n",
      "Train Loss: 0.02045123651623726\n",
      "Average Return: 12.082313537597656\n",
      "\n",
      "Iteration: 10500\n",
      "Train Loss: 0.047044359147548676\n",
      "Average Return: 11.151820182800293\n",
      "\n",
      "Iteration: 11000\n",
      "Train Loss: 0.023692000657320023\n",
      "Average Return: 9.840096473693848\n",
      "\n",
      "Iteration: 11500\n",
      "Train Loss: 0.012706519104540348\n",
      "Average Return: 11.282443046569824\n",
      "\n",
      "Iteration: 12000\n",
      "Train Loss: 0.030355803668498993\n",
      "Average Return: 11.580157279968262\n",
      "\n",
      "Iteration: 12500\n",
      "Train Loss: 0.022876393049955368\n",
      "Average Return: 9.537688255310059\n",
      "\n",
      "Iteration: 13000\n",
      "Train Loss: 0.012401582673192024\n",
      "Average Return: 11.86578369140625\n",
      "\n",
      "Iteration: 13500\n",
      "Train Loss: 0.04003974422812462\n",
      "Average Return: 10.783052444458008\n",
      "\n",
      "Iteration: 14000\n",
      "Train Loss: 0.040780209004879\n",
      "Average Return: 11.030410766601562\n",
      "\n",
      "Iteration: 14500\n",
      "Train Loss: 0.010803582146763802\n",
      "Average Return: 12.003902435302734\n",
      "\n",
      "Iteration: 15000\n",
      "Train Loss: 0.034498430788517\n",
      "Average Return: 11.524162292480469\n",
      "\n",
      "Iteration: 15500\n",
      "Train Loss: 0.0553700253367424\n",
      "Average Return: 10.191627502441406\n",
      "\n",
      "Iteration: 16000\n",
      "Train Loss: 0.03237643092870712\n",
      "Average Return: 10.887598991394043\n",
      "\n",
      "Iteration: 16500\n",
      "Train Loss: 0.01743544451892376\n",
      "Average Return: 11.112170219421387\n",
      "\n",
      "Iteration: 17000\n",
      "Train Loss: 0.04939386621117592\n",
      "Average Return: 10.825931549072266\n",
      "\n",
      "Iteration: 17500\n",
      "Train Loss: 0.07485759258270264\n",
      "Average Return: 12.90882396697998\n",
      "\n",
      "Iteration: 18000\n",
      "Train Loss: 0.034465838223695755\n",
      "Average Return: 11.9978609085083\n",
      "\n",
      "Iteration: 18500\n",
      "Train Loss: 0.09442149102687836\n",
      "Average Return: 12.876579284667969\n",
      "\n",
      "Iteration: 19000\n",
      "Train Loss: 0.02458348497748375\n",
      "Average Return: 13.379129409790039\n",
      "\n",
      "Iteration: 19500\n",
      "Train Loss: 0.024709045886993408\n",
      "Average Return: 11.353768348693848\n",
      "\n",
      "Iteration: 20000\n",
      "Train Loss: 0.051070116460323334\n",
      "Average Return: 12.218992233276367\n",
      "\n",
      "Iteration: 20500\n",
      "Train Loss: 0.046258941292762756\n",
      "Average Return: 10.400492668151855\n",
      "\n",
      "Iteration: 21000\n",
      "Train Loss: 0.024624310433864594\n",
      "Average Return: 13.405577659606934\n",
      "\n",
      "Iteration: 21500\n",
      "Train Loss: 0.10272764414548874\n",
      "Average Return: 11.303449630737305\n",
      "\n",
      "Iteration: 22000\n",
      "Train Loss: 0.07031713426113129\n",
      "Average Return: 10.22841739654541\n",
      "\n",
      "Iteration: 22500\n",
      "Train Loss: 0.029917437583208084\n",
      "Average Return: 12.713135719299316\n",
      "\n",
      "Iteration: 23000\n",
      "Train Loss: 0.044330526143312454\n",
      "Average Return: 13.283618927001953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kohli\\Desktop\\TraderNetv2\\metrics\\trading\\sortino.py:20: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.exp(average_returns/std_downfall_returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 23500\n",
      "Train Loss: 0.0453534796833992\n",
      "Average Return: 10.824241638183594\n",
      "\n",
      "Iteration: 24000\n",
      "Train Loss: 0.08497361093759537\n",
      "Average Return: 10.749188423156738\n",
      "\n",
      "Iteration: 24500\n",
      "Train Loss: 0.04144695773720741\n",
      "Average Return: 11.465618133544922\n",
      "\n",
      "Iteration: 25000\n",
      "Train Loss: 0.0644921362400055\n",
      "Average Return: 11.526368141174316\n",
      "\n",
      "Iteration: 25500\n",
      "Train Loss: 0.02836482599377632\n",
      "Average Return: 12.08371639251709\n",
      "\n",
      "Iteration: 26000\n",
      "Train Loss: 0.02560250088572502\n",
      "Average Return: 12.655488014221191\n",
      "\n",
      "Iteration: 26500\n",
      "Train Loss: 0.05495087429881096\n",
      "Average Return: 11.808276176452637\n",
      "\n",
      "Iteration: 27000\n",
      "Train Loss: 0.034831173717975616\n",
      "Average Return: 13.371866226196289\n",
      "\n",
      "Iteration: 27500\n",
      "Train Loss: 0.045236457139253616\n",
      "Average Return: 13.369311332702637\n",
      "\n",
      "Iteration: 28000\n",
      "Train Loss: 0.08772952854633331\n",
      "Average Return: 13.747553825378418\n",
      "\n",
      "Iteration: 28500\n",
      "Train Loss: 0.0548773854970932\n",
      "Average Return: 12.388287544250488\n",
      "\n",
      "Iteration: 29000\n",
      "Train Loss: 0.029802102595567703\n",
      "Average Return: 11.268547058105469\n",
      "\n",
      "Iteration: 29500\n",
      "Train Loss: 0.03292527049779892\n",
      "Average Return: 14.205012321472168\n",
      "\n",
      "Iteration: 30000\n",
      "Train Loss: 0.06629399955272675\n",
      "Average Return: 12.973602294921875\n",
      "\n",
      "Iteration: 30500\n",
      "Train Loss: 0.046956583857536316\n",
      "Average Return: 11.09410572052002\n",
      "\n",
      "Iteration: 31000\n",
      "Train Loss: 0.030287280678749084\n",
      "Average Return: 11.511719703674316\n",
      "\n",
      "Iteration: 31500\n",
      "Train Loss: 0.09899552911520004\n",
      "Average Return: 11.7806396484375\n",
      "\n",
      "Iteration: 32000\n",
      "Train Loss: 0.05050331726670265\n",
      "Average Return: 11.496458053588867\n",
      "\n",
      "Iteration: 32500\n",
      "Train Loss: 0.048208169639110565\n",
      "Average Return: 12.999298095703125\n",
      "\n",
      "Iteration: 33000\n",
      "Train Loss: 0.028572306036949158\n",
      "Average Return: 11.828886032104492\n",
      "\n",
      "Iteration: 33500\n",
      "Train Loss: 0.053599853068590164\n",
      "Average Return: 12.663890838623047\n",
      "\n",
      "Iteration: 34000\n",
      "Train Loss: 0.03936024755239487\n",
      "Average Return: 11.999905586242676\n",
      "\n",
      "Iteration: 34500\n",
      "Train Loss: 0.06164770573377609\n",
      "Average Return: 14.310738563537598\n",
      "\n",
      "Iteration: 35000\n",
      "Train Loss: 0.04938936606049538\n",
      "Average Return: 10.701234817504883\n",
      "\n",
      "Iteration: 35500\n",
      "Train Loss: 0.0849401205778122\n",
      "Average Return: 12.029484748840332\n",
      "\n",
      "Iteration: 36000\n",
      "Train Loss: 0.0607389360666275\n",
      "Average Return: 14.043875694274902\n",
      "\n",
      "Iteration: 36500\n",
      "Train Loss: 0.04235595837235451\n",
      "Average Return: 12.93690299987793\n",
      "\n",
      "Iteration: 37000\n",
      "Train Loss: 0.06825180351734161\n",
      "Average Return: 13.200514793395996\n",
      "\n",
      "Iteration: 37500\n",
      "Train Loss: 0.056235574185848236\n",
      "Average Return: 13.214858055114746\n",
      "\n",
      "New best average return found at 15.58976936340332! Saving checkpoint at iteration 38000\n",
      "\n",
      "Iteration: 38000\n",
      "Train Loss: 0.08024972677230835\n",
      "Average Return: 15.58976936340332\n",
      "\n",
      "Iteration: 38500\n",
      "Train Loss: 0.051080651581287384\n",
      "Average Return: 14.817853927612305\n",
      "\n",
      "Iteration: 39000\n",
      "Train Loss: 0.09857600927352905\n",
      "Average Return: 14.938543319702148\n",
      "\n",
      "Iteration: 39500\n",
      "Train Loss: 0.040788933634757996\n",
      "Average Return: 15.092511177062988\n",
      "\n",
      "Iteration: 40000\n",
      "Train Loss: 0.04483136907219887\n",
      "Average Return: 14.210620880126953\n",
      "\n",
      "New best average return found at 16.044048309326172! Saving checkpoint at iteration 40500\n",
      "\n",
      "Iteration: 40500\n",
      "Train Loss: 0.03483106195926666\n",
      "Average Return: 16.044048309326172\n",
      "\n",
      "Iteration: 41000\n",
      "Train Loss: 0.03236648812890053\n",
      "Average Return: 16.007549285888672\n",
      "\n",
      "Iteration: 41500\n",
      "Train Loss: 0.024555087089538574\n",
      "Average Return: 16.02364158630371\n",
      "\n",
      "Iteration: 42000\n",
      "Train Loss: 0.03621381148695946\n",
      "Average Return: 15.715054512023926\n",
      "\n",
      "Iteration: 42500\n",
      "Train Loss: 0.038694411516189575\n",
      "Average Return: 15.084989547729492\n",
      "\n",
      "Iteration: 43000\n",
      "Train Loss: 0.05394652485847473\n",
      "Average Return: 14.7763032913208\n",
      "\n",
      "New best average return found at 16.476198196411133! Saving checkpoint at iteration 43500\n",
      "\n",
      "Iteration: 43500\n",
      "Train Loss: 0.049345970153808594\n",
      "Average Return: 16.476198196411133\n",
      "\n",
      "Iteration: 44000\n",
      "Train Loss: 0.06496383249759674\n",
      "Average Return: 16.288684844970703\n",
      "\n",
      "New best average return found at 16.983556747436523! Saving checkpoint at iteration 44500\n",
      "\n",
      "Iteration: 44500\n",
      "Train Loss: 0.037098608911037445\n",
      "Average Return: 16.983556747436523\n",
      "\n",
      "Iteration: 45000\n",
      "Train Loss: 0.03285551816225052\n",
      "Average Return: 16.26906394958496\n",
      "\n",
      "Iteration: 45500\n",
      "Train Loss: 0.02580549567937851\n",
      "Average Return: 14.860706329345703\n",
      "\n",
      "Iteration: 46000\n",
      "Train Loss: 0.07713090628385544\n",
      "Average Return: 15.034330368041992\n",
      "\n",
      "Iteration: 46500\n",
      "Train Loss: 0.0465189591050148\n",
      "Average Return: 15.562292098999023\n",
      "\n",
      "Iteration: 47000\n",
      "Train Loss: 0.024176280945539474\n",
      "Average Return: 16.028486251831055\n",
      "\n",
      "Iteration: 47500\n",
      "Train Loss: 0.034075308591127396\n",
      "Average Return: 15.357691764831543\n",
      "\n",
      "Iteration: 48000\n",
      "Train Loss: 0.03931136056780815\n",
      "Average Return: 15.47105884552002\n",
      "\n",
      "Iteration: 48500\n",
      "Train Loss: 0.03398844227194786\n",
      "Average Return: 15.980911254882812\n",
      "\n",
      "Iteration: 49000\n",
      "Train Loss: 0.05334573984146118\n",
      "Average Return: 14.723357200622559\n",
      "\n",
      "Iteration: 49500\n",
      "Train Loss: 0.05648282915353775\n",
      "Average Return: 15.982002258300781\n",
      "\n",
      "Iteration: 50000\n",
      "Train Loss: 0.03660065308213234\n",
      "Average Return: 13.144104957580566\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    'PPO': {dataset_name: {} for dataset_name in datasets_dict.keys()},\n",
    "    'DDQN': {dataset_name: {} for dataset_name in datasets_dict.keys()}\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'BTC': 'green',\n",
    "    'ETH': 'blue',\n",
    "    'XRP': 'red',\n",
    "    'ADA': 'black',\n",
    "    'LTC': 'orange'\n",
    "}\n",
    "linestyles = {\n",
    "    'Market-Orders': '--',\n",
    "    'Market-Limit-Orders': '-'\n",
    "}\n",
    "\n",
    "for agent_name, agent_config in agents_configs.items():\n",
    "    for dataset_name, dataset_filepath in datasets_dict.items():\n",
    "        for reward_fn_name, reward_fn_instance in rewards_dict.items():\n",
    "            tf.random.set_seed(seed=0)\n",
    "\n",
    "            train_params = {\n",
    "                'dataset_filepath': dataset_filepath,\n",
    "                'reward_fn_instance': reward_fn_instance,\n",
    "                'checkpoint_filepath': f'database/storage/checkpoints/experiments/smurf/{agent_name}/{dataset_name}/{reward_fn_name}/',\n",
    "                **train_dict,\n",
    "                **agent_config\n",
    "            }\n",
    "            eval_avg_returns, eval_metrics = train(**train_params)\n",
    "\n",
    "            results[agent_name][dataset_name][reward_fn_name] = (eval_avg_returns, eval_metrics)\n",
    "\n",
    "        for reward_fn_name, reward_fn_results in results[agent_name][dataset_name].items():\n",
    "            eval_avg_returns, eval_metrics = reward_fn_results\n",
    "\n",
    "            metrics_dict = {\n",
    "                'steps': [10000*i for i in range(len(eval_avg_returns))],\n",
    "                'average_returns': eval_avg_returns,\n",
    "                **{metric.name: metric.episode_metrics for metric in eval_metrics}\n",
    "            }\n",
    "            metrics_df = pd.DataFrame(metrics_dict)\n",
    "            metrics_df.to_csv(f'experiments/smurf/{agent_name}/{dataset_name}_{reward_fn_name}.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
